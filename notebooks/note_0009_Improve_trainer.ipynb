{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from specq_jax.core import (\n",
    "    SpecQDataset,\n",
    "    # create_train_step,\n",
    "    # create_train_step_v2,\n",
    "    # loss as loss_fn,\n",
    "    # loss_v2 as loss_fn_v2,\n",
    "    gate_loss,\n",
    "    X,\n",
    "    Y,\n",
    "    Z,\n",
    "    calculate_expvals,\n",
    "    plot_expvals,\n",
    "    gate_fidelity,\n",
    "    Wo_2_level,\n",
    "    rotating_transmon_hamiltonian,\n",
    "    batched_calculate_expectation_value,\n",
    "    batch_mse\n",
    ")\n",
    "from specq_jax.data import load_data\n",
    "\n",
    "from specq_jax.model import BasicBlackBox\n",
    "from exp_data_0020 import get_multi_drag_pulse_sequence\n",
    "import specq_dev.specq.shared as specq\n",
    "\n",
    "from jaxopt import ProjectedGradient\n",
    "from jaxopt.projection import projection_box\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from alive_progress import alive_bar, alive_it\n",
    "\n",
    "from torch import Generator, manual_seed\n",
    "\n",
    "from flax.training.train_state import TrainState\n",
    "from jaxtyping import Array, Complex, Float\n",
    "from flax import linen as nn\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_v2(\n",
    "    state: TrainState,\n",
    "    pulse_parameters: Float[Array, \"batch num_pulses num_features\"],  # noqa: F722\n",
    "    unitaries: Complex[Array, \"batch dim dim\"],  # noqa: F722\n",
    "    expectation_values: Complex[Array, \"batch num_expectations\"],  # noqa: F722\n",
    "    evaluate_expectation_values: list[\n",
    "        specq.ExpectationValue\n",
    "    ] = specq.default_expectation_values,\n",
    "):\n",
    "    # Predict Vo for each pauli operator from paluse parameters\n",
    "    Wos_params = state.apply_fn(state.params, pulse_parameters)\n",
    "\n",
    "    predict_expectation_values = []\n",
    "\n",
    "    # Calculate expectation values for all cases\n",
    "    for idx, exp_case in enumerate(evaluate_expectation_values):\n",
    "        Wo = jax.vmap(Wo_2_level, in_axes=(0, 0))(\n",
    "            Wos_params[exp_case.observable][\"U\"], Wos_params[exp_case.observable][\"D\"]\n",
    "        )\n",
    "        # Calculate expectation value for each pauli operator\n",
    "        batch_expectaion_values = batched_calculate_expectation_value(\n",
    "            unitaries,\n",
    "            Wo,\n",
    "            jnp.array(exp_case.initial_statevector),\n",
    "        )\n",
    "\n",
    "        predict_expectation_values.append(batch_expectaion_values)\n",
    "\n",
    "    return jnp.mean(\n",
    "        batch_mse(expectation_values, jnp.array(predict_expectation_values).T)\n",
    "    )\n",
    "\n",
    "\n",
    "def create_train_step_v2(\n",
    "    key: jnp.ndarray,\n",
    "    model: nn.Module,\n",
    "    optimiser: optax.GradientTransformation,\n",
    "    loss_fn: Callable[\n",
    "        [\n",
    "            TrainState,\n",
    "            Float[Array, \"batch num_pulses num_features\"],  # noqa: F722\n",
    "            Complex[Array, \"batch dim dim\"],  # noqa: F722\n",
    "            Complex[Array, \"batch num_expectations\"],  # noqa: F722\n",
    "        ],\n",
    "        Float[Array, \"1\"],\n",
    "    ],\n",
    "    input_shape: Array,\n",
    "):\n",
    "    params = model.init(\n",
    "        key,\n",
    "        jnp.ones(input_shape, jnp.float32),\n",
    "    )  # dummy key just as example input\n",
    "\n",
    "    # opt_state = optimiser.init(params)\n",
    "\n",
    "    state = TrainState.create(apply_fn=model.apply, params=params, tx=optimiser)\n",
    "\n",
    "    @jax.jit\n",
    "    def train_step(\n",
    "        state: TrainState,\n",
    "        pulse_parameters: Float[Array, \"batch num_pulses num_features\"],  # noqa: F722\n",
    "        unitaries: Complex[Array, \"batch dim dim\"],  # noqa: F722\n",
    "        expectations: Complex[Array, \"batch num_expectations\"],  # noqa: F722\n",
    "    ):\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(\n",
    "            state,\n",
    "            pulse_parameters,\n",
    "            unitaries,\n",
    "            expectations,\n",
    "        )\n",
    "\n",
    "        state = state.apply_gradients(grads=grads)\n",
    "\n",
    "        return state, loss\n",
    "\n",
    "    @jax.jit\n",
    "    def test_step(\n",
    "        state: TrainState,\n",
    "        pulse_parameters: Float[Array, \"batch num_pulses num_features\"],  # noqa: F722\n",
    "        unitaries: Complex[Array, \"batch dim dim\"],  # noqa: F722\n",
    "        expectations: Complex[Array, \"batch num_expectations\"],  # noqa: F722\n",
    "    ):\n",
    "        loss = loss_fn(\n",
    "            state,\n",
    "            pulse_parameters,\n",
    "            unitaries,\n",
    "            expectations,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    return train_step, test_step, state\n",
    "\n",
    "\n",
    "def with_validation_train(\n",
    "    train_dataloader: DataLoader,\n",
    "    val_dataloader: DataLoader,\n",
    "    train_step,\n",
    "    test_step,\n",
    "    state,\n",
    "    num_epochs=1250,\n",
    "):\n",
    "\n",
    "    history = []\n",
    "    total_len = len(train_dataloader)\n",
    "\n",
    "    NUM_EPOCHS = num_epochs\n",
    "\n",
    "    with alive_bar(int(NUM_EPOCHS * total_len), force_tty=True) as bar:\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            total_loss = 0.0\n",
    "            for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "                _pulse_parameters = batch[\"x0\"].numpy()\n",
    "                _unitaries = batch[\"x1\"].numpy()\n",
    "                _expectations = batch[\"y\"].numpy()\n",
    "\n",
    "                state, loss = train_step(\n",
    "                    state, _pulse_parameters, _unitaries, _expectations\n",
    "                )\n",
    "\n",
    "                history.append(\n",
    "                    {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"step\": i,\n",
    "                        \"loss\": float(loss),\n",
    "                        \"global_step\": epoch * total_len + i,\n",
    "                        \"val_loss\": None,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                total_loss += loss\n",
    "\n",
    "                bar()\n",
    "\n",
    "            # Validation\n",
    "            val_loss = 0.0\n",
    "            for i, batch in enumerate(val_dataloader):\n",
    "\n",
    "                _pulse_parameters = batch[\"x0\"].numpy()\n",
    "                _unitaries = batch[\"x1\"].numpy()\n",
    "                _expectations = batch[\"y\"].numpy()\n",
    "\n",
    "                val_loss += test_step(\n",
    "                    state, _pulse_parameters, _unitaries, _expectations\n",
    "                )\n",
    "\n",
    "            history[-1][\"val_loss\"] = float(val_loss / len(val_dataloader))\n",
    "\n",
    "    return state, history\n",
    "\n",
    "\n",
    "def plot_history(history, lr_scheduler):\n",
    "\n",
    "    hist_df = pd.DataFrame(history)\n",
    "    train = hist_df[[\"global_step\", \"loss\"]].values\n",
    "\n",
    "    train_x = train[:, 0]\n",
    "    train_y = train[:, 1]\n",
    "\n",
    "    validate = hist_df[[\"global_step\", \"val_loss\"]].replace(0, jnp.nan).dropna().values\n",
    "\n",
    "    validate_x = validate[:, 0]\n",
    "    validate_y = validate[:, 1]\n",
    "    # The second plot has height ratio 2\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 6), sharex=True, height_ratios=[3, 1])\n",
    "\n",
    "    # The first plot is the training loss and the validation loss\n",
    "    ax[0].plot(train_x, train_y, label=\"train_loss\")\n",
    "    ax[0].plot(validate_x, validate_y, \".-\", label=\"val_loss\")\n",
    "    ax[0].set_yscale(\"log\")\n",
    "\n",
    "    # plot the horizontal line [1e-3, 1e-2]\n",
    "    ax[0].axhline(1e-3, color=\"red\", linestyle=\"--\")\n",
    "    ax[0].axhline(1e-2, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    # The second plot is the learning rate\n",
    "    lr = lr_scheduler(train_x)\n",
    "    ax[1].plot(train_x, lr, label=\"learning_rate\")\n",
    "    ax[1].set_yscale(\"log\")\n",
    "\n",
    "    # for thred in [1e-3, 1e-2, 1e-5, 1e-4]:\n",
    "    #     ax[1].axhline(thred, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax[0].legend()\n",
    "    ax[1].legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def optimize(x0, lower, upper, fun):\n",
    "\n",
    "    pg = ProjectedGradient(fun=fun, projection=projection_box)\n",
    "    opt_params, state = pg.run(jnp.array(x0), hyperparams_proj=(lower, upper))\n",
    "\n",
    "    return opt_params, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data, pulse_parameters, unitaries, expectations, pulse_sequence, simulator = (\n",
    "    load_data(\n",
    "        \"../../specq-experiment/datasets/0020\",\n",
    "        get_multi_drag_pulse_sequence,\n",
    "        rotating_transmon_hamiltonian,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx, end_idx = 0, 1500\n",
    "\n",
    "# Final goal of setting up is to create a dataset and a dataloader\n",
    "dataset = SpecQDataset(\n",
    "    pulse_parameters=pulse_parameters[start_idx: end_idx],\n",
    "    unitaries=unitaries[start_idx: end_idx],\n",
    "    expectation_values=expectations[start_idx: end_idx],\n",
    ")\n",
    "\n",
    "batch_size = 150\n",
    "# Randomly split dataset into training and validation\n",
    "key = jax.random.PRNGKey(0)\n",
    "val_indices = jax.random.choice(\n",
    "    key, len(dataset), (int(0.2 * len(dataset)),), replace=False\n",
    ").tolist()\n",
    "\n",
    "training_indices = list(\n",
    "    set([i for i in range(len(dataset))]) - set(val_indices)\n",
    ")\n",
    "\n",
    "train_dataset = Subset(dataset, training_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "g = Generator()\n",
    "g.manual_seed(0)\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=g)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, generator=g)\n",
    "\n",
    "# len(train_dataloader), len(val_dataloader)\n",
    "\n",
    "key, model_key = jax.random.split(jax.random.PRNGKey(0))\n",
    "model = BasicBlackBox(feature_size=5)\n",
    "# model = ParallelBlackBox(hidden_sizes=(20, 10))\n",
    "# optimiser = optax.adam(learning_rate=1e-3)\n",
    "\n",
    "warmup_start_lr, warmup_steps = 1e-6, 1000\n",
    "start_lr, end_lr, steps = 1e-2, 1e-5, 10_000\n",
    "lr_scheduler = optax.join_schedules(\n",
    "    [\n",
    "        optax.linear_schedule(\n",
    "            warmup_start_lr,\n",
    "            start_lr,\n",
    "            warmup_steps,\n",
    "        ),\n",
    "        optax.linear_schedule(\n",
    "            start_lr,\n",
    "            end_lr,\n",
    "            steps - warmup_steps,\n",
    "        ),\n",
    "    ],\n",
    "    [warmup_steps],\n",
    ")\n",
    "\n",
    "optimiser = optax.adam(lr_scheduler)\n",
    "\n",
    "train_step, test_step, state = create_train_step_v2(\n",
    "    key=model_key,\n",
    "    model=model,\n",
    "    optimiser=optimiser,\n",
    "    loss_fn=loss_v2,\n",
    "    input_shape=(batch_size, pulse_parameters.shape[1]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|⚠︎                                       | (!) 0/10000 [0%] in 0.5s (0.00/s)    \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument of type 'TrainState' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m state, history \u001b[38;5;241m=\u001b[39m \u001b[43mwith_validation_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 128\u001b[0m, in \u001b[0;36mwith_validation_train\u001b[0;34m(train_dataloader, val_dataloader, train_step, test_step, state, num_epochs)\u001b[0m\n\u001b[1;32m    125\u001b[0m _unitaries \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    126\u001b[0m _expectations \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m--> 128\u001b[0m state, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pulse_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_unitaries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_expectations\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m history\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    133\u001b[0m     {\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m     }\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    142\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[28], line 83\u001b[0m, in \u001b[0;36mcreate_train_step_v2.<locals>.train_step\u001b[0;34m(state, pulse_parameters, unitaries, expectations)\u001b[0m\n\u001b[1;32m     81\u001b[0m grad_fn \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mgrad(loss_fn, allow_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     82\u001b[0m grads \u001b[38;5;241m=\u001b[39m grad_fn(state, pulse_parameters, unitaries, expectations)\n\u001b[0;32m---> 83\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, loss\n",
      "File \u001b[0;32m~/miniconda3/envs/specq-jax/lib/python3.12/site-packages/flax/training/train_state.py:94\u001b[0m, in \u001b[0;36mTrainState.apply_gradients\u001b[0;34m(self, grads, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, grads, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     80\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Updates ``step``, ``params``, ``opt_state`` and ``**kwargs`` in return value.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m  Note that internally this function calls ``.tx.update()`` followed by a call\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m    replaced as specified by ``kwargs``.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mOVERWRITE_WITH_GRADIENT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m:\n\u001b[1;32m     95\u001b[0m     grads_with_opt \u001b[38;5;241m=\u001b[39m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     96\u001b[0m     params_with_opt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'TrainState' is not iterable"
     ]
    }
   ],
   "source": [
    "state, history = with_validation_train(\n",
    "    train_dataloader, val_dataloader, train_step, test_step, state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': TrainState(step=1, apply_fn=<bound method Module.apply of Dense(\n",
       "     # attributes\n",
       "     features = 3\n",
       "     use_bias = True\n",
       "     dtype = None\n",
       "     param_dtype = float32\n",
       "     precision = None\n",
       "     kernel_init = init\n",
       "     bias_init = zeros\n",
       "     dot_general = None\n",
       "     dot_general_cls = None\n",
       " )>, params={'bias': Array([-0.001, -0.001, -0.001], dtype=float32), 'kernel': Array([[ 0.2641001 , -0.6103823 , -0.23097458],\n",
       "        [ 0.11147016, -0.87561315,  0.9810296 ],\n",
       "        [ 0.36252323,  0.18267715, -0.6856925 ],\n",
       "        [-0.849457  , -0.63919145, -0.4793319 ],\n",
       "        [-0.68809915, -0.33936214, -0.05847799]], dtype=float32)}, tx=GradientTransformationExtraArgs(init=<function chain.<locals>.init_fn at 0x361395760>, update=<function chain.<locals>.update_fn at 0x361395940>), opt_state=(EmptyState(), EmptyState())),\n",
       " 'config': {'dimensions': Array([5, 3], dtype=int64)},\n",
       " 'data': [Array([-2.61055618,  0.03385296,  1.08633353, -1.48029861,  0.4889569 ],      dtype=float64)]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple model with one linear layer.\n",
    "key1, key2 = jax.random.split(jax.random.key(0))\n",
    "x1 = jax.random.normal(key1, (5,))  # A simple JAX array.\n",
    "model = nn.Dense(features=3)\n",
    "variables = model.init(key2, x1)\n",
    "\n",
    "# Flax's TrainState is a pytree dataclass and is supported in checkpointing.\n",
    "# Define your class with `@flax.struct.dataclass` decorator to make it compatible.\n",
    "tx = optax.sgd(learning_rate=0.001)  # An Optax SGD optimizer.\n",
    "state = TrainState.create(apply_fn=model.apply, params=variables[\"params\"], tx=tx)\n",
    "\n",
    "loss_fn = lambda params, x: jnp.sum(state.apply_fn({\"params\": params}, x))\n",
    "loss, grad = jax.value_and_grad(loss_fn)(state.params, x1)\n",
    "\n",
    "# Perform a simple gradient update similar to the one during a normal training workflow.\n",
    "state = state.apply_gradients(grads=grad)\n",
    "# state = state.apply_gradients(grads=jax.tree_util.tree_map(jnp.ones_like, state.params))\n",
    "\n",
    "# Some arbitrary nested pytree with a dictionary and a NumPy array.\n",
    "config = {\"dimensions\": jnp.array([5, 3])}\n",
    "\n",
    "# Bundle everything together.\n",
    "ckpt = {\"model\": state, \"config\": config, \"data\": [x1]}\n",
    "ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ApplyScopeInvalidVariablesTypeError",
     "evalue": "The first argument passed to an apply function should be a dictionary of collections. Each collection should be a dictionary with string keys. (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ApplyScopeInvalidVariablesTypeError)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApplyScopeInvalidVariablesTypeError\u001b[0m       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m state \u001b[38;5;241m=\u001b[39m TrainState\u001b[38;5;241m.\u001b[39mcreate(apply_fn\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mapply, params\u001b[38;5;241m=\u001b[39mvariables[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m], tx\u001b[38;5;241m=\u001b[39mtx)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/specq-jax/lib/python3.12/site-packages/flax/core/scope.py:1048\u001b[0m, in \u001b[0;36mbind\u001b[0;34m(variables, rngs, mutable, flags)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Binds variables and rngs to a new ``Scope``.\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \n\u001b[1;32m   1028\u001b[0m \u001b[38;5;124;03mbind provides a ``Scope`` instance without transforming a function with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;124;03m  A new scope with the variables and rngs bound to it.\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_variables(variables):\n\u001b[0;32m-> 1048\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mApplyScopeInvalidVariablesTypeError()\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rngs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_rngs(rngs):\n\u001b[1;32m   1050\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidRngError(\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrngs should be a dictionary mapping strings to `jax.PRNGKey`.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1052\u001b[0m   )\n",
      "\u001b[0;31mApplyScopeInvalidVariablesTypeError\u001b[0m: The first argument passed to an apply function should be a dictionary of collections. Each collection should be a dictionary with string keys. (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ApplyScopeInvalidVariablesTypeError)"
     ]
    }
   ],
   "source": [
    "state = TrainState.create(apply_fn=model.apply, params=variables[\"params\"], tx=tx)\n",
    "\n",
    "\n",
    "state.apply_fn(state.params, x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.63957241, 2.55293094, 0.58267994], dtype=float64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(variables, x1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "specq-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
