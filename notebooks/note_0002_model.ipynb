{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "from flax import linen as nn\n",
    "import jax.numpy as jnp\n",
    "from typing import Callable, Sequence\n",
    "from jaxtyping import Array, Complex, Float\n",
    "import specq_dev.specq.shared as specq\n",
    "import optax\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "from core import (\n",
    "    Wo_2_level,\n",
    "    calculate_exp,\n",
    "    get_simulator,\n",
    "    SpecQDataset,\n",
    "    mse,\n",
    "    batch_mse,\n",
    "    batched_calculate_expectation_value,\n",
    "    loss as loss_fn,\n",
    "    create_train_step,\n",
    "    calculate_expvals,\n",
    "    plot_expvals,\n",
    "    gate_fidelity\n",
    ")\n",
    "from pulse import get_drag_pulse_sequence\n",
    "import logging\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlackBox(nn.Module):\n",
    "    feature_size: int\n",
    "    hidden_sizes_1: Sequence[int] = (20, 10)\n",
    "    hidden_sizes_2: Sequence[int] = (20, 10)\n",
    "    pauli_operators: Sequence[str] = (\"X\", \"Y\", \"Z\")\n",
    "\n",
    "    activation: Callable = nn.tanh\n",
    "    NUM_UNITARY_PARAMS: int = 3\n",
    "    NUM_DIAGONAL_PARAMS: int = 2\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray):\n",
    "        x = nn.Dense(features=self.feature_size)(x)\n",
    "        # Flatten the input\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        # Apply a activation function\n",
    "        x = nn.relu(x)\n",
    "        # Dropout\n",
    "        # x = nn.Dropout(0.2)(x)\n",
    "        # Apply a dense layer for each hidden size\n",
    "        for hidden_size in self.hidden_sizes_1:\n",
    "            x = nn.Dense(features=hidden_size)(x)\n",
    "            x = nn.relu(x)\n",
    "\n",
    "        Wos_params = dict()\n",
    "        for op in self.pauli_operators:\n",
    "            # Sub hidden layer\n",
    "            for hidden_size in self.hidden_sizes_2:\n",
    "                _x = nn.Dense(features=hidden_size)(x)\n",
    "                _x = nn.relu(_x)\n",
    "\n",
    "            Wos_params[op] = dict()\n",
    "            # For the unitary part, we use a dense layer with 3 features\n",
    "            unitary_params = nn.Dense(features=self.NUM_UNITARY_PARAMS)(_x)\n",
    "            # Apply sigmoid to this layer\n",
    "            unitary_params = 2 * jnp.pi * nn.sigmoid(unitary_params)\n",
    "            # For the diagonal part, we use a dense layer with 1 feature\n",
    "            diag_params = nn.Dense(features=self.NUM_DIAGONAL_PARAMS)(_x)\n",
    "            # Apply the activation function\n",
    "            diag_params = self.activation(diag_params)\n",
    "\n",
    "            Wos_params[op] = {\n",
    "                \"U\": unitary_params,\n",
    "                \"D\": diag_params,\n",
    "            }\n",
    "\n",
    "        return Wos_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GRUBlackBox(nn.Module):\n",
    "\n",
    "    def setup(self) -> None:\n",
    "        \n",
    "        condense_layer = [\n",
    "            nn.Dense(10),\n",
    "            nn.relu,\n",
    "            nn.Dense(5),\n",
    "            nn.relu,\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x: jnp.ndarray):\n",
    "        \n",
    "        output_auto_1 = ...\n",
    "        \n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_model = nn.Dense(5)\n",
    "model_key, pulse_key = jax.random.split(jax.random.PRNGKey(0))\n",
    "temp_model = BasicBlackBox(feature_size=10)\n",
    "\n",
    "temp_pulse_params = jax.random.uniform(pulse_key, (3, 30))\n",
    "\n",
    "params = temp_model.init(model_key, temp_pulse_params)\n",
    "\n",
    "out = temp_model.apply(params, temp_pulse_params)\n",
    "# out.shape, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model with dummies data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_data(\n",
    "    key: jnp.ndarray,\n",
    "    batch_size: int,\n",
    "    pulse_params_shape: Sequence[int],\n",
    ") -> tuple[\n",
    "    jnp.ndarray,\n",
    "    jnp.ndarray,\n",
    "    jnp.ndarray,\n",
    "]:\n",
    "    DIMENSION = 2\n",
    "\n",
    "    # Split the key\n",
    "    unitaries_key, pulse_parameters_key, expectations_value_key = jax.random.split(\n",
    "        key, 3 # <--- 3 keys\n",
    "    )\n",
    "\n",
    "    real_key, imag_key = jax.random.split(unitaries_key)\n",
    "\n",
    "    # Random the unitary operators\n",
    "    unitaries = jax.random.normal(\n",
    "        real_key, (batch_size, DIMENSION, DIMENSION)\n",
    "    ) + 1j * jax.random.normal(imag_key, (batch_size, DIMENSION, DIMENSION))\n",
    "    # Normalize the unitaries\n",
    "    unitaries = jnp.array([jnp.linalg.qr(unitary)[0] for unitary in unitaries])\n",
    "\n",
    "    # Random the pulse parameters\n",
    "    pulse_parameters = jax.random.normal(\n",
    "        pulse_parameters_key, (batch_size, *pulse_params_shape)\n",
    "    )\n",
    "    # Random the expectation values\n",
    "    expectations = jax.random.normal(\n",
    "        expectations_value_key, (batch_size, len(specq.default_expectation_values))\n",
    "    )\n",
    "    return pulse_parameters, unitaries, expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = BasicBlackBox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulse sequence with 2 dimensions shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_key, subkey = jax.random.split(jax.random.PRNGKey(0))\n",
    "batch_size = 10\n",
    "num_pulses = 80\n",
    "feature_size = 2\n",
    "shape = (num_pulses, feature_size)\n",
    "\n",
    "# initialize the model\n",
    "model = model_class(feature_size=feature_size)\n",
    "params = model.init(model_key, jnp.ones((batch_size, *shape)))\n",
    "\n",
    "pulse_parameters, unitaries, expectations = create_dummy_data(\n",
    "    subkey, batch_size, shape\n",
    ")\n",
    "\n",
    "y_pred = loss_fn(params, pulse_parameters, unitaries, expectations, model)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulse sequence with 1 dimension shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_key, subkey = jax.random.split(jax.random.PRNGKey(0))\n",
    "batch_size = 10\n",
    "shape = (30, )\n",
    "\n",
    "# initialize the model\n",
    "model = model_class(feature_size=feature_size)\n",
    "params = model.init(model_key, jnp.ones((batch_size, *shape)))\n",
    "\n",
    "pulse_parameters, unitaries, expectations = create_dummy_data(\n",
    "    subkey, batch_size, shape\n",
    ")\n",
    "\n",
    "y_pred = loss_fn(params, pulse_parameters, unitaries, expectations, model)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from 0019\n",
      "Prepared the waveforms for the experiment 0019\n",
      "Got the unitaries for the experiment 0019\n",
      "Finished preparing the data for the experiment 0019\n"
     ]
    }
   ],
   "source": [
    "def get_exp_data():\n",
    "    # Load the data from the experiment\n",
    "    exp_data = specq.ExperimentDataV3.from_folder(\"../specq-experiment/datasets/0019\")\n",
    "\n",
    "    print(f\"Loaded data from {exp_data.experiment_config.EXPERIMENT_IDENTIFIER}\")\n",
    "\n",
    "    # Setup the simulator\n",
    "    dt = exp_data.experiment_config.device_cycle_time_ns\n",
    "    qubit_info = exp_data.experiment_config.qubits[0]\n",
    "    pulse_sequence = get_drag_pulse_sequence(dt=dt, drive_str=qubit_info.drive_strength)\n",
    "    t_eval = jnp.linspace(\n",
    "        0, pulse_sequence.pulse_length_dt * dt, pulse_sequence.pulse_length_dt\n",
    "    )\n",
    "    simulator = get_simulator(qubit_info=qubit_info, t_eval=t_eval)\n",
    "\n",
    "    # The properties of the pulse parameters\n",
    "    num_pulses = 80\n",
    "    feature_size = 2\n",
    "\n",
    "    # Get the waveforms for each pulse parameters to get the unitaries\n",
    "    waveforms = []\n",
    "    for pulse_params in exp_data.get_parameters_dict_list():\n",
    "        waveforms.append(pulse_sequence.get_waveform(pulse_params))\n",
    "\n",
    "    waveforms = jnp.array(waveforms)\n",
    "\n",
    "    print(\n",
    "        f\"Prepared the waveforms for the experiment {exp_data.experiment_config.EXPERIMENT_IDENTIFIER}\"\n",
    "    )\n",
    "\n",
    "    # jit the simulator\n",
    "    jitted_simulator = jax.jit(simulator)\n",
    "    # batch the simulator\n",
    "    batched_simulator = jax.vmap(jitted_simulator, in_axes=(0))\n",
    "    # Get the unitaries\n",
    "    unitaries = batched_simulator(waveforms)\n",
    "\n",
    "    print(\n",
    "        f\"Got the unitaries for the experiment {exp_data.experiment_config.EXPERIMENT_IDENTIFIER}\"\n",
    "    )\n",
    "\n",
    "    # Get the final unitaries\n",
    "    unitaries = np.array(unitaries[:, -1, :, :])\n",
    "    # Get the expectation values from the experiment\n",
    "    expectations = exp_data.get_expectation_values()\n",
    "    # Get the pulse parameters\n",
    "    pulse_parameters = exp_data.parameters.reshape(\n",
    "        (exp_data.parameters.shape[0], num_pulses, feature_size)\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Finished preparing the data for the experiment {exp_data.experiment_config.EXPERIMENT_IDENTIFIER}\"\n",
    "    )\n",
    "\n",
    "    return pulse_parameters, unitaries, expectations, pulse_sequence, simulator\n",
    "\n",
    "pulse_parameters, unitaries, expectations, pulse_sequence, simulator = get_exp_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final goal of setting up is to create a dataset and a dataloader\n",
    "dataset = SpecQDataset(\n",
    "    pulse_parameters=pulse_parameters.reshape(-1, 160),\n",
    "    unitaries=unitaries,\n",
    "    expectation_values=expectations,\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, model_key = jax.random.split(jax.random.PRNGKey(0))\n",
    "model = BasicBlackBox(feature_size=30)\n",
    "# optimiser = optax.adam(learning_rate=1e-3)\n",
    "\n",
    "warmup_start_lr, warmup_steps = 1e-6, 100\n",
    "start_lr, end_lr, steps = 1e-2, 1e-5, 1_000\n",
    "lr_scheduler = optax.join_schedules(\n",
    "    [\n",
    "        optax.linear_schedule(\n",
    "            warmup_start_lr,\n",
    "            start_lr,\n",
    "            warmup_steps,\n",
    "        ),\n",
    "        optax.linear_schedule(\n",
    "            start_lr,\n",
    "            end_lr,\n",
    "            steps - warmup_steps,\n",
    "        ),\n",
    "    ],\n",
    "    [warmup_steps],\n",
    ")\n",
    "\n",
    "optimiser = optax.adam(lr_scheduler)\n",
    "\n",
    "train_step, test_step, model_params, opt_state = create_train_step(\n",
    "    key=model_key,\n",
    "    model=model,\n",
    "    optimiser=optimiser,\n",
    "    loss_fn=lambda params, pulse_parameters, unitaries, expectations: loss_fn(\n",
    "        params, pulse_parameters, unitaries, expectations, model\n",
    "    ),\n",
    "    input_shape=(128, 160),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/porametpathumsoot/miniconda3/envs/specq-dev/lib/python3.12/site-packages/jax/_src/lax/lax.py:2660: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  x_bar = _convert_element_type(x_bar, x.aval.dtype, x.aval.weak_type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | step 0 | loss: 0.061516014259857886\n",
      "epoch 0 | step 1 | loss: 0.12314624096146236\n",
      "epoch 0 | step 2 | loss: 0.18465948641019098\n",
      "epoch 0 | step 3 | loss: 0.24613037899659868\n",
      "epoch 0 | step 4 | loss: 0.30754691616074714\n",
      "epoch 0 | step 5 | loss: 0.36885490427168516\n",
      "epoch 0 | step 6 | loss: 0.4299714766956235\n",
      "epoch 0 | step 7 | loss: 0.49081072386840574\n",
      "epoch 0 | step 8 | loss: 0.5515428817070159\n",
      "epoch 0 | step 9 | loss: 0.6119762747867009\n",
      "epoch 0 | step 10 | loss: 0.6722495453148655\n",
      "epoch 0 | step 11 | loss: 0.73225741033545\n",
      "epoch 1 | step 0 | loss: 0.05947775735225382\n",
      "epoch 1 | step 1 | loss: 0.1184938978200322\n",
      "epoch 1 | step 2 | loss: 0.17697231369423022\n",
      "epoch 1 | step 3 | loss: 0.2348560807267437\n",
      "epoch 1 | step 4 | loss: 0.2920519896189518\n",
      "epoch 1 | step 5 | loss: 0.3487333446482862\n",
      "epoch 1 | step 6 | loss: 0.40458284205104594\n",
      "epoch 1 | step 7 | loss: 0.45955343364057877\n",
      "epoch 1 | step 8 | loss: 0.5136923008436229\n",
      "epoch 1 | step 9 | loss: 0.5666690805067022\n",
      "epoch 1 | step 10 | loss: 0.6187295296486621\n",
      "epoch 1 | step 11 | loss: 0.6699996607335528\n",
      "epoch 2 | step 0 | loss: 0.04985714007759913\n",
      "epoch 2 | step 1 | loss: 0.09851003375902616\n",
      "epoch 2 | step 2 | loss: 0.14625869143014114\n",
      "epoch 2 | step 3 | loss: 0.19233123298738927\n",
      "epoch 2 | step 4 | loss: 0.23690350275620312\n",
      "epoch 2 | step 5 | loss: 0.2795904108639447\n",
      "epoch 2 | step 6 | loss: 0.3202072687167406\n",
      "epoch 2 | step 7 | loss: 0.3584258681560375\n",
      "epoch 2 | step 8 | loss: 0.3938390743861156\n",
      "epoch 2 | step 9 | loss: 0.42724644531927625\n",
      "epoch 2 | step 10 | loss: 0.4583486520775691\n",
      "epoch 2 | step 11 | loss: 0.48741863489638354\n",
      "epoch 3 | step 0 | loss: 0.027267113386667274\n",
      "epoch 3 | step 1 | loss: 0.05245545350164395\n",
      "epoch 3 | step 2 | loss: 0.07648863642391603\n",
      "epoch 3 | step 3 | loss: 0.09915751672495102\n",
      "epoch 3 | step 4 | loss: 0.11979427291007202\n",
      "epoch 3 | step 5 | loss: 0.13852092941104557\n",
      "epoch 3 | step 6 | loss: 0.15518013057457875\n",
      "epoch 3 | step 7 | loss: 0.16949269826627192\n",
      "epoch 3 | step 8 | loss: 0.18187732017565986\n",
      "epoch 3 | step 9 | loss: 0.193004937324318\n",
      "epoch 3 | step 10 | loss: 0.20399667663717613\n",
      "epoch 3 | step 11 | loss: 0.21413205393855078\n",
      "epoch 4 | step 0 | loss: 0.009203655464548412\n",
      "epoch 4 | step 1 | loss: 0.017135755835888715\n",
      "epoch 4 | step 2 | loss: 0.0238815418764907\n",
      "epoch 4 | step 3 | loss: 0.0294149499504279\n",
      "epoch 4 | step 4 | loss: 0.03425401155812612\n",
      "epoch 4 | step 5 | loss: 0.03851128733146902\n",
      "epoch 4 | step 6 | loss: 0.04203461522490672\n",
      "epoch 4 | step 7 | loss: 0.045076844625883315\n",
      "epoch 4 | step 8 | loss: 0.048084068684265444\n",
      "epoch 4 | step 9 | loss: 0.05135428283660951\n",
      "epoch 4 | step 10 | loss: 0.05396178144641265\n",
      "epoch 4 | step 11 | loss: 0.0561252500003972\n",
      "epoch 5 | step 0 | loss: 0.0018556604870567125\n",
      "epoch 5 | step 1 | loss: 0.003814735181637137\n",
      "epoch 5 | step 2 | loss: 0.005964987820454994\n",
      "epoch 5 | step 3 | loss: 0.008063502422349513\n",
      "epoch 5 | step 4 | loss: 0.009867569357085267\n",
      "epoch 5 | step 5 | loss: 0.011472513140131554\n",
      "epoch 5 | step 6 | loss: 0.013059498986433188\n",
      "epoch 5 | step 7 | loss: 0.014816997727578596\n",
      "epoch 5 | step 8 | loss: 0.016577501784355485\n",
      "epoch 5 | step 9 | loss: 0.018164058911049625\n",
      "epoch 5 | step 10 | loss: 0.01958195655262969\n",
      "epoch 5 | step 11 | loss: 0.021139280575513834\n",
      "epoch 6 | step 0 | loss: 0.0015622622787519911\n",
      "epoch 6 | step 1 | loss: 0.003102086781132127\n",
      "epoch 6 | step 2 | loss: 0.004530101206330994\n",
      "epoch 6 | step 3 | loss: 0.005962349161660394\n",
      "epoch 6 | step 4 | loss: 0.007306087760295128\n",
      "epoch 6 | step 5 | loss: 0.008986813244039178\n",
      "epoch 6 | step 6 | loss: 0.010392564321190892\n",
      "epoch 6 | step 7 | loss: 0.01180723458775856\n",
      "epoch 6 | step 8 | loss: 0.013102442437712577\n",
      "epoch 6 | step 9 | loss: 0.014402630734877107\n",
      "epoch 6 | step 10 | loss: 0.015671890874578147\n",
      "epoch 6 | step 11 | loss: 0.01695790235489749\n",
      "epoch 7 | step 0 | loss: 0.0010734432087847073\n",
      "epoch 7 | step 1 | loss: 0.0023354625153195275\n",
      "epoch 7 | step 2 | loss: 0.0036960909778809697\n",
      "epoch 7 | step 3 | loss: 0.005034448066704991\n",
      "epoch 7 | step 4 | loss: 0.006237674776248055\n",
      "epoch 7 | step 5 | loss: 0.007479231024677355\n",
      "epoch 7 | step 6 | loss: 0.008829540536790685\n",
      "epoch 7 | step 7 | loss: 0.010193257390308606\n",
      "epoch 7 | step 8 | loss: 0.011302405860182087\n",
      "epoch 7 | step 9 | loss: 0.012630688006220966\n",
      "epoch 7 | step 10 | loss: 0.013801074642487896\n",
      "epoch 7 | step 11 | loss: 0.01492611789117584\n",
      "epoch 8 | step 0 | loss: 0.001152864478687201\n",
      "epoch 8 | step 1 | loss: 0.0021595673900249393\n",
      "epoch 8 | step 2 | loss: 0.003272002177506332\n",
      "epoch 8 | step 3 | loss: 0.004430209122373826\n",
      "epoch 8 | step 4 | loss: 0.0055305510476991605\n",
      "epoch 8 | step 5 | loss: 0.006535766474998486\n",
      "epoch 8 | step 6 | loss: 0.007582338544458081\n",
      "epoch 8 | step 7 | loss: 0.00870411762202635\n",
      "epoch 8 | step 8 | loss: 0.009825798793673879\n",
      "epoch 8 | step 9 | loss: 0.010845004496216833\n",
      "epoch 8 | step 10 | loss: 0.011800305167569572\n",
      "epoch 8 | step 11 | loss: 0.012771670042401432\n",
      "epoch 9 | step 0 | loss: 0.0010145311444888455\n",
      "epoch 9 | step 1 | loss: 0.0018996899183808791\n",
      "epoch 9 | step 2 | loss: 0.002727889247607332\n",
      "epoch 9 | step 3 | loss: 0.0036210029300329403\n",
      "epoch 9 | step 4 | loss: 0.004519221076146799\n",
      "epoch 9 | step 5 | loss: 0.005412282973252659\n",
      "epoch 9 | step 6 | loss: 0.006178203130593391\n",
      "epoch 9 | step 7 | loss: 0.0070231781674861635\n",
      "epoch 9 | step 8 | loss: 0.007845279949581827\n",
      "epoch 9 | step 9 | loss: 0.008614740671178452\n",
      "epoch 9 | step 10 | loss: 0.009508351574110313\n",
      "epoch 9 | step 11 | loss: 0.01032863375872111\n",
      "epoch 10 | step 0 | loss: 0.0008009794838518325\n",
      "epoch 10 | step 1 | loss: 0.0015443619555364316\n",
      "epoch 10 | step 2 | loss: 0.0022655317270495316\n",
      "epoch 10 | step 3 | loss: 0.0030272320453957656\n",
      "epoch 10 | step 4 | loss: 0.003800563501724167\n",
      "epoch 10 | step 5 | loss: 0.004528476392544038\n",
      "epoch 10 | step 6 | loss: 0.005242332584635702\n",
      "epoch 10 | step 7 | loss: 0.006037324586696686\n",
      "epoch 10 | step 8 | loss: 0.006776519879236322\n",
      "epoch 10 | step 9 | loss: 0.0074590836714384795\n",
      "epoch 10 | step 10 | loss: 0.008167927697104952\n",
      "epoch 10 | step 11 | loss: 0.00881698049107087\n",
      "epoch 11 | step 0 | loss: 0.0006549536687427011\n",
      "epoch 11 | step 1 | loss: 0.0014138489385804914\n",
      "epoch 11 | step 2 | loss: 0.0021802883835073095\n",
      "epoch 11 | step 3 | loss: 0.002863832605512128\n",
      "epoch 11 | step 4 | loss: 0.0035494173554838303\n",
      "epoch 11 | step 5 | loss: 0.004176159266754184\n",
      "epoch 11 | step 6 | loss: 0.004791845770356216\n",
      "epoch 11 | step 7 | loss: 0.005525548162070597\n",
      "epoch 11 | step 8 | loss: 0.006169926125980722\n",
      "epoch 11 | step 9 | loss: 0.006795000986683931\n",
      "epoch 11 | step 10 | loss: 0.007390775575473475\n",
      "epoch 11 | step 11 | loss: 0.008034095816224815\n",
      "epoch 12 | step 0 | loss: 0.0006558180601656295\n",
      "epoch 12 | step 1 | loss: 0.0012365640478992496\n",
      "epoch 12 | step 2 | loss: 0.001832082651237125\n",
      "epoch 12 | step 3 | loss: 0.002441286607596268\n",
      "epoch 12 | step 4 | loss: 0.003061298794738124\n",
      "epoch 12 | step 5 | loss: 0.0036615552559828173\n",
      "epoch 12 | step 6 | loss: 0.004365642793497124\n",
      "epoch 12 | step 7 | loss: 0.004962427059983825\n",
      "epoch 12 | step 8 | loss: 0.005602735760772884\n",
      "epoch 12 | step 9 | loss: 0.006214098752833011\n",
      "epoch 12 | step 10 | loss: 0.006863752402091744\n",
      "epoch 12 | step 11 | loss: 0.007534512659169072\n",
      "epoch 13 | step 0 | loss: 0.0006332595535369851\n",
      "epoch 13 | step 1 | loss: 0.0012594340874729124\n",
      "epoch 13 | step 2 | loss: 0.0019230987104806095\n",
      "epoch 13 | step 3 | loss: 0.002481213446133632\n",
      "epoch 13 | step 4 | loss: 0.0031359390820991017\n",
      "epoch 13 | step 5 | loss: 0.0036920196950876493\n",
      "epoch 13 | step 6 | loss: 0.004323623976233616\n",
      "epoch 13 | step 7 | loss: 0.004918225120034992\n",
      "epoch 13 | step 8 | loss: 0.005471226168782158\n",
      "epoch 13 | step 9 | loss: 0.00607413147368012\n",
      "epoch 13 | step 10 | loss: 0.006609391417081498\n",
      "epoch 13 | step 11 | loss: 0.007143987232646256\n",
      "epoch 14 | step 0 | loss: 0.0005622660032749778\n",
      "epoch 14 | step 1 | loss: 0.0010994238474071407\n",
      "epoch 14 | step 2 | loss: 0.0016497244884334898\n",
      "epoch 14 | step 3 | loss: 0.0022141641896231143\n",
      "epoch 14 | step 4 | loss: 0.0028083254609300758\n",
      "epoch 14 | step 5 | loss: 0.0033638075301801506\n",
      "epoch 14 | step 6 | loss: 0.003866848648135276\n",
      "epoch 14 | step 7 | loss: 0.004452605621044239\n",
      "epoch 14 | step 8 | loss: 0.005037371445293556\n",
      "epoch 14 | step 9 | loss: 0.00559314825432898\n",
      "epoch 14 | step 10 | loss: 0.006150893437931152\n",
      "epoch 14 | step 11 | loss: 0.006738229550688026\n",
      "epoch 15 | step 0 | loss: 0.0005322814268949669\n",
      "epoch 15 | step 1 | loss: 0.0010773831165237422\n",
      "epoch 15 | step 2 | loss: 0.0016575599371153392\n",
      "epoch 15 | step 3 | loss: 0.0021908585908215704\n",
      "epoch 15 | step 4 | loss: 0.0027527302675229063\n",
      "epoch 15 | step 5 | loss: 0.003308841983306242\n",
      "epoch 15 | step 6 | loss: 0.003850427026770833\n",
      "epoch 15 | step 7 | loss: 0.004417713278337327\n",
      "epoch 15 | step 8 | loss: 0.004958353914061213\n",
      "epoch 15 | step 9 | loss: 0.005591147393018644\n",
      "epoch 15 | step 10 | loss: 0.006092033817980112\n",
      "epoch 15 | step 11 | loss: 0.0065795490028332446\n",
      "epoch 16 | step 0 | loss: 0.0005024629579640942\n",
      "epoch 16 | step 1 | loss: 0.0010309997003297264\n",
      "epoch 16 | step 2 | loss: 0.0015461733496475482\n",
      "epoch 16 | step 3 | loss: 0.0021489354360972913\n",
      "epoch 16 | step 4 | loss: 0.0026777308816392596\n",
      "epoch 16 | step 5 | loss: 0.003221861602775372\n",
      "epoch 16 | step 6 | loss: 0.0037424613948483323\n",
      "epoch 16 | step 7 | loss: 0.004217265947160787\n",
      "epoch 16 | step 8 | loss: 0.004762294123452693\n",
      "epoch 16 | step 9 | loss: 0.0052549107041849966\n",
      "epoch 16 | step 10 | loss: 0.005801195391630145\n",
      "epoch 16 | step 11 | loss: 0.006349864289250567\n",
      "epoch 17 | step 0 | loss: 0.0004646324040732128\n",
      "epoch 17 | step 1 | loss: 0.0010237992097202977\n",
      "epoch 17 | step 2 | loss: 0.0014977818168961377\n",
      "epoch 17 | step 3 | loss: 0.0019927024823211724\n",
      "epoch 17 | step 4 | loss: 0.0025060829250255533\n",
      "epoch 17 | step 5 | loss: 0.003047508730268028\n",
      "epoch 17 | step 6 | loss: 0.0035956776259289795\n",
      "epoch 17 | step 7 | loss: 0.004094415010930412\n",
      "epoch 17 | step 8 | loss: 0.004649604685592437\n",
      "epoch 17 | step 9 | loss: 0.0051707503119960705\n",
      "epoch 17 | step 10 | loss: 0.005707243113477032\n",
      "epoch 17 | step 11 | loss: 0.00624325304922269\n",
      "epoch 18 | step 0 | loss: 0.0004986706390602746\n",
      "epoch 18 | step 1 | loss: 0.0009830780488848463\n",
      "epoch 18 | step 2 | loss: 0.0015655468591201402\n",
      "epoch 18 | step 3 | loss: 0.002028027742849751\n",
      "epoch 18 | step 4 | loss: 0.0025260649513246746\n",
      "epoch 18 | step 5 | loss: 0.0030310073828377733\n",
      "epoch 18 | step 6 | loss: 0.003566255990984852\n",
      "epoch 18 | step 7 | loss: 0.004074271204088417\n",
      "epoch 18 | step 8 | loss: 0.0045721558547213124\n",
      "epoch 18 | step 9 | loss: 0.005112937205200438\n",
      "epoch 18 | step 10 | loss: 0.005609346042927974\n",
      "epoch 18 | step 11 | loss: 0.0060988839149376155\n",
      "epoch 19 | step 0 | loss: 0.000489354801378875\n",
      "epoch 19 | step 1 | loss: 0.0009787481223605995\n",
      "epoch 19 | step 2 | loss: 0.0015022243638273993\n",
      "epoch 19 | step 3 | loss: 0.001974676063675911\n",
      "epoch 19 | step 4 | loss: 0.002471036000215095\n",
      "epoch 19 | step 5 | loss: 0.0029879984685914442\n",
      "epoch 19 | step 6 | loss: 0.0034752651350113553\n",
      "epoch 19 | step 7 | loss: 0.003941234693416332\n",
      "epoch 19 | step 8 | loss: 0.004423291732174835\n",
      "epoch 19 | step 9 | loss: 0.004896161091355383\n",
      "epoch 19 | step 10 | loss: 0.005363147894043496\n",
      "epoch 19 | step 11 | loss: 0.005838625507922765\n",
      "epoch 20 | step 0 | loss: 0.0004418727735899967\n",
      "epoch 20 | step 1 | loss: 0.0009317756706652095\n",
      "epoch 20 | step 2 | loss: 0.0014323911158264668\n",
      "epoch 20 | step 3 | loss: 0.0019118747765349635\n",
      "epoch 20 | step 4 | loss: 0.0023694254190703514\n",
      "epoch 20 | step 5 | loss: 0.002815248438356326\n",
      "epoch 20 | step 6 | loss: 0.0032845370523344936\n",
      "epoch 20 | step 7 | loss: 0.0037540460145630192\n",
      "epoch 20 | step 8 | loss: 0.004218735142264739\n",
      "epoch 20 | step 9 | loss: 0.0046410488915852545\n",
      "epoch 20 | step 10 | loss: 0.005108150461507801\n",
      "epoch 20 | step 11 | loss: 0.005534975423307168\n",
      "epoch 21 | step 0 | loss: 0.0005037195425186397\n",
      "epoch 21 | step 1 | loss: 0.0009036116052152335\n",
      "epoch 21 | step 2 | loss: 0.0013809326502324155\n",
      "epoch 21 | step 3 | loss: 0.0018029129331992226\n",
      "epoch 21 | step 4 | loss: 0.0022508406350952625\n",
      "epoch 21 | step 5 | loss: 0.002729471881292967\n",
      "epoch 21 | step 6 | loss: 0.003172023590204657\n",
      "epoch 21 | step 7 | loss: 0.0035844926171565094\n",
      "epoch 21 | step 8 | loss: 0.004024588189062943\n",
      "epoch 21 | step 9 | loss: 0.004457037950369128\n",
      "epoch 21 | step 10 | loss: 0.004872831415542558\n",
      "epoch 21 | step 11 | loss: 0.005311431787003643\n",
      "epoch 22 | step 0 | loss: 0.00040907479247857955\n",
      "epoch 22 | step 1 | loss: 0.0008363170034141462\n",
      "epoch 22 | step 2 | loss: 0.001300541228255936\n",
      "epoch 22 | step 3 | loss: 0.0016902773419186202\n",
      "epoch 22 | step 4 | loss: 0.0020987235631535887\n",
      "epoch 22 | step 5 | loss: 0.002502490249936118\n",
      "epoch 22 | step 6 | loss: 0.0029836484600503227\n",
      "epoch 22 | step 7 | loss: 0.003397117567633786\n",
      "epoch 22 | step 8 | loss: 0.0037992964514369496\n",
      "epoch 22 | step 9 | loss: 0.004215370713062974\n",
      "epoch 22 | step 10 | loss: 0.004621414264227099\n",
      "epoch 22 | step 11 | loss: 0.004990936423536917\n",
      "epoch 23 | step 0 | loss: 0.00039811682405051246\n",
      "epoch 23 | step 1 | loss: 0.0007689473159888558\n",
      "epoch 23 | step 2 | loss: 0.0011487431067569766\n",
      "epoch 23 | step 3 | loss: 0.0015773717158983583\n",
      "epoch 23 | step 4 | loss: 0.0019710293700771126\n",
      "epoch 23 | step 5 | loss: 0.0024235238790901196\n",
      "epoch 23 | step 6 | loss: 0.002818527037614967\n",
      "epoch 23 | step 7 | loss: 0.0032309674317165574\n",
      "epoch 23 | step 8 | loss: 0.003631101104332139\n",
      "epoch 23 | step 9 | loss: 0.004010896799797691\n",
      "epoch 23 | step 10 | loss: 0.004462598841008623\n",
      "epoch 23 | step 11 | loss: 0.004854689725076329\n",
      "epoch 24 | step 0 | loss: 0.0003930666273596555\n",
      "epoch 24 | step 1 | loss: 0.0007837402728152146\n",
      "epoch 24 | step 2 | loss: 0.001156470629072781\n",
      "epoch 24 | step 3 | loss: 0.0015564451937344916\n",
      "epoch 24 | step 4 | loss: 0.001943521463279392\n",
      "epoch 24 | step 5 | loss: 0.002363530680746872\n",
      "epoch 24 | step 6 | loss: 0.002768475019998501\n",
      "epoch 24 | step 7 | loss: 0.0031400183427092\n",
      "epoch 24 | step 8 | loss: 0.0035475507263554224\n",
      "epoch 24 | step 9 | loss: 0.003945299827424617\n",
      "epoch 24 | step 10 | loss: 0.004342142544912486\n",
      "epoch 24 | step 11 | loss: 0.004691165287551885\n",
      "epoch 25 | step 0 | loss: 0.0004214276534684031\n",
      "epoch 25 | step 1 | loss: 0.0008045390247399279\n",
      "epoch 25 | step 2 | loss: 0.0012022171594115892\n",
      "epoch 25 | step 3 | loss: 0.0015755900586606956\n",
      "epoch 25 | step 4 | loss: 0.001968244364312275\n",
      "epoch 25 | step 5 | loss: 0.0023791942891523816\n",
      "epoch 25 | step 6 | loss: 0.0027584481441410495\n",
      "epoch 25 | step 7 | loss: 0.0031250799410790634\n",
      "epoch 25 | step 8 | loss: 0.0034646731013466027\n",
      "epoch 25 | step 9 | loss: 0.003864422945344683\n",
      "epoch 25 | step 10 | loss: 0.004233537873017213\n",
      "epoch 25 | step 11 | loss: 0.004622250849750266\n",
      "epoch 26 | step 0 | loss: 0.00036486193109138353\n",
      "epoch 26 | step 1 | loss: 0.0008178547279819682\n",
      "epoch 26 | step 2 | loss: 0.0011871629742674589\n",
      "epoch 26 | step 3 | loss: 0.001545464558656571\n",
      "epoch 26 | step 4 | loss: 0.0019105153148240132\n",
      "epoch 26 | step 5 | loss: 0.002265567837892676\n",
      "epoch 26 | step 6 | loss: 0.002655556505488577\n",
      "epoch 26 | step 7 | loss: 0.0030663929982229715\n",
      "epoch 26 | step 8 | loss: 0.003436467298588471\n",
      "epoch 26 | step 9 | loss: 0.003788398210447577\n",
      "epoch 26 | step 10 | loss: 0.004160064811132289\n",
      "epoch 26 | step 11 | loss: 0.00457177853296279\n",
      "epoch 27 | step 0 | loss: 0.00037202450390390606\n",
      "epoch 27 | step 1 | loss: 0.0007464957030309174\n",
      "epoch 27 | step 2 | loss: 0.0011160390459897297\n",
      "epoch 27 | step 3 | loss: 0.0014671122858570908\n",
      "epoch 27 | step 4 | loss: 0.0018424596648171007\n",
      "epoch 27 | step 5 | loss: 0.0022370396497139854\n",
      "epoch 27 | step 6 | loss: 0.002653551977702785\n",
      "epoch 27 | step 7 | loss: 0.0030372397517394864\n",
      "epoch 27 | step 8 | loss: 0.0034014472443871014\n",
      "epoch 27 | step 9 | loss: 0.0037368471318944495\n",
      "epoch 27 | step 10 | loss: 0.004127460809838807\n",
      "epoch 27 | step 11 | loss: 0.004507013397629396\n",
      "epoch 28 | step 0 | loss: 0.00038053388610550197\n",
      "epoch 28 | step 1 | loss: 0.0007468100554588514\n",
      "epoch 28 | step 2 | loss: 0.001136977793334477\n",
      "epoch 28 | step 3 | loss: 0.0015150538257774839\n",
      "epoch 28 | step 4 | loss: 0.001907462513162665\n",
      "epoch 28 | step 5 | loss: 0.0022648917720007227\n",
      "epoch 28 | step 6 | loss: 0.002625195439328479\n",
      "epoch 28 | step 7 | loss: 0.002990156913759448\n",
      "epoch 28 | step 8 | loss: 0.0033350747355373606\n",
      "epoch 28 | step 9 | loss: 0.003690641323891458\n",
      "epoch 28 | step 10 | loss: 0.004039534636064849\n",
      "epoch 28 | step 11 | loss: 0.004448972539233897\n",
      "epoch 29 | step 0 | loss: 0.00032609261124966394\n",
      "epoch 29 | step 1 | loss: 0.0006810001089968535\n",
      "epoch 29 | step 2 | loss: 0.0010807149763239804\n",
      "epoch 29 | step 3 | loss: 0.0014667424237055679\n",
      "epoch 29 | step 4 | loss: 0.0018380802222852939\n",
      "epoch 29 | step 5 | loss: 0.0022110003789829794\n",
      "epoch 29 | step 6 | loss: 0.0025720756169319075\n",
      "epoch 29 | step 7 | loss: 0.002915955280099799\n",
      "epoch 29 | step 8 | loss: 0.0032631806314455973\n",
      "epoch 29 | step 9 | loss: 0.003650649270443687\n",
      "epoch 29 | step 10 | loss: 0.004034161805363981\n",
      "epoch 29 | step 11 | loss: 0.0043584152184241625\n",
      "epoch 30 | step 0 | loss: 0.0003415716545325093\n",
      "epoch 30 | step 1 | loss: 0.0006829903994020167\n",
      "epoch 30 | step 2 | loss: 0.0010945819191108528\n",
      "epoch 30 | step 3 | loss: 0.001457724753180327\n",
      "epoch 30 | step 4 | loss: 0.0018169267838795944\n",
      "epoch 30 | step 5 | loss: 0.0021970331209698246\n",
      "epoch 30 | step 6 | loss: 0.002535385050622675\n",
      "epoch 30 | step 7 | loss: 0.002883153402706004\n",
      "epoch 30 | step 8 | loss: 0.0032243037454462416\n",
      "epoch 30 | step 9 | loss: 0.0035948446211699478\n",
      "epoch 30 | step 10 | loss: 0.003955280552579587\n",
      "epoch 30 | step 11 | loss: 0.004350806473724459\n",
      "epoch 31 | step 0 | loss: 0.00038471448915342206\n",
      "epoch 31 | step 1 | loss: 0.000727936406980202\n",
      "epoch 31 | step 2 | loss: 0.0010824430120388304\n",
      "epoch 31 | step 3 | loss: 0.0014508116041350174\n",
      "epoch 31 | step 4 | loss: 0.0017864011106411798\n",
      "epoch 31 | step 5 | loss: 0.0021692881946552543\n",
      "epoch 31 | step 6 | loss: 0.0025274992030946885\n",
      "epoch 31 | step 7 | loss: 0.002872103860227086\n",
      "epoch 31 | step 8 | loss: 0.003211734687840915\n",
      "epoch 31 | step 9 | loss: 0.0035814200690664237\n",
      "epoch 31 | step 10 | loss: 0.003906771436015028\n",
      "epoch 31 | step 11 | loss: 0.004286652335637244\n",
      "epoch 32 | step 0 | loss: 0.00036588966027584587\n",
      "epoch 32 | step 1 | loss: 0.0007025051678472317\n",
      "epoch 32 | step 2 | loss: 0.001066087747576118\n",
      "epoch 32 | step 3 | loss: 0.0014062163639949263\n",
      "epoch 32 | step 4 | loss: 0.0017597337136111106\n",
      "epoch 32 | step 5 | loss: 0.002130996859212721\n",
      "epoch 32 | step 6 | loss: 0.0024766218832303314\n",
      "epoch 32 | step 7 | loss: 0.002868615895543232\n",
      "epoch 32 | step 8 | loss: 0.003202590185395837\n",
      "epoch 32 | step 9 | loss: 0.0035526900903917447\n",
      "epoch 32 | step 10 | loss: 0.003926761563571708\n",
      "epoch 32 | step 11 | loss: 0.004256277214927606\n",
      "epoch 33 | step 0 | loss: 0.0003210810165592898\n",
      "epoch 33 | step 1 | loss: 0.000696168781161088\n",
      "epoch 33 | step 2 | loss: 0.0010492317239939\n",
      "epoch 33 | step 3 | loss: 0.0014289900429536677\n",
      "epoch 33 | step 4 | loss: 0.0017476383121230316\n",
      "epoch 33 | step 5 | loss: 0.002101624727506032\n",
      "epoch 33 | step 6 | loss: 0.0024482323689516534\n",
      "epoch 33 | step 7 | loss: 0.0028003151895534158\n",
      "epoch 33 | step 8 | loss: 0.00318371450762263\n",
      "epoch 33 | step 9 | loss: 0.003506346882755413\n",
      "epoch 33 | step 10 | loss: 0.0038770310511354383\n",
      "epoch 33 | step 11 | loss: 0.004187413507939485\n",
      "epoch 34 | step 0 | loss: 0.00034450512550667276\n",
      "epoch 34 | step 1 | loss: 0.0006887244721296275\n",
      "epoch 34 | step 2 | loss: 0.001042832629438733\n",
      "epoch 34 | step 3 | loss: 0.0013968950276472339\n",
      "epoch 34 | step 4 | loss: 0.0017327738558258893\n",
      "epoch 34 | step 5 | loss: 0.002065661112274085\n",
      "epoch 34 | step 6 | loss: 0.002413702424502018\n",
      "epoch 34 | step 7 | loss: 0.0027675993575927158\n",
      "epoch 34 | step 8 | loss: 0.0031184755146905344\n",
      "epoch 34 | step 9 | loss: 0.0034713942027344387\n",
      "epoch 34 | step 10 | loss: 0.003816570954731909\n",
      "epoch 34 | step 11 | loss: 0.0041373931929630315\n",
      "epoch 35 | step 0 | loss: 0.0003292239622166385\n",
      "epoch 35 | step 1 | loss: 0.0007028825071122189\n",
      "epoch 35 | step 2 | loss: 0.0010437102495234351\n",
      "epoch 35 | step 3 | loss: 0.0013825288967206186\n",
      "epoch 35 | step 4 | loss: 0.0016756696557679363\n",
      "epoch 35 | step 5 | loss: 0.0020472040783370694\n",
      "epoch 35 | step 6 | loss: 0.0023673828091418923\n",
      "epoch 35 | step 7 | loss: 0.0027091794981121014\n",
      "epoch 35 | step 8 | loss: 0.0030568499678329275\n",
      "epoch 35 | step 9 | loss: 0.0033983128036275028\n",
      "epoch 35 | step 10 | loss: 0.0037770772926669443\n",
      "epoch 35 | step 11 | loss: 0.0040741334154504785\n",
      "epoch 36 | step 0 | loss: 0.0003380620220297838\n",
      "epoch 36 | step 1 | loss: 0.0006865877221819666\n",
      "epoch 36 | step 2 | loss: 0.0010402667492721731\n",
      "epoch 36 | step 3 | loss: 0.001397281229752517\n",
      "epoch 36 | step 4 | loss: 0.0016978242935508064\n",
      "epoch 36 | step 5 | loss: 0.0020387199622314165\n",
      "epoch 36 | step 6 | loss: 0.002348209360207463\n",
      "epoch 36 | step 7 | loss: 0.002650095592448995\n",
      "epoch 36 | step 8 | loss: 0.003012916588837163\n",
      "epoch 36 | step 9 | loss: 0.0033431132627919544\n",
      "epoch 36 | step 10 | loss: 0.0036730614055751733\n",
      "epoch 36 | step 11 | loss: 0.004022058480979125\n",
      "epoch 37 | step 0 | loss: 0.00032543477026627865\n",
      "epoch 37 | step 1 | loss: 0.0006685106605002801\n",
      "epoch 37 | step 2 | loss: 0.0009832832911337876\n",
      "epoch 37 | step 3 | loss: 0.0013424946995016893\n",
      "epoch 37 | step 4 | loss: 0.0016912410493695879\n",
      "epoch 37 | step 5 | loss: 0.0020069257271263267\n",
      "epoch 37 | step 6 | loss: 0.00232816825068086\n",
      "epoch 37 | step 7 | loss: 0.0026759847100157547\n",
      "epoch 37 | step 8 | loss: 0.0029931256852814797\n",
      "epoch 37 | step 9 | loss: 0.0033103525627339015\n",
      "epoch 37 | step 10 | loss: 0.0036435939860845674\n",
      "epoch 37 | step 11 | loss: 0.0039778826547705246\n",
      "epoch 38 | step 0 | loss: 0.00035070451756617023\n",
      "epoch 38 | step 1 | loss: 0.0007140671266079323\n",
      "epoch 38 | step 2 | loss: 0.0010106157014874552\n",
      "epoch 38 | step 3 | loss: 0.0013389582943777658\n",
      "epoch 38 | step 4 | loss: 0.0016848621295857211\n",
      "epoch 38 | step 5 | loss: 0.0020315536249962355\n",
      "epoch 38 | step 6 | loss: 0.0023435475855908454\n",
      "epoch 38 | step 7 | loss: 0.0026569040973485275\n",
      "epoch 38 | step 8 | loss: 0.0029643077682948776\n",
      "epoch 38 | step 9 | loss: 0.003281779731476193\n",
      "epoch 38 | step 10 | loss: 0.0036217960382525045\n",
      "epoch 38 | step 11 | loss: 0.003944489122481114\n",
      "epoch 39 | step 0 | loss: 0.0002935750858450974\n",
      "epoch 39 | step 1 | loss: 0.0006164688938758644\n",
      "epoch 39 | step 2 | loss: 0.000939899477235278\n",
      "epoch 39 | step 3 | loss: 0.0012534814245637005\n",
      "epoch 39 | step 4 | loss: 0.0015848564501512717\n",
      "epoch 39 | step 5 | loss: 0.0019068754247161645\n",
      "epoch 39 | step 6 | loss: 0.0022293445299061366\n",
      "epoch 39 | step 7 | loss: 0.0025761407056233507\n",
      "epoch 39 | step 8 | loss: 0.0029145231865089242\n",
      "epoch 39 | step 9 | loss: 0.0032651335616115155\n",
      "epoch 39 | step 10 | loss: 0.003592528625830732\n",
      "epoch 39 | step 11 | loss: 0.003905488228548327\n",
      "epoch 40 | step 0 | loss: 0.0003256827884642784\n",
      "epoch 40 | step 1 | loss: 0.0006463916915054186\n",
      "epoch 40 | step 2 | loss: 0.0010046018303874636\n",
      "epoch 40 | step 3 | loss: 0.0013220856941951141\n",
      "epoch 40 | step 4 | loss: 0.0016246517309692187\n",
      "epoch 40 | step 5 | loss: 0.0019717284885201565\n",
      "epoch 40 | step 6 | loss: 0.002325979189511239\n",
      "epoch 40 | step 7 | loss: 0.002632208603225609\n",
      "epoch 40 | step 8 | loss: 0.0029485066580250963\n",
      "epoch 40 | step 9 | loss: 0.0032439146929919595\n",
      "epoch 40 | step 10 | loss: 0.003551872856742161\n",
      "epoch 40 | step 11 | loss: 0.0038820752885182106\n",
      "epoch 41 | step 0 | loss: 0.0003293008585357794\n",
      "epoch 41 | step 1 | loss: 0.0006871018661982706\n",
      "epoch 41 | step 2 | loss: 0.0010022776817459327\n",
      "epoch 41 | step 3 | loss: 0.0013480473305694863\n",
      "epoch 41 | step 4 | loss: 0.0016539378527649147\n",
      "epoch 41 | step 5 | loss: 0.0019832653803890125\n",
      "epoch 41 | step 6 | loss: 0.0022932306053938677\n",
      "epoch 41 | step 7 | loss: 0.002650358260520918\n",
      "epoch 41 | step 8 | loss: 0.0029777066059854746\n",
      "epoch 41 | step 9 | loss: 0.003289419372576105\n",
      "epoch 41 | step 10 | loss: 0.0035851367516441107\n",
      "epoch 41 | step 11 | loss: 0.0038717957645798204\n",
      "epoch 42 | step 0 | loss: 0.00031753167256142633\n",
      "epoch 42 | step 1 | loss: 0.000652366775554936\n",
      "epoch 42 | step 2 | loss: 0.0010012602361848025\n",
      "epoch 42 | step 3 | loss: 0.0013035102479551012\n",
      "epoch 42 | step 4 | loss: 0.0016306634163405988\n",
      "epoch 42 | step 5 | loss: 0.0019328701481597353\n",
      "epoch 42 | step 6 | loss: 0.00223433239294831\n",
      "epoch 42 | step 7 | loss: 0.0025670766297011317\n",
      "epoch 42 | step 8 | loss: 0.0028621596371780347\n",
      "epoch 42 | step 9 | loss: 0.0031628603265096296\n",
      "epoch 42 | step 10 | loss: 0.0035133675402224144\n",
      "epoch 42 | step 11 | loss: 0.003821431955890059\n",
      "epoch 43 | step 0 | loss: 0.00035087199678329945\n",
      "epoch 43 | step 1 | loss: 0.0006748587998220163\n",
      "epoch 43 | step 2 | loss: 0.000983344476086861\n",
      "epoch 43 | step 3 | loss: 0.0013086153994058057\n",
      "epoch 43 | step 4 | loss: 0.0016201679189192038\n",
      "epoch 43 | step 5 | loss: 0.0019326342373405798\n",
      "epoch 43 | step 6 | loss: 0.0022530456306570973\n",
      "epoch 43 | step 7 | loss: 0.002562410058347291\n",
      "epoch 43 | step 8 | loss: 0.0028829919187011\n",
      "epoch 43 | step 9 | loss: 0.003212136715561258\n",
      "epoch 43 | step 10 | loss: 0.0034836564389527313\n",
      "epoch 43 | step 11 | loss: 0.0038097457269895274\n",
      "epoch 44 | step 0 | loss: 0.0003147097834290979\n",
      "epoch 44 | step 1 | loss: 0.0006268640012889862\n",
      "epoch 44 | step 2 | loss: 0.0009415342732867875\n",
      "epoch 44 | step 3 | loss: 0.0012726121823392587\n",
      "epoch 44 | step 4 | loss: 0.0015777940792791538\n",
      "epoch 44 | step 5 | loss: 0.0019010254200185043\n",
      "epoch 44 | step 6 | loss: 0.0022248564214176615\n",
      "epoch 44 | step 7 | loss: 0.002539539679544135\n",
      "epoch 44 | step 8 | loss: 0.002858031637848068\n",
      "epoch 44 | step 9 | loss: 0.003172200322787784\n",
      "epoch 44 | step 10 | loss: 0.0035024372645995755\n",
      "epoch 44 | step 11 | loss: 0.003798726919683298\n",
      "epoch 45 | step 0 | loss: 0.000328942612903711\n",
      "epoch 45 | step 1 | loss: 0.0006414284669064549\n",
      "epoch 45 | step 2 | loss: 0.0009642694152450457\n",
      "epoch 45 | step 3 | loss: 0.0012799565191843956\n",
      "epoch 45 | step 4 | loss: 0.0016029873863865189\n",
      "epoch 45 | step 5 | loss: 0.0018995757841529278\n",
      "epoch 45 | step 6 | loss: 0.002212384612966896\n",
      "epoch 45 | step 7 | loss: 0.0025520393939955\n",
      "epoch 45 | step 8 | loss: 0.002867494514156569\n",
      "epoch 45 | step 9 | loss: 0.0031918414353841866\n",
      "epoch 45 | step 10 | loss: 0.003479273283536034\n",
      "epoch 45 | step 11 | loss: 0.0037875290832882413\n",
      "epoch 46 | step 0 | loss: 0.0003047029751974831\n",
      "epoch 46 | step 1 | loss: 0.0006006058179687572\n",
      "epoch 46 | step 2 | loss: 0.0009176288628543585\n",
      "epoch 46 | step 3 | loss: 0.0012421336609513447\n",
      "epoch 46 | step 4 | loss: 0.0015666847267609397\n",
      "epoch 46 | step 5 | loss: 0.0018546060493325554\n",
      "epoch 46 | step 6 | loss: 0.002157649598587364\n",
      "epoch 46 | step 7 | loss: 0.0024803944521224306\n",
      "epoch 46 | step 8 | loss: 0.0027936428041816564\n",
      "epoch 46 | step 9 | loss: 0.00311602982672891\n",
      "epoch 46 | step 10 | loss: 0.0034369170116371334\n",
      "epoch 46 | step 11 | loss: 0.0037539835458887593\n",
      "epoch 47 | step 0 | loss: 0.00028864993060276264\n",
      "epoch 47 | step 1 | loss: 0.0005920453436723769\n",
      "epoch 47 | step 2 | loss: 0.0008990564170438932\n",
      "epoch 47 | step 3 | loss: 0.0012158766931677763\n",
      "epoch 47 | step 4 | loss: 0.0015332136296993322\n",
      "epoch 47 | step 5 | loss: 0.0018623141830133767\n",
      "epoch 47 | step 6 | loss: 0.002155408056596902\n",
      "epoch 47 | step 7 | loss: 0.002465407100290723\n",
      "epoch 47 | step 8 | loss: 0.002775298792375388\n",
      "epoch 47 | step 9 | loss: 0.003069478444235328\n",
      "epoch 47 | step 10 | loss: 0.0034044593496579227\n",
      "epoch 47 | step 11 | loss: 0.00374076054781122\n",
      "epoch 48 | step 0 | loss: 0.00032488623250843675\n",
      "epoch 48 | step 1 | loss: 0.000638690499591916\n",
      "epoch 48 | step 2 | loss: 0.0009554578335314734\n",
      "epoch 48 | step 3 | loss: 0.0012657703149328567\n",
      "epoch 48 | step 4 | loss: 0.0015708528628380273\n",
      "epoch 48 | step 5 | loss: 0.0018684170503669255\n",
      "epoch 48 | step 6 | loss: 0.0021970969465000194\n",
      "epoch 48 | step 7 | loss: 0.0025079801510646703\n",
      "epoch 48 | step 8 | loss: 0.0028014573040864065\n",
      "epoch 48 | step 9 | loss: 0.00308268163255595\n",
      "epoch 48 | step 10 | loss: 0.0034014066305213695\n",
      "epoch 48 | step 11 | loss: 0.0037017581220301354\n",
      "epoch 49 | step 0 | loss: 0.00031383554408846313\n",
      "epoch 49 | step 1 | loss: 0.0006141613070298508\n",
      "epoch 49 | step 2 | loss: 0.0009199944706052091\n",
      "epoch 49 | step 3 | loss: 0.001237998388956354\n",
      "epoch 49 | step 4 | loss: 0.0015338756811938358\n",
      "epoch 49 | step 5 | loss: 0.0018697973045187382\n",
      "epoch 49 | step 6 | loss: 0.002157957174667776\n",
      "epoch 49 | step 7 | loss: 0.002467213279354021\n",
      "epoch 49 | step 8 | loss: 0.002776907368978112\n",
      "epoch 49 | step 9 | loss: 0.0030853444487929224\n",
      "epoch 49 | step 10 | loss: 0.003388433486588016\n",
      "epoch 49 | step 11 | loss: 0.0037014868216166544\n",
      "epoch 50 | step 0 | loss: 0.00029599900859750874\n",
      "epoch 50 | step 1 | loss: 0.0005902917027904852\n",
      "epoch 50 | step 2 | loss: 0.0008999078069784616\n",
      "epoch 50 | step 3 | loss: 0.001209621215581744\n",
      "epoch 50 | step 4 | loss: 0.0015378777058461953\n",
      "epoch 50 | step 5 | loss: 0.001840420565532013\n",
      "epoch 50 | step 6 | loss: 0.002122570081768544\n",
      "epoch 50 | step 7 | loss: 0.00241827747582436\n",
      "epoch 50 | step 8 | loss: 0.0027590519706674173\n",
      "epoch 50 | step 9 | loss: 0.003075745021531838\n",
      "epoch 50 | step 10 | loss: 0.0033666002361646503\n",
      "epoch 50 | step 11 | loss: 0.0037060414558102084\n",
      "epoch 51 | step 0 | loss: 0.00030769367605423917\n",
      "epoch 51 | step 1 | loss: 0.0006211770291068162\n",
      "epoch 51 | step 2 | loss: 0.0009020293134108164\n",
      "epoch 51 | step 3 | loss: 0.001229276911476523\n",
      "epoch 51 | step 4 | loss: 0.0015203176555746978\n",
      "epoch 51 | step 5 | loss: 0.001821151528701442\n",
      "epoch 51 | step 6 | loss: 0.002120150592756714\n",
      "epoch 51 | step 7 | loss: 0.0024604289788755875\n",
      "epoch 51 | step 8 | loss: 0.0027900062580102996\n",
      "epoch 51 | step 9 | loss: 0.003090329921534744\n",
      "epoch 51 | step 10 | loss: 0.0033959428715215076\n",
      "epoch 51 | step 11 | loss: 0.003703214381036746\n",
      "epoch 52 | step 0 | loss: 0.00030877849530674457\n",
      "epoch 52 | step 1 | loss: 0.000593518227883588\n",
      "epoch 52 | step 2 | loss: 0.0008825838211990146\n",
      "epoch 52 | step 3 | loss: 0.0012017262463702321\n",
      "epoch 52 | step 4 | loss: 0.00149421494926824\n",
      "epoch 52 | step 5 | loss: 0.0017983856422688898\n",
      "epoch 52 | step 6 | loss: 0.0021026040355903905\n",
      "epoch 52 | step 7 | loss: 0.002419281933446757\n",
      "epoch 52 | step 8 | loss: 0.0027236306037563543\n",
      "epoch 52 | step 9 | loss: 0.0030537832982086145\n",
      "epoch 52 | step 10 | loss: 0.0033672679628737297\n",
      "epoch 52 | step 11 | loss: 0.0036608701266216638\n",
      "epoch 53 | step 0 | loss: 0.00030736713425885665\n",
      "epoch 53 | step 1 | loss: 0.0006430093087363674\n",
      "epoch 53 | step 2 | loss: 0.0009443947340641775\n",
      "epoch 53 | step 3 | loss: 0.0012510110228286248\n",
      "epoch 53 | step 4 | loss: 0.0015761455472771964\n",
      "epoch 53 | step 5 | loss: 0.0018607227740088373\n",
      "epoch 53 | step 6 | loss: 0.0021682629350212638\n",
      "epoch 53 | step 7 | loss: 0.0024692364759906957\n",
      "epoch 53 | step 8 | loss: 0.0027857088273382942\n",
      "epoch 53 | step 9 | loss: 0.0030822310775060146\n",
      "epoch 53 | step 10 | loss: 0.0033530524790868323\n",
      "epoch 53 | step 11 | loss: 0.0036511300364106118\n",
      "epoch 54 | step 0 | loss: 0.0003060870336139343\n",
      "epoch 54 | step 1 | loss: 0.000632776992531398\n",
      "epoch 54 | step 2 | loss: 0.0009423201248283146\n",
      "epoch 54 | step 3 | loss: 0.0012356140004355226\n",
      "epoch 54 | step 4 | loss: 0.0015216675234259092\n",
      "epoch 54 | step 5 | loss: 0.0018156215742439678\n",
      "epoch 54 | step 6 | loss: 0.0021414273801723034\n",
      "epoch 54 | step 7 | loss: 0.002467073538003287\n",
      "epoch 54 | step 8 | loss: 0.0027725213544588\n",
      "epoch 54 | step 9 | loss: 0.0030675278226605547\n",
      "epoch 54 | step 10 | loss: 0.0033688825999977057\n",
      "epoch 54 | step 11 | loss: 0.0036516186168717934\n",
      "epoch 55 | step 0 | loss: 0.00030150641871089343\n",
      "epoch 55 | step 1 | loss: 0.0006340498428819522\n",
      "epoch 55 | step 2 | loss: 0.0009492952806841853\n",
      "epoch 55 | step 3 | loss: 0.0012648139573204066\n",
      "epoch 55 | step 4 | loss: 0.0016097028650534269\n",
      "epoch 55 | step 5 | loss: 0.0018989414048158036\n",
      "epoch 55 | step 6 | loss: 0.0021767750311796146\n",
      "epoch 55 | step 7 | loss: 0.0024684643914602068\n",
      "epoch 55 | step 8 | loss: 0.002779858148093917\n",
      "epoch 55 | step 9 | loss: 0.0030803822546644766\n",
      "epoch 55 | step 10 | loss: 0.0033457781014049836\n",
      "epoch 55 | step 11 | loss: 0.003669525153939598\n",
      "epoch 56 | step 0 | loss: 0.00027664237561738644\n",
      "epoch 56 | step 1 | loss: 0.0005802767241529728\n",
      "epoch 56 | step 2 | loss: 0.0008677590982693045\n",
      "epoch 56 | step 3 | loss: 0.0011890797566757008\n",
      "epoch 56 | step 4 | loss: 0.0014578885729420968\n",
      "epoch 56 | step 5 | loss: 0.0017836040161424195\n",
      "epoch 56 | step 6 | loss: 0.0020810556984345143\n",
      "epoch 56 | step 7 | loss: 0.0024226566779936808\n",
      "epoch 56 | step 8 | loss: 0.0027156168260045735\n",
      "epoch 56 | step 9 | loss: 0.0029896583562564944\n",
      "epoch 56 | step 10 | loss: 0.0033061512667690836\n",
      "epoch 56 | step 11 | loss: 0.0036434311241248076\n",
      "epoch 57 | step 0 | loss: 0.00028484645271424386\n",
      "epoch 57 | step 1 | loss: 0.0006226598500702482\n",
      "epoch 57 | step 2 | loss: 0.0009446164064885954\n",
      "epoch 57 | step 3 | loss: 0.0012495719547537475\n",
      "epoch 57 | step 4 | loss: 0.0015618587600063974\n",
      "epoch 57 | step 5 | loss: 0.0018567545930010282\n",
      "epoch 57 | step 6 | loss: 0.0021526965896695163\n",
      "epoch 57 | step 7 | loss: 0.0024319286821138466\n",
      "epoch 57 | step 8 | loss: 0.0027314903265234576\n",
      "epoch 57 | step 9 | loss: 0.0030118383456190965\n",
      "epoch 57 | step 10 | loss: 0.0033185438551454477\n",
      "epoch 57 | step 11 | loss: 0.0036531352988792816\n",
      "epoch 58 | step 0 | loss: 0.0002679773265275856\n",
      "epoch 58 | step 1 | loss: 0.0005508911537844309\n",
      "epoch 58 | step 2 | loss: 0.0008637979848371415\n",
      "epoch 58 | step 3 | loss: 0.001167291319394717\n",
      "epoch 58 | step 4 | loss: 0.0014803833507989754\n",
      "epoch 58 | step 5 | loss: 0.0017980879527516535\n",
      "epoch 58 | step 6 | loss: 0.0021165065734385377\n",
      "epoch 58 | step 7 | loss: 0.0024219505318469614\n",
      "epoch 58 | step 8 | loss: 0.0027028960930596158\n",
      "epoch 58 | step 9 | loss: 0.0030055664220526865\n",
      "epoch 58 | step 10 | loss: 0.0033319038750647977\n",
      "epoch 58 | step 11 | loss: 0.003646302504843367\n",
      "epoch 59 | step 0 | loss: 0.0002981180528324777\n",
      "epoch 59 | step 1 | loss: 0.0006241247011047874\n",
      "epoch 59 | step 2 | loss: 0.0009066795475489248\n",
      "epoch 59 | step 3 | loss: 0.00121419097407122\n",
      "epoch 59 | step 4 | loss: 0.001526261321721329\n",
      "epoch 59 | step 5 | loss: 0.0018131397270069161\n",
      "epoch 59 | step 6 | loss: 0.00213665081348209\n",
      "epoch 59 | step 7 | loss: 0.002421017571845206\n",
      "epoch 59 | step 8 | loss: 0.002705087438424266\n",
      "epoch 59 | step 9 | loss: 0.0029978428213474967\n",
      "epoch 59 | step 10 | loss: 0.0033024635912956963\n",
      "epoch 59 | step 11 | loss: 0.0036331397737699176\n",
      "epoch 60 | step 0 | loss: 0.0002998913099438598\n",
      "epoch 60 | step 1 | loss: 0.0006066310311360285\n",
      "epoch 60 | step 2 | loss: 0.0008836654622308111\n",
      "epoch 60 | step 3 | loss: 0.0011984913502537532\n",
      "epoch 60 | step 4 | loss: 0.00148914264645814\n",
      "epoch 60 | step 5 | loss: 0.0018039075502837975\n",
      "epoch 60 | step 6 | loss: 0.0021297204656737603\n",
      "epoch 60 | step 7 | loss: 0.0024202858958971103\n",
      "epoch 60 | step 8 | loss: 0.0027208754040217668\n",
      "epoch 60 | step 9 | loss: 0.00301293804246813\n",
      "epoch 60 | step 10 | loss: 0.0033066905286363336\n",
      "epoch 60 | step 11 | loss: 0.00360467459917358\n",
      "epoch 61 | step 0 | loss: 0.0003139358035263347\n",
      "epoch 61 | step 1 | loss: 0.0006162315666192839\n",
      "epoch 61 | step 2 | loss: 0.0009458984594706326\n",
      "epoch 61 | step 3 | loss: 0.0012703849396458852\n",
      "epoch 61 | step 4 | loss: 0.0015844074686095556\n",
      "epoch 61 | step 5 | loss: 0.001861343334814427\n",
      "epoch 61 | step 6 | loss: 0.0021321591074217033\n",
      "epoch 61 | step 7 | loss: 0.0023951152437951947\n",
      "epoch 61 | step 8 | loss: 0.002702561480108875\n",
      "epoch 61 | step 9 | loss: 0.0029896396324883695\n",
      "epoch 61 | step 10 | loss: 0.003296578042505354\n",
      "epoch 61 | step 11 | loss: 0.0036050129486057967\n",
      "epoch 62 | step 0 | loss: 0.000312952721969717\n",
      "epoch 62 | step 1 | loss: 0.0006082562005103129\n",
      "epoch 62 | step 2 | loss: 0.0009249500508381765\n",
      "epoch 62 | step 3 | loss: 0.0012125191040904012\n",
      "epoch 62 | step 4 | loss: 0.0015259071446711986\n",
      "epoch 62 | step 5 | loss: 0.00182084673583899\n",
      "epoch 62 | step 6 | loss: 0.002090684809804192\n",
      "epoch 62 | step 7 | loss: 0.002388108400534757\n",
      "epoch 62 | step 8 | loss: 0.002685818274168513\n",
      "epoch 62 | step 9 | loss: 0.002980535717348205\n",
      "epoch 62 | step 10 | loss: 0.0032856802152029054\n",
      "epoch 62 | step 11 | loss: 0.003603407703851523\n",
      "epoch 63 | step 0 | loss: 0.0002858732223440482\n",
      "epoch 63 | step 1 | loss: 0.0005735297842841854\n",
      "epoch 63 | step 2 | loss: 0.0008895201873021074\n",
      "epoch 63 | step 3 | loss: 0.0012116815640994694\n",
      "epoch 63 | step 4 | loss: 0.0014976504524302973\n",
      "epoch 63 | step 5 | loss: 0.0018115423175378495\n",
      "epoch 63 | step 6 | loss: 0.0021283061741142187\n",
      "epoch 63 | step 7 | loss: 0.002401201791762724\n",
      "epoch 63 | step 8 | loss: 0.0027105164640803875\n",
      "epoch 63 | step 9 | loss: 0.0030045972608441843\n",
      "epoch 63 | step 10 | loss: 0.003310508592831217\n",
      "epoch 63 | step 11 | loss: 0.003602742109607069\n",
      "epoch 64 | step 0 | loss: 0.0002873875331341715\n",
      "epoch 64 | step 1 | loss: 0.0005678010192252796\n",
      "epoch 64 | step 2 | loss: 0.0008847050511524724\n",
      "epoch 64 | step 3 | loss: 0.0011988422484333894\n",
      "epoch 64 | step 4 | loss: 0.0015095439929370188\n",
      "epoch 64 | step 5 | loss: 0.0017791646289679907\n",
      "epoch 64 | step 6 | loss: 0.0020844268319448033\n",
      "epoch 64 | step 7 | loss: 0.0023734236845194953\n",
      "epoch 64 | step 8 | loss: 0.0026763201000137337\n",
      "epoch 64 | step 9 | loss: 0.002965846010188433\n",
      "epoch 64 | step 10 | loss: 0.0032665751075844395\n",
      "epoch 64 | step 11 | loss: 0.0035945201705497574\n",
      "epoch 65 | step 0 | loss: 0.0003099634042458062\n",
      "epoch 65 | step 1 | loss: 0.0006188899224038987\n",
      "epoch 65 | step 2 | loss: 0.000920579560213756\n",
      "epoch 65 | step 3 | loss: 0.001214166390378349\n",
      "epoch 65 | step 4 | loss: 0.001525635736226942\n",
      "epoch 65 | step 5 | loss: 0.001803589646922962\n",
      "epoch 65 | step 6 | loss: 0.002118625540565277\n",
      "epoch 65 | step 7 | loss: 0.0023885798077489043\n",
      "epoch 65 | step 8 | loss: 0.002667709650525469\n",
      "epoch 65 | step 9 | loss: 0.0029626263743043808\n",
      "epoch 65 | step 10 | loss: 0.0032678227249350925\n",
      "epoch 65 | step 11 | loss: 0.0035820340853507937\n",
      "epoch 66 | step 0 | loss: 0.00026481871249998647\n",
      "epoch 66 | step 1 | loss: 0.0005571555331011144\n",
      "epoch 66 | step 2 | loss: 0.0008261541903404326\n",
      "epoch 66 | step 3 | loss: 0.0011613967937950896\n",
      "epoch 66 | step 4 | loss: 0.0014480384524238159\n",
      "epoch 66 | step 5 | loss: 0.0017241432335738578\n",
      "epoch 66 | step 6 | loss: 0.002022451136361292\n",
      "epoch 66 | step 7 | loss: 0.002304414791800759\n",
      "epoch 66 | step 8 | loss: 0.002608700191871775\n",
      "epoch 66 | step 9 | loss: 0.002921961017607346\n",
      "epoch 66 | step 10 | loss: 0.003229124899844813\n",
      "epoch 66 | step 11 | loss: 0.003583983430104172\n",
      "epoch 67 | step 0 | loss: 0.0003124664868140378\n",
      "epoch 67 | step 1 | loss: 0.0006331953466109166\n",
      "epoch 67 | step 2 | loss: 0.0009262850425544946\n",
      "epoch 67 | step 3 | loss: 0.0011992570614244356\n",
      "epoch 67 | step 4 | loss: 0.001505490292140715\n",
      "epoch 67 | step 5 | loss: 0.0017886137242600307\n",
      "epoch 67 | step 6 | loss: 0.002086981535317205\n",
      "epoch 67 | step 7 | loss: 0.00239848611123209\n",
      "epoch 67 | step 8 | loss: 0.0026992211088517322\n",
      "epoch 67 | step 9 | loss: 0.002987508188978645\n",
      "epoch 67 | step 10 | loss: 0.0032796736179354614\n",
      "epoch 67 | step 11 | loss: 0.003568752509324169\n",
      "epoch 68 | step 0 | loss: 0.00028324927919920327\n",
      "epoch 68 | step 1 | loss: 0.0005739328681860409\n",
      "epoch 68 | step 2 | loss: 0.0008509581757672744\n",
      "epoch 68 | step 3 | loss: 0.0012087180526853879\n",
      "epoch 68 | step 4 | loss: 0.0014877450035472746\n",
      "epoch 68 | step 5 | loss: 0.0017930916187587197\n",
      "epoch 68 | step 6 | loss: 0.002091539378802381\n",
      "epoch 68 | step 7 | loss: 0.0023751255931664063\n",
      "epoch 68 | step 8 | loss: 0.002660014611461599\n",
      "epoch 68 | step 9 | loss: 0.0029654263908691037\n",
      "epoch 68 | step 10 | loss: 0.003263358327627014\n",
      "epoch 68 | step 11 | loss: 0.003562497830933541\n",
      "epoch 69 | step 0 | loss: 0.00029723894599808067\n",
      "epoch 69 | step 1 | loss: 0.0006159350514430474\n",
      "epoch 69 | step 2 | loss: 0.0009329310877788462\n",
      "epoch 69 | step 3 | loss: 0.001223741678325356\n",
      "epoch 69 | step 4 | loss: 0.0015493171100913379\n",
      "epoch 69 | step 5 | loss: 0.0018613318637144746\n",
      "epoch 69 | step 6 | loss: 0.002175662264415022\n",
      "epoch 69 | step 7 | loss: 0.002452978039480249\n",
      "epoch 69 | step 8 | loss: 0.0027024389420820987\n",
      "epoch 69 | step 9 | loss: 0.0029866991796185798\n",
      "epoch 69 | step 10 | loss: 0.00326304027896118\n",
      "epoch 69 | step 11 | loss: 0.0035587728007337497\n",
      "epoch 70 | step 0 | loss: 0.00030788545749057123\n",
      "epoch 70 | step 1 | loss: 0.0005933880247687526\n",
      "epoch 70 | step 2 | loss: 0.0008962970000910241\n",
      "epoch 70 | step 3 | loss: 0.0012103262700468493\n",
      "epoch 70 | step 4 | loss: 0.0015119190119605975\n",
      "epoch 70 | step 5 | loss: 0.00176751676772791\n",
      "epoch 70 | step 6 | loss: 0.0020614155233829525\n",
      "epoch 70 | step 7 | loss: 0.0023462524605395167\n",
      "epoch 70 | step 8 | loss: 0.002678812647384038\n",
      "epoch 70 | step 9 | loss: 0.00296433803629126\n",
      "epoch 70 | step 10 | loss: 0.003262544384452748\n",
      "epoch 70 | step 11 | loss: 0.003556476791906825\n",
      "epoch 71 | step 0 | loss: 0.00029381870663933765\n",
      "epoch 71 | step 1 | loss: 0.0005875463999432064\n",
      "epoch 71 | step 2 | loss: 0.0008847000934763047\n",
      "epoch 71 | step 3 | loss: 0.0012161383578945934\n",
      "epoch 71 | step 4 | loss: 0.00150109594916037\n",
      "epoch 71 | step 5 | loss: 0.0017726206480707853\n",
      "epoch 71 | step 6 | loss: 0.0020475085596187294\n",
      "epoch 71 | step 7 | loss: 0.0023729549233909106\n",
      "epoch 71 | step 8 | loss: 0.0026731355921260614\n",
      "epoch 71 | step 9 | loss: 0.0029675440300389547\n",
      "epoch 71 | step 10 | loss: 0.0032699017961679505\n",
      "epoch 71 | step 11 | loss: 0.0035515155168111823\n",
      "epoch 72 | step 0 | loss: 0.0002893149238295094\n",
      "epoch 72 | step 1 | loss: 0.0005913547023826693\n",
      "epoch 72 | step 2 | loss: 0.0009033233742368159\n",
      "epoch 72 | step 3 | loss: 0.001173172805293775\n",
      "epoch 72 | step 4 | loss: 0.0014997078014890382\n",
      "epoch 72 | step 5 | loss: 0.0018066369363150699\n",
      "epoch 72 | step 6 | loss: 0.002109844452357687\n",
      "epoch 72 | step 7 | loss: 0.0024062611384766085\n",
      "epoch 72 | step 8 | loss: 0.0027011451991762548\n",
      "epoch 72 | step 9 | loss: 0.0029818981625907653\n",
      "epoch 72 | step 10 | loss: 0.003256147147361634\n",
      "epoch 72 | step 11 | loss: 0.0035592651523092557\n",
      "epoch 73 | step 0 | loss: 0.0002951035560723477\n",
      "epoch 73 | step 1 | loss: 0.0005948861292496518\n",
      "epoch 73 | step 2 | loss: 0.0008713900883804547\n",
      "epoch 73 | step 3 | loss: 0.0011726787612968597\n",
      "epoch 73 | step 4 | loss: 0.0014613027377772749\n",
      "epoch 73 | step 5 | loss: 0.0017536078860840708\n",
      "epoch 73 | step 6 | loss: 0.0020462759377680453\n",
      "epoch 73 | step 7 | loss: 0.0023511047933038745\n",
      "epoch 73 | step 8 | loss: 0.0026708887990735566\n",
      "epoch 73 | step 9 | loss: 0.0029411387946745296\n",
      "epoch 73 | step 10 | loss: 0.003253040872579277\n",
      "epoch 73 | step 11 | loss: 0.0035471177998839575\n",
      "epoch 74 | step 0 | loss: 0.0002871698841089835\n",
      "epoch 74 | step 1 | loss: 0.0005556341427796839\n",
      "epoch 74 | step 2 | loss: 0.0008256359720226026\n",
      "epoch 74 | step 3 | loss: 0.001130428908896815\n",
      "epoch 74 | step 4 | loss: 0.001420223829021994\n",
      "epoch 74 | step 5 | loss: 0.0017431613598816915\n",
      "epoch 74 | step 6 | loss: 0.0020420259506078917\n",
      "epoch 74 | step 7 | loss: 0.002324773093058316\n",
      "epoch 74 | step 8 | loss: 0.002645679152173627\n",
      "epoch 74 | step 9 | loss: 0.002934934253714081\n",
      "epoch 74 | step 10 | loss: 0.0032414866336645414\n",
      "epoch 74 | step 11 | loss: 0.003542181608271945\n",
      "epoch 75 | step 0 | loss: 0.00029925289448043564\n",
      "epoch 75 | step 1 | loss: 0.0006036740675035351\n",
      "epoch 75 | step 2 | loss: 0.0008698431004902524\n",
      "epoch 75 | step 3 | loss: 0.0011837109072041299\n",
      "epoch 75 | step 4 | loss: 0.0014699421947467824\n",
      "epoch 75 | step 5 | loss: 0.0017521779046073202\n",
      "epoch 75 | step 6 | loss: 0.002057157457556053\n",
      "epoch 75 | step 7 | loss: 0.002366839821314584\n",
      "epoch 75 | step 8 | loss: 0.0026541313169711643\n",
      "epoch 75 | step 9 | loss: 0.002956922011813127\n",
      "epoch 75 | step 10 | loss: 0.0032470979954966534\n",
      "epoch 75 | step 11 | loss: 0.003533798467882322\n",
      "epoch 76 | step 0 | loss: 0.0003038195852353857\n",
      "epoch 76 | step 1 | loss: 0.0005916837986251656\n",
      "epoch 76 | step 2 | loss: 0.000862038298758639\n",
      "epoch 76 | step 3 | loss: 0.0011399568293815484\n",
      "epoch 76 | step 4 | loss: 0.0014410364910733497\n",
      "epoch 76 | step 5 | loss: 0.0017390635445532286\n",
      "epoch 76 | step 6 | loss: 0.0020449370929634383\n",
      "epoch 76 | step 7 | loss: 0.002322726055382624\n",
      "epoch 76 | step 8 | loss: 0.0026180446032811125\n",
      "epoch 76 | step 9 | loss: 0.0029357424875769284\n",
      "epoch 76 | step 10 | loss: 0.003221973860607426\n",
      "epoch 76 | step 11 | loss: 0.003540198711519658\n",
      "epoch 77 | step 0 | loss: 0.00029371221776848083\n",
      "epoch 77 | step 1 | loss: 0.0005577837617373217\n",
      "epoch 77 | step 2 | loss: 0.0008534319360401057\n",
      "epoch 77 | step 3 | loss: 0.001148083055112807\n",
      "epoch 77 | step 4 | loss: 0.001441489232691076\n",
      "epoch 77 | step 5 | loss: 0.001757305089617097\n",
      "epoch 77 | step 6 | loss: 0.0020444517992660232\n",
      "epoch 77 | step 7 | loss: 0.0023330637749069657\n",
      "epoch 77 | step 8 | loss: 0.002645209334386736\n",
      "epoch 77 | step 9 | loss: 0.0029558496807233652\n",
      "epoch 77 | step 10 | loss: 0.003244118826132294\n",
      "epoch 77 | step 11 | loss: 0.0035308660739018097\n",
      "epoch 78 | step 0 | loss: 0.00028907715196515503\n",
      "epoch 78 | step 1 | loss: 0.0005666727495978226\n",
      "epoch 78 | step 2 | loss: 0.0008650510103500684\n",
      "epoch 78 | step 3 | loss: 0.00115047642085296\n",
      "epoch 78 | step 4 | loss: 0.0014521394967840586\n",
      "epoch 78 | step 5 | loss: 0.001730404132577485\n",
      "epoch 78 | step 6 | loss: 0.002043380380649182\n",
      "epoch 78 | step 7 | loss: 0.0023155108278574916\n",
      "epoch 78 | step 8 | loss: 0.0026161629118147053\n",
      "epoch 78 | step 9 | loss: 0.0029406733448116367\n",
      "epoch 78 | step 10 | loss: 0.0032251333504148085\n",
      "epoch 78 | step 11 | loss: 0.0035339978637879315\n",
      "epoch 79 | step 0 | loss: 0.0002909395124938768\n",
      "epoch 79 | step 1 | loss: 0.000602917013282005\n",
      "epoch 79 | step 2 | loss: 0.0008854704568117437\n",
      "epoch 79 | step 3 | loss: 0.001175684871683663\n",
      "epoch 79 | step 4 | loss: 0.0014638740555525005\n",
      "epoch 79 | step 5 | loss: 0.0017451363761305596\n",
      "epoch 79 | step 6 | loss: 0.0020526811770603818\n",
      "epoch 79 | step 7 | loss: 0.0023645651032420155\n",
      "epoch 79 | step 8 | loss: 0.0026427377394434683\n",
      "epoch 79 | step 9 | loss: 0.0029327046621336977\n",
      "epoch 79 | step 10 | loss: 0.0032407699933288854\n",
      "epoch 79 | step 11 | loss: 0.0035265961405740996\n",
      "epoch 80 | step 0 | loss: 0.00028622472727819515\n",
      "epoch 80 | step 1 | loss: 0.0005769735059810651\n",
      "epoch 80 | step 2 | loss: 0.0008675880214311178\n",
      "epoch 80 | step 3 | loss: 0.0011605491570849014\n",
      "epoch 80 | step 4 | loss: 0.0014540599220868662\n",
      "epoch 80 | step 5 | loss: 0.0017546695057590398\n",
      "epoch 80 | step 6 | loss: 0.002063383011036989\n",
      "epoch 80 | step 7 | loss: 0.0023450094802044677\n",
      "epoch 80 | step 8 | loss: 0.002637087911913083\n",
      "epoch 80 | step 9 | loss: 0.002940570866797036\n",
      "epoch 80 | step 10 | loss: 0.003242551288326024\n",
      "epoch 80 | step 11 | loss: 0.00352493632524754\n",
      "epoch 81 | step 0 | loss: 0.0003327158169788559\n",
      "epoch 81 | step 1 | loss: 0.0006296769177395736\n",
      "epoch 81 | step 2 | loss: 0.0009264171000681169\n",
      "epoch 81 | step 3 | loss: 0.0012119255090258963\n",
      "epoch 81 | step 4 | loss: 0.0014929929375653243\n",
      "epoch 81 | step 5 | loss: 0.001784399197166945\n",
      "epoch 81 | step 6 | loss: 0.0020721456492278946\n",
      "epoch 81 | step 7 | loss: 0.00235544336576305\n",
      "epoch 81 | step 8 | loss: 0.002631649549686123\n",
      "epoch 81 | step 9 | loss: 0.002931728103668901\n",
      "epoch 81 | step 10 | loss: 0.0032242052627785727\n",
      "epoch 81 | step 11 | loss: 0.0035287685888215565\n",
      "epoch 82 | step 0 | loss: 0.0002910687170327787\n",
      "epoch 82 | step 1 | loss: 0.0005847035833201316\n",
      "epoch 82 | step 2 | loss: 0.0008742542881581345\n",
      "epoch 82 | step 3 | loss: 0.0011368180303985872\n",
      "epoch 82 | step 4 | loss: 0.0014525021207537733\n",
      "epoch 82 | step 5 | loss: 0.0017315535981997337\n",
      "epoch 82 | step 6 | loss: 0.0020271064366087256\n",
      "epoch 82 | step 7 | loss: 0.0023267411720560823\n",
      "epoch 82 | step 8 | loss: 0.0026323793598681893\n",
      "epoch 82 | step 9 | loss: 0.002950443235213387\n",
      "epoch 82 | step 10 | loss: 0.0032443615337910637\n",
      "epoch 82 | step 11 | loss: 0.0035181525017746575\n",
      "epoch 83 | step 0 | loss: 0.0002839960910058724\n",
      "epoch 83 | step 1 | loss: 0.0005791417557328964\n",
      "epoch 83 | step 2 | loss: 0.0008488626583427279\n",
      "epoch 83 | step 3 | loss: 0.001158256356888885\n",
      "epoch 83 | step 4 | loss: 0.001450363345948546\n",
      "epoch 83 | step 5 | loss: 0.0017362456956304504\n",
      "epoch 83 | step 6 | loss: 0.0020152582334230775\n",
      "epoch 83 | step 7 | loss: 0.002343665336257005\n",
      "epoch 83 | step 8 | loss: 0.002620565675545929\n",
      "epoch 83 | step 9 | loss: 0.0029300083922437414\n",
      "epoch 83 | step 10 | loss: 0.0032133292914600666\n",
      "epoch 83 | step 11 | loss: 0.0035285751544221504\n",
      "epoch 84 | step 0 | loss: 0.0002728458405850261\n",
      "epoch 84 | step 1 | loss: 0.0006000915089357834\n",
      "epoch 84 | step 2 | loss: 0.0008881441675675816\n",
      "epoch 84 | step 3 | loss: 0.0012247118148929071\n",
      "epoch 84 | step 4 | loss: 0.0015194809162456875\n",
      "epoch 84 | step 5 | loss: 0.001804407675518388\n",
      "epoch 84 | step 6 | loss: 0.002082887620930178\n",
      "epoch 84 | step 7 | loss: 0.00236474898460969\n",
      "epoch 84 | step 8 | loss: 0.002688504522671094\n",
      "epoch 84 | step 9 | loss: 0.0029812859530177352\n",
      "epoch 84 | step 10 | loss: 0.003242210027164516\n",
      "epoch 84 | step 11 | loss: 0.0035170366867960552\n",
      "epoch 85 | step 0 | loss: 0.0003060749590930939\n",
      "epoch 85 | step 1 | loss: 0.0006035641304794736\n",
      "epoch 85 | step 2 | loss: 0.0008841428000836728\n",
      "epoch 85 | step 3 | loss: 0.0011802481476113832\n",
      "epoch 85 | step 4 | loss: 0.0014740292040501383\n",
      "epoch 85 | step 5 | loss: 0.001766378207859826\n",
      "epoch 85 | step 6 | loss: 0.0020523912996252974\n",
      "epoch 85 | step 7 | loss: 0.002360039941118022\n",
      "epoch 85 | step 8 | loss: 0.0026554563482058596\n",
      "epoch 85 | step 9 | loss: 0.002939510932244444\n",
      "epoch 85 | step 10 | loss: 0.0032479645484819143\n",
      "epoch 85 | step 11 | loss: 0.003514701616090781\n",
      "epoch 86 | step 0 | loss: 0.0002871036056168235\n",
      "epoch 86 | step 1 | loss: 0.0005541456605276114\n",
      "epoch 86 | step 2 | loss: 0.0008712583761625087\n",
      "epoch 86 | step 3 | loss: 0.0011609546448253512\n",
      "epoch 86 | step 4 | loss: 0.0014424257383582426\n",
      "epoch 86 | step 5 | loss: 0.001739603895220101\n",
      "epoch 86 | step 6 | loss: 0.0020322596149535866\n",
      "epoch 86 | step 7 | loss: 0.0023123732384469384\n",
      "epoch 86 | step 8 | loss: 0.0026382447679362295\n",
      "epoch 86 | step 9 | loss: 0.0029320818831276053\n",
      "epoch 86 | step 10 | loss: 0.0032172629372306206\n",
      "epoch 86 | step 11 | loss: 0.003526685223649398\n",
      "epoch 87 | step 0 | loss: 0.000284611954320844\n",
      "epoch 87 | step 1 | loss: 0.0006052888431545527\n",
      "epoch 87 | step 2 | loss: 0.0008876290851900906\n",
      "epoch 87 | step 3 | loss: 0.0011929803869377622\n",
      "epoch 87 | step 4 | loss: 0.0014994096869812745\n",
      "epoch 87 | step 5 | loss: 0.001800881288102032\n",
      "epoch 87 | step 6 | loss: 0.0020779204642552935\n",
      "epoch 87 | step 7 | loss: 0.0023525897720316985\n",
      "epoch 87 | step 8 | loss: 0.00261483016634841\n",
      "epoch 87 | step 9 | loss: 0.0029081679931579904\n",
      "epoch 87 | step 10 | loss: 0.0032341737837197635\n",
      "epoch 87 | step 11 | loss: 0.0035200468053416104\n",
      "epoch 88 | step 0 | loss: 0.00028549049833182145\n",
      "epoch 88 | step 1 | loss: 0.00054415223640369\n",
      "epoch 88 | step 2 | loss: 0.0008453114667507755\n",
      "epoch 88 | step 3 | loss: 0.0011237957137698105\n",
      "epoch 88 | step 4 | loss: 0.001428092145103979\n",
      "epoch 88 | step 5 | loss: 0.0017085503915261716\n",
      "epoch 88 | step 6 | loss: 0.002021879993575637\n",
      "epoch 88 | step 7 | loss: 0.002338106716876242\n",
      "epoch 88 | step 8 | loss: 0.0026434343611030967\n",
      "epoch 88 | step 9 | loss: 0.002933068011133967\n",
      "epoch 88 | step 10 | loss: 0.003236823599393211\n",
      "epoch 88 | step 11 | loss: 0.0035189732517882993\n",
      "epoch 89 | step 0 | loss: 0.00030668347748839244\n",
      "epoch 89 | step 1 | loss: 0.0006172672363243903\n",
      "epoch 89 | step 2 | loss: 0.0009030555283773445\n",
      "epoch 89 | step 3 | loss: 0.0012020819462919047\n",
      "epoch 89 | step 4 | loss: 0.0014776219184769461\n",
      "epoch 89 | step 5 | loss: 0.0017716406130254888\n",
      "epoch 89 | step 6 | loss: 0.002062098025464916\n",
      "epoch 89 | step 7 | loss: 0.0023562628677611702\n",
      "epoch 89 | step 8 | loss: 0.0026298174254625207\n",
      "epoch 89 | step 9 | loss: 0.002920949514775065\n",
      "epoch 89 | step 10 | loss: 0.003223518171822922\n",
      "epoch 89 | step 11 | loss: 0.003524136464564022\n",
      "epoch 90 | step 0 | loss: 0.0003043759411354968\n",
      "epoch 90 | step 1 | loss: 0.0005934169187523078\n",
      "epoch 90 | step 2 | loss: 0.0008813889997224747\n",
      "epoch 90 | step 3 | loss: 0.001180599675735557\n",
      "epoch 90 | step 4 | loss: 0.0014472735766404207\n",
      "epoch 90 | step 5 | loss: 0.001733333453814633\n",
      "epoch 90 | step 6 | loss: 0.002043742450882102\n",
      "epoch 90 | step 7 | loss: 0.002345509256874752\n",
      "epoch 90 | step 8 | loss: 0.00264202849148461\n",
      "epoch 90 | step 9 | loss: 0.00291798463955225\n",
      "epoch 90 | step 10 | loss: 0.0032062818130347706\n",
      "epoch 90 | step 11 | loss: 0.0035308651342173354\n",
      "epoch 91 | step 0 | loss: 0.0003002051136688591\n",
      "epoch 91 | step 1 | loss: 0.0006079740988818132\n",
      "epoch 91 | step 2 | loss: 0.000872639214511603\n",
      "epoch 91 | step 3 | loss: 0.0011640343542025312\n",
      "epoch 91 | step 4 | loss: 0.0014537542836343988\n",
      "epoch 91 | step 5 | loss: 0.0017497110876704123\n",
      "epoch 91 | step 6 | loss: 0.002068975735390035\n",
      "epoch 91 | step 7 | loss: 0.002342633457334652\n",
      "epoch 91 | step 8 | loss: 0.002628623340559573\n",
      "epoch 91 | step 9 | loss: 0.00291800022621021\n",
      "epoch 91 | step 10 | loss: 0.003229861647976111\n",
      "epoch 91 | step 11 | loss: 0.003521656196899475\n",
      "epoch 92 | step 0 | loss: 0.0002872921060202318\n",
      "epoch 92 | step 1 | loss: 0.0005582671305771161\n",
      "epoch 92 | step 2 | loss: 0.0008785363151915662\n",
      "epoch 92 | step 3 | loss: 0.0011619540326460922\n",
      "epoch 92 | step 4 | loss: 0.0014925538358902546\n",
      "epoch 92 | step 5 | loss: 0.0017784573069596552\n",
      "epoch 92 | step 6 | loss: 0.0020862965305223284\n",
      "epoch 92 | step 7 | loss: 0.0023999512488988113\n",
      "epoch 92 | step 8 | loss: 0.0026948800595019027\n",
      "epoch 92 | step 9 | loss: 0.0029509483627387987\n",
      "epoch 92 | step 10 | loss: 0.003238353472191513\n",
      "epoch 92 | step 11 | loss: 0.003518308576119296\n",
      "epoch 93 | step 0 | loss: 0.0002856501720273277\n",
      "epoch 93 | step 1 | loss: 0.0005812422621625003\n",
      "epoch 93 | step 2 | loss: 0.0008805568397956785\n",
      "epoch 93 | step 3 | loss: 0.0011992282607468097\n",
      "epoch 93 | step 4 | loss: 0.0014912356016429742\n",
      "epoch 93 | step 5 | loss: 0.0017913991510647817\n",
      "epoch 93 | step 6 | loss: 0.0020531355559140632\n",
      "epoch 93 | step 7 | loss: 0.0023467037548098304\n",
      "epoch 93 | step 8 | loss: 0.002614212143692179\n",
      "epoch 93 | step 9 | loss: 0.002919964707208498\n",
      "epoch 93 | step 10 | loss: 0.0032180123372494624\n",
      "epoch 93 | step 11 | loss: 0.003526253996250562\n",
      "epoch 94 | step 0 | loss: 0.0003004253194886262\n",
      "epoch 94 | step 1 | loss: 0.0006086248756833236\n",
      "epoch 94 | step 2 | loss: 0.0009132634330243364\n",
      "epoch 94 | step 3 | loss: 0.0011918047211825018\n",
      "epoch 94 | step 4 | loss: 0.0014917820886187112\n",
      "epoch 94 | step 5 | loss: 0.0017795677029920308\n",
      "epoch 94 | step 6 | loss: 0.0020787301915057006\n",
      "epoch 94 | step 7 | loss: 0.002380735199746625\n",
      "epoch 94 | step 8 | loss: 0.0026883206771319062\n",
      "epoch 94 | step 9 | loss: 0.0029793805149371105\n",
      "epoch 94 | step 10 | loss: 0.003253192843184308\n",
      "epoch 94 | step 11 | loss: 0.0035124471652794574\n",
      "epoch 95 | step 0 | loss: 0.00026961186058442805\n",
      "epoch 95 | step 1 | loss: 0.0005459847332268805\n",
      "epoch 95 | step 2 | loss: 0.0008134576183089635\n",
      "epoch 95 | step 3 | loss: 0.0011161742099168735\n",
      "epoch 95 | step 4 | loss: 0.0014220663551894205\n",
      "epoch 95 | step 5 | loss: 0.00171495207318672\n",
      "epoch 95 | step 6 | loss: 0.0020198259830175743\n",
      "epoch 95 | step 7 | loss: 0.0023369256495991003\n",
      "epoch 95 | step 8 | loss: 0.0026173343519046736\n",
      "epoch 95 | step 9 | loss: 0.002918292789265315\n",
      "epoch 95 | step 10 | loss: 0.0032228329619058\n",
      "epoch 95 | step 11 | loss: 0.003524298909365157\n",
      "epoch 96 | step 0 | loss: 0.00028131737625190816\n",
      "epoch 96 | step 1 | loss: 0.0005679203217495464\n",
      "epoch 96 | step 2 | loss: 0.000861743194265864\n",
      "epoch 96 | step 3 | loss: 0.0011372514332227061\n",
      "epoch 96 | step 4 | loss: 0.0014372296500292608\n",
      "epoch 96 | step 5 | loss: 0.0017374472074613099\n",
      "epoch 96 | step 6 | loss: 0.0020419290901411845\n",
      "epoch 96 | step 7 | loss: 0.0023612139231563127\n",
      "epoch 96 | step 8 | loss: 0.002674846293099055\n",
      "epoch 96 | step 9 | loss: 0.0029464594070964046\n",
      "epoch 96 | step 10 | loss: 0.0032450799795210187\n",
      "epoch 96 | step 11 | loss: 0.0035155932391087025\n",
      "epoch 97 | step 0 | loss: 0.00028093194824069955\n",
      "epoch 97 | step 1 | loss: 0.0005559952104417817\n",
      "epoch 97 | step 2 | loss: 0.0008640206334620509\n",
      "epoch 97 | step 3 | loss: 0.001171677701747809\n",
      "epoch 97 | step 4 | loss: 0.00146330101016713\n",
      "epoch 97 | step 5 | loss: 0.0017623022444906169\n",
      "epoch 97 | step 6 | loss: 0.002055417083602718\n",
      "epoch 97 | step 7 | loss: 0.002337251207437955\n",
      "epoch 97 | step 8 | loss: 0.00262837978502568\n",
      "epoch 97 | step 9 | loss: 0.0029320343512421855\n",
      "epoch 97 | step 10 | loss: 0.0032137510658015586\n",
      "epoch 97 | step 11 | loss: 0.0035278199377801016\n",
      "epoch 98 | step 0 | loss: 0.00026406337315874526\n",
      "epoch 98 | step 1 | loss: 0.0005628565209503366\n",
      "epoch 98 | step 2 | loss: 0.0008508631358375964\n",
      "epoch 98 | step 3 | loss: 0.0011697446667451304\n",
      "epoch 98 | step 4 | loss: 0.001439364091422784\n",
      "epoch 98 | step 5 | loss: 0.0017344807574449402\n",
      "epoch 98 | step 6 | loss: 0.0020220595169484325\n",
      "epoch 98 | step 7 | loss: 0.002312561377229572\n",
      "epoch 98 | step 8 | loss: 0.0026096054659525904\n",
      "epoch 98 | step 9 | loss: 0.0029247892104826484\n",
      "epoch 98 | step 10 | loss: 0.003223617949353949\n",
      "epoch 98 | step 11 | loss: 0.003523966687610171\n",
      "epoch 99 | step 0 | loss: 0.0002925595932258849\n",
      "epoch 99 | step 1 | loss: 0.0005751746258752685\n",
      "epoch 99 | step 2 | loss: 0.0008751577468551666\n",
      "epoch 99 | step 3 | loss: 0.0011637718222230697\n",
      "epoch 99 | step 4 | loss: 0.0014481316034115185\n",
      "epoch 99 | step 5 | loss: 0.0017218498935236478\n",
      "epoch 99 | step 6 | loss: 0.002010410592979993\n",
      "epoch 99 | step 7 | loss: 0.0022991978926846183\n",
      "epoch 99 | step 8 | loss: 0.0025853732312956013\n",
      "epoch 99 | step 9 | loss: 0.0028922890709522585\n",
      "epoch 99 | step 10 | loss: 0.0032044458708878607\n",
      "epoch 99 | step 11 | loss: 0.0035314066287752952\n",
      "epoch 100 | step 0 | loss: 0.00028901697755975424\n",
      "epoch 100 | step 1 | loss: 0.0005989435408425795\n",
      "epoch 100 | step 2 | loss: 0.0008801523307723795\n",
      "epoch 100 | step 3 | loss: 0.0011719647550436554\n",
      "epoch 100 | step 4 | loss: 0.0014547001584108638\n",
      "epoch 100 | step 5 | loss: 0.0017694193257816956\n",
      "epoch 100 | step 6 | loss: 0.002070186020216524\n",
      "epoch 100 | step 7 | loss: 0.0023563558060131365\n",
      "epoch 100 | step 8 | loss: 0.002653759228417843\n",
      "epoch 100 | step 9 | loss: 0.0029618239301943275\n",
      "epoch 100 | step 10 | loss: 0.003223883802836168\n",
      "epoch 100 | step 11 | loss: 0.0035237930247047607\n",
      "epoch 101 | step 0 | loss: 0.0003082253325328075\n",
      "epoch 101 | step 1 | loss: 0.0006081248505283457\n",
      "epoch 101 | step 2 | loss: 0.0009115240279105863\n",
      "epoch 101 | step 3 | loss: 0.0012063185883260442\n",
      "epoch 101 | step 4 | loss: 0.0015156099949316455\n",
      "epoch 101 | step 5 | loss: 0.0018089561023845943\n",
      "epoch 101 | step 6 | loss: 0.002080714143474467\n",
      "epoch 101 | step 7 | loss: 0.002375734232671618\n",
      "epoch 101 | step 8 | loss: 0.0026717376400750984\n",
      "epoch 101 | step 9 | loss: 0.0029464299089479646\n",
      "epoch 101 | step 10 | loss: 0.0032208164614497973\n",
      "epoch 101 | step 11 | loss: 0.0035250172715213684\n",
      "epoch 102 | step 0 | loss: 0.0002873158945262276\n",
      "epoch 102 | step 1 | loss: 0.0005907886884316956\n",
      "epoch 102 | step 2 | loss: 0.0009203919485472757\n",
      "epoch 102 | step 3 | loss: 0.0012069841416108627\n",
      "epoch 102 | step 4 | loss: 0.001486535149616143\n",
      "epoch 102 | step 5 | loss: 0.0017711811939264983\n",
      "epoch 102 | step 6 | loss: 0.0020616779329749525\n",
      "epoch 102 | step 7 | loss: 0.0023726399030343705\n",
      "epoch 102 | step 8 | loss: 0.0026417445663475883\n",
      "epoch 102 | step 9 | loss: 0.0029269870680588735\n",
      "epoch 102 | step 10 | loss: 0.003235992184911259\n",
      "epoch 102 | step 11 | loss: 0.0035190081006042035\n",
      "epoch 103 | step 0 | loss: 0.00031638737242938085\n",
      "epoch 103 | step 1 | loss: 0.000601311450275388\n",
      "epoch 103 | step 2 | loss: 0.0009137672730571148\n",
      "epoch 103 | step 3 | loss: 0.0011846428746942138\n",
      "epoch 103 | step 4 | loss: 0.0014883485460202774\n",
      "epoch 103 | step 5 | loss: 0.00178528368977443\n",
      "epoch 103 | step 6 | loss: 0.0020776457230640925\n",
      "epoch 103 | step 7 | loss: 0.002366094751774724\n",
      "epoch 103 | step 8 | loss: 0.0026501586002802024\n",
      "epoch 103 | step 9 | loss: 0.002927795927465844\n",
      "epoch 103 | step 10 | loss: 0.003220535142934214\n",
      "epoch 103 | step 11 | loss: 0.003525039955616692\n",
      "epoch 104 | step 0 | loss: 0.00027955928077223475\n",
      "epoch 104 | step 1 | loss: 0.0005360658219532141\n",
      "epoch 104 | step 2 | loss: 0.0008526921476022043\n",
      "epoch 104 | step 3 | loss: 0.0011293201790074877\n",
      "epoch 104 | step 4 | loss: 0.0014365165784571367\n",
      "epoch 104 | step 5 | loss: 0.001720605458765298\n",
      "epoch 104 | step 6 | loss: 0.0020066719672918935\n",
      "epoch 104 | step 7 | loss: 0.002319182304510434\n",
      "epoch 104 | step 8 | loss: 0.0026222444977865777\n",
      "epoch 104 | step 9 | loss: 0.0029186337961822533\n",
      "epoch 104 | step 10 | loss: 0.0032296349160776485\n",
      "epoch 104 | step 11 | loss: 0.0035214715275451677\n",
      "epoch 105 | step 0 | loss: 0.00027763681680222183\n",
      "epoch 105 | step 1 | loss: 0.0005886466135505378\n",
      "epoch 105 | step 2 | loss: 0.000920168364048328\n",
      "epoch 105 | step 3 | loss: 0.0011932385787998187\n",
      "epoch 105 | step 4 | loss: 0.001503295664513324\n",
      "epoch 105 | step 5 | loss: 0.0018121202497761967\n",
      "epoch 105 | step 6 | loss: 0.00212160545776388\n",
      "epoch 105 | step 7 | loss: 0.002414681729114865\n",
      "epoch 105 | step 8 | loss: 0.0027036676393594116\n",
      "epoch 105 | step 9 | loss: 0.002980166518250207\n",
      "epoch 105 | step 10 | loss: 0.003237243045727805\n",
      "epoch 105 | step 11 | loss: 0.003518431591966257\n",
      "epoch 106 | step 0 | loss: 0.0003199215514831467\n",
      "epoch 106 | step 1 | loss: 0.0005991804064539249\n",
      "epoch 106 | step 2 | loss: 0.0008641007433249345\n",
      "epoch 106 | step 3 | loss: 0.0011548319129259577\n",
      "epoch 106 | step 4 | loss: 0.001415285186510603\n",
      "epoch 106 | step 5 | loss: 0.0017155944368512526\n",
      "epoch 106 | step 6 | loss: 0.002012750250187817\n",
      "epoch 106 | step 7 | loss: 0.002320088711192747\n",
      "epoch 106 | step 8 | loss: 0.002623558917780078\n",
      "epoch 106 | step 9 | loss: 0.0029229098366249625\n",
      "epoch 106 | step 10 | loss: 0.003213773108399214\n",
      "epoch 106 | step 11 | loss: 0.003527600656797717\n",
      "epoch 107 | step 0 | loss: 0.0002835106418805353\n",
      "epoch 107 | step 1 | loss: 0.0005714553696121434\n",
      "epoch 107 | step 2 | loss: 0.0008703493431722248\n",
      "epoch 107 | step 3 | loss: 0.001181563913070735\n",
      "epoch 107 | step 4 | loss: 0.0014941163074607769\n",
      "epoch 107 | step 5 | loss: 0.0018130598327795173\n",
      "epoch 107 | step 6 | loss: 0.0020898049166491093\n",
      "epoch 107 | step 7 | loss: 0.0023934941618999404\n",
      "epoch 107 | step 8 | loss: 0.002683174395370516\n",
      "epoch 107 | step 9 | loss: 0.002974550548279218\n",
      "epoch 107 | step 10 | loss: 0.003256908068651321\n",
      "epoch 107 | step 11 | loss: 0.003510699221929854\n",
      "epoch 108 | step 0 | loss: 0.0002977809353525067\n",
      "epoch 108 | step 1 | loss: 0.0005857989005591835\n",
      "epoch 108 | step 2 | loss: 0.0009261773593266676\n",
      "epoch 108 | step 3 | loss: 0.0012202518559929058\n",
      "epoch 108 | step 4 | loss: 0.0015118784859812228\n",
      "epoch 108 | step 5 | loss: 0.001791921240475169\n",
      "epoch 108 | step 6 | loss: 0.0021005749954692765\n",
      "epoch 108 | step 7 | loss: 0.0023786950449219612\n",
      "epoch 108 | step 8 | loss: 0.0026611027299634064\n",
      "epoch 108 | step 9 | loss: 0.002951713121477424\n",
      "epoch 108 | step 10 | loss: 0.003238008826511804\n",
      "epoch 108 | step 11 | loss: 0.0035180660907714718\n",
      "epoch 109 | step 0 | loss: 0.00031356190935653665\n",
      "epoch 109 | step 1 | loss: 0.0006289530608595608\n",
      "epoch 109 | step 2 | loss: 0.0009352526223458654\n",
      "epoch 109 | step 3 | loss: 0.0012141396593117152\n",
      "epoch 109 | step 4 | loss: 0.0015223875910741278\n",
      "epoch 109 | step 5 | loss: 0.0018103652150905567\n",
      "epoch 109 | step 6 | loss: 0.002086644287258756\n",
      "epoch 109 | step 7 | loss: 0.00237820642982576\n",
      "epoch 109 | step 8 | loss: 0.0026626721001571144\n",
      "epoch 109 | step 9 | loss: 0.002955416072693037\n",
      "epoch 109 | step 10 | loss: 0.003250710529660309\n",
      "epoch 109 | step 11 | loss: 0.0035131068448475923\n",
      "epoch 110 | step 0 | loss: 0.00028308951849433643\n",
      "epoch 110 | step 1 | loss: 0.000566636807342278\n",
      "epoch 110 | step 2 | loss: 0.0008780398628296011\n",
      "epoch 110 | step 3 | loss: 0.0011517870633258626\n",
      "epoch 110 | step 4 | loss: 0.0014075369052745444\n",
      "epoch 110 | step 5 | loss: 0.0017168561495298964\n",
      "epoch 110 | step 6 | loss: 0.002027554969687255\n",
      "epoch 110 | step 7 | loss: 0.002347905827693417\n",
      "epoch 110 | step 8 | loss: 0.0026488767336460004\n",
      "epoch 110 | step 9 | loss: 0.002944388688033972\n",
      "epoch 110 | step 10 | loss: 0.0032369554355609733\n",
      "epoch 110 | step 11 | loss: 0.003518454113520455\n",
      "epoch 111 | step 0 | loss: 0.00028096638877242225\n",
      "epoch 111 | step 1 | loss: 0.0005788957175415494\n",
      "epoch 111 | step 2 | loss: 0.0008556383119935757\n",
      "epoch 111 | step 3 | loss: 0.001189578072698577\n",
      "epoch 111 | step 4 | loss: 0.0015091305613762576\n",
      "epoch 111 | step 5 | loss: 0.001797855067845896\n",
      "epoch 111 | step 6 | loss: 0.002068087058720753\n",
      "epoch 111 | step 7 | loss: 0.002359199674489302\n",
      "epoch 111 | step 8 | loss: 0.0026722493262467044\n",
      "epoch 111 | step 9 | loss: 0.0029549080750721378\n",
      "epoch 111 | step 10 | loss: 0.003237081240904325\n",
      "epoch 111 | step 11 | loss: 0.0035184343386074207\n",
      "epoch 112 | step 0 | loss: 0.00028276231374368374\n",
      "epoch 112 | step 1 | loss: 0.000581920181774277\n",
      "epoch 112 | step 2 | loss: 0.0008527527356722174\n",
      "epoch 112 | step 3 | loss: 0.0011452857580894929\n",
      "epoch 112 | step 4 | loss: 0.0014599486701315813\n",
      "epoch 112 | step 5 | loss: 0.001745208765710658\n",
      "epoch 112 | step 6 | loss: 0.002032821828370964\n",
      "epoch 112 | step 7 | loss: 0.002354251175125787\n",
      "epoch 112 | step 8 | loss: 0.002663192895843985\n",
      "epoch 112 | step 9 | loss: 0.0029367914466571863\n",
      "epoch 112 | step 10 | loss: 0.003244822514637857\n",
      "epoch 112 | step 11 | loss: 0.0035153015609433883\n",
      "epoch 113 | step 0 | loss: 0.0002982896923857288\n",
      "epoch 113 | step 1 | loss: 0.000583027712164954\n",
      "epoch 113 | step 2 | loss: 0.000885169601066842\n",
      "epoch 113 | step 3 | loss: 0.0012110003423706512\n",
      "epoch 113 | step 4 | loss: 0.0014876997445913294\n",
      "epoch 113 | step 5 | loss: 0.0017964878304737725\n",
      "epoch 113 | step 6 | loss: 0.0021083180829057097\n",
      "epoch 113 | step 7 | loss: 0.0023896390809076784\n",
      "epoch 113 | step 8 | loss: 0.0026982979751629258\n",
      "epoch 113 | step 9 | loss: 0.0029651799013719555\n",
      "epoch 113 | step 10 | loss: 0.0032457622101349616\n",
      "epoch 113 | step 11 | loss: 0.0035149325059590218\n",
      "epoch 114 | step 0 | loss: 0.00027996299273640645\n",
      "epoch 114 | step 1 | loss: 0.0005694852870975287\n",
      "epoch 114 | step 2 | loss: 0.000877237926470835\n",
      "epoch 114 | step 3 | loss: 0.0011869668233314271\n",
      "epoch 114 | step 4 | loss: 0.0015005538164906958\n",
      "epoch 114 | step 5 | loss: 0.0017947405282838654\n",
      "epoch 114 | step 6 | loss: 0.0020658155612194753\n",
      "epoch 114 | step 7 | loss: 0.0023539150740863545\n",
      "epoch 114 | step 8 | loss: 0.002653611157828735\n",
      "epoch 114 | step 9 | loss: 0.002955876510818438\n",
      "epoch 114 | step 10 | loss: 0.0032314709402752875\n",
      "epoch 114 | step 11 | loss: 0.003520491209023814\n",
      "epoch 115 | step 0 | loss: 0.00026529067997443717\n",
      "epoch 115 | step 1 | loss: 0.0005740995371637278\n",
      "epoch 115 | step 2 | loss: 0.0008715238206226614\n",
      "epoch 115 | step 3 | loss: 0.0011647586706504368\n",
      "epoch 115 | step 4 | loss: 0.0014883152221792542\n",
      "epoch 115 | step 5 | loss: 0.0017521398017688286\n",
      "epoch 115 | step 6 | loss: 0.0020455765281622864\n",
      "epoch 115 | step 7 | loss: 0.002343989294999099\n",
      "epoch 115 | step 8 | loss: 0.0026249523004938475\n",
      "epoch 115 | step 9 | loss: 0.0029074452045926456\n",
      "epoch 115 | step 10 | loss: 0.003214135331327603\n",
      "epoch 115 | step 11 | loss: 0.003527228483672024\n",
      "epoch 116 | step 0 | loss: 0.0002764671031280997\n",
      "epoch 116 | step 1 | loss: 0.0005883838520167779\n",
      "epoch 116 | step 2 | loss: 0.0009133074619511782\n",
      "epoch 116 | step 3 | loss: 0.0012212156810259007\n",
      "epoch 116 | step 4 | loss: 0.0015076197478701527\n",
      "epoch 116 | step 5 | loss: 0.001818816400076596\n",
      "epoch 116 | step 6 | loss: 0.00211029742552322\n",
      "epoch 116 | step 7 | loss: 0.0024072399537757565\n",
      "epoch 116 | step 8 | loss: 0.002696191000282494\n",
      "epoch 116 | step 9 | loss: 0.0029654528642389646\n",
      "epoch 116 | step 10 | loss: 0.003249445985687395\n",
      "epoch 116 | step 11 | loss: 0.0035134010661708884\n",
      "epoch 117 | step 0 | loss: 0.0002657620426487729\n",
      "epoch 117 | step 1 | loss: 0.0005542778970812435\n",
      "epoch 117 | step 2 | loss: 0.0008555567171974721\n",
      "epoch 117 | step 3 | loss: 0.001139007922981421\n",
      "epoch 117 | step 4 | loss: 0.0014359813625241871\n",
      "epoch 117 | step 5 | loss: 0.0017125043559007055\n",
      "epoch 117 | step 6 | loss: 0.001983580822554021\n",
      "epoch 117 | step 7 | loss: 0.0023484929822279073\n",
      "epoch 117 | step 8 | loss: 0.0026523695539878247\n",
      "epoch 117 | step 9 | loss: 0.002930402875281479\n",
      "epoch 117 | step 10 | loss: 0.0032223527211283133\n",
      "epoch 117 | step 11 | loss: 0.0035239895600547564\n",
      "epoch 118 | step 0 | loss: 0.0002987202496497515\n",
      "epoch 118 | step 1 | loss: 0.0005930992100146758\n",
      "epoch 118 | step 2 | loss: 0.000867742024084344\n",
      "epoch 118 | step 3 | loss: 0.0011721167022136427\n",
      "epoch 118 | step 4 | loss: 0.0014814991330513497\n",
      "epoch 118 | step 5 | loss: 0.001782794955787695\n",
      "epoch 118 | step 6 | loss: 0.0021067906970199334\n",
      "epoch 118 | step 7 | loss: 0.002373245178345949\n",
      "epoch 118 | step 8 | loss: 0.0026617928210867737\n",
      "epoch 118 | step 9 | loss: 0.0029330266978539474\n",
      "epoch 118 | step 10 | loss: 0.0032372896710369447\n",
      "epoch 118 | step 11 | loss: 0.0035181171901030378\n",
      "epoch 119 | step 0 | loss: 0.0002701789568396818\n",
      "epoch 119 | step 1 | loss: 0.0005610585092304732\n",
      "epoch 119 | step 2 | loss: 0.000843805376033604\n",
      "epoch 119 | step 3 | loss: 0.001153419586461547\n",
      "epoch 119 | step 4 | loss: 0.0014665373195532163\n",
      "epoch 119 | step 5 | loss: 0.0017608825176861523\n",
      "epoch 119 | step 6 | loss: 0.002063959856458178\n",
      "epoch 119 | step 7 | loss: 0.002371920991464848\n",
      "epoch 119 | step 8 | loss: 0.0026602127039214345\n",
      "epoch 119 | step 9 | loss: 0.002935167445870532\n",
      "epoch 119 | step 10 | loss: 0.003228384206748129\n",
      "epoch 119 | step 11 | loss: 0.0035216226106683742\n",
      "epoch 120 | step 0 | loss: 0.00027217225139125335\n",
      "epoch 120 | step 1 | loss: 0.0005356114000237848\n",
      "epoch 120 | step 2 | loss: 0.0008356595227029933\n",
      "epoch 120 | step 3 | loss: 0.0011279088824391096\n",
      "epoch 120 | step 4 | loss: 0.001433584459723636\n",
      "epoch 120 | step 5 | loss: 0.001732368260517873\n",
      "epoch 120 | step 6 | loss: 0.0020201489223178395\n",
      "epoch 120 | step 7 | loss: 0.0023289780004405293\n",
      "epoch 120 | step 8 | loss: 0.0026367429239410566\n",
      "epoch 120 | step 9 | loss: 0.002917573076866675\n",
      "epoch 120 | step 10 | loss: 0.003215117275753366\n",
      "epoch 120 | step 11 | loss: 0.003526794513189013\n",
      "epoch 121 | step 0 | loss: 0.00030078281216520914\n",
      "epoch 121 | step 1 | loss: 0.0005917695875339011\n",
      "epoch 121 | step 2 | loss: 0.0009036581616421648\n",
      "epoch 121 | step 3 | loss: 0.0011975980713708147\n",
      "epoch 121 | step 4 | loss: 0.001491109239351596\n",
      "epoch 121 | step 5 | loss: 0.0018053460619005801\n",
      "epoch 121 | step 6 | loss: 0.0020997644156756723\n",
      "epoch 121 | step 7 | loss: 0.002390550641902581\n",
      "epoch 121 | step 8 | loss: 0.0026816012331315198\n",
      "epoch 121 | step 9 | loss: 0.0029451924478421593\n",
      "epoch 121 | step 10 | loss: 0.00322638656565739\n",
      "epoch 121 | step 11 | loss: 0.003522359917135544\n",
      "epoch 122 | step 0 | loss: 0.000288244399774175\n",
      "epoch 122 | step 1 | loss: 0.0005749732143685042\n",
      "epoch 122 | step 2 | loss: 0.0008789119453370082\n",
      "epoch 122 | step 3 | loss: 0.001167910969802634\n",
      "epoch 122 | step 4 | loss: 0.0014648039443196435\n",
      "epoch 122 | step 5 | loss: 0.0017672015518810386\n",
      "epoch 122 | step 6 | loss: 0.0020456473908305505\n",
      "epoch 122 | step 7 | loss: 0.0023362421382002304\n",
      "epoch 122 | step 8 | loss: 0.0026348111546468356\n",
      "epoch 122 | step 9 | loss: 0.0029282467859983083\n",
      "epoch 122 | step 10 | loss: 0.0032231293029658216\n",
      "epoch 122 | step 11 | loss: 0.003523572196402528\n",
      "epoch 123 | step 0 | loss: 0.00031793695323882224\n",
      "epoch 123 | step 1 | loss: 0.0006076106831673286\n",
      "epoch 123 | step 2 | loss: 0.0009375568172186328\n",
      "epoch 123 | step 3 | loss: 0.0012369368330681207\n",
      "epoch 123 | step 4 | loss: 0.00151143297463039\n",
      "epoch 123 | step 5 | loss: 0.00179169268557659\n",
      "epoch 123 | step 6 | loss: 0.0020867592897040486\n",
      "epoch 123 | step 7 | loss: 0.0023582062004053788\n",
      "epoch 123 | step 8 | loss: 0.002660349321656307\n",
      "epoch 123 | step 9 | loss: 0.002948091649022057\n",
      "epoch 123 | step 10 | loss: 0.0032332878379342975\n",
      "epoch 123 | step 11 | loss: 0.0035196000495982495\n",
      "epoch 124 | step 0 | loss: 0.00028881193600918415\n",
      "epoch 124 | step 1 | loss: 0.0005657861401032122\n",
      "epoch 124 | step 2 | loss: 0.0008675609889486607\n",
      "epoch 124 | step 3 | loss: 0.0011399340628856244\n",
      "epoch 124 | step 4 | loss: 0.0014459271951268605\n",
      "epoch 124 | step 5 | loss: 0.0017401497830739515\n",
      "epoch 124 | step 6 | loss: 0.0020454989085244325\n",
      "epoch 124 | step 7 | loss: 0.0023532707246533262\n",
      "epoch 124 | step 8 | loss: 0.002625483364893746\n",
      "epoch 124 | step 9 | loss: 0.002936758145393343\n",
      "epoch 124 | step 10 | loss: 0.003237149433564967\n",
      "epoch 124 | step 11 | loss: 0.003518067235462648\n",
      "epoch 125 | step 0 | loss: 0.0002910562981577724\n",
      "epoch 125 | step 1 | loss: 0.00059379398069441\n",
      "epoch 125 | step 2 | loss: 0.0008870488973196977\n",
      "epoch 125 | step 3 | loss: 0.0011476008184341897\n",
      "epoch 125 | step 4 | loss: 0.001455608271010175\n",
      "epoch 125 | step 5 | loss: 0.001750038592508264\n",
      "epoch 125 | step 6 | loss: 0.002056327081755131\n",
      "epoch 125 | step 7 | loss: 0.0023671438331169456\n",
      "epoch 125 | step 8 | loss: 0.0026606156474763398\n",
      "epoch 125 | step 9 | loss: 0.0029339545771597565\n",
      "epoch 125 | step 10 | loss: 0.0032218110780578542\n",
      "epoch 125 | step 11 | loss: 0.0035239780520459557\n",
      "epoch 126 | step 0 | loss: 0.0002881725112785594\n",
      "epoch 126 | step 1 | loss: 0.0005625893936834061\n",
      "epoch 126 | step 2 | loss: 0.0008685143766994991\n",
      "epoch 126 | step 3 | loss: 0.001134886439317195\n",
      "epoch 126 | step 4 | loss: 0.0014448429986839047\n",
      "epoch 126 | step 5 | loss: 0.0017333179571786845\n",
      "epoch 126 | step 6 | loss: 0.002058485337712778\n",
      "epoch 126 | step 7 | loss: 0.0023577980610998703\n",
      "epoch 126 | step 8 | loss: 0.002667140648080147\n",
      "epoch 126 | step 9 | loss: 0.002948409705482736\n",
      "epoch 126 | step 10 | loss: 0.003220011284793072\n",
      "epoch 126 | step 11 | loss: 0.0035247353353966967\n",
      "epoch 127 | step 0 | loss: 0.00028337701990305937\n",
      "epoch 127 | step 1 | loss: 0.0005812880988482075\n",
      "epoch 127 | step 2 | loss: 0.0008473321157869945\n",
      "epoch 127 | step 3 | loss: 0.0011368791410807106\n",
      "epoch 127 | step 4 | loss: 0.0014373554630715004\n",
      "epoch 127 | step 5 | loss: 0.0017396704263073167\n",
      "epoch 127 | step 6 | loss: 0.00201698961572022\n",
      "epoch 127 | step 7 | loss: 0.0022920303393975483\n",
      "epoch 127 | step 8 | loss: 0.002599350979843524\n",
      "epoch 127 | step 9 | loss: 0.002913613876186545\n",
      "epoch 127 | step 10 | loss: 0.0032095470148571607\n",
      "epoch 127 | step 11 | loss: 0.0035287902695924816\n",
      "epoch 128 | step 0 | loss: 0.00028343715741300656\n",
      "epoch 128 | step 1 | loss: 0.0005769814118942788\n",
      "epoch 128 | step 2 | loss: 0.0008711199191409841\n",
      "epoch 128 | step 3 | loss: 0.0011498144231912829\n",
      "epoch 128 | step 4 | loss: 0.001435800958255912\n",
      "epoch 128 | step 5 | loss: 0.0017509164775628308\n",
      "epoch 128 | step 6 | loss: 0.0020445889872999874\n",
      "epoch 128 | step 7 | loss: 0.002335906712118843\n",
      "epoch 128 | step 8 | loss: 0.002628728556352544\n",
      "epoch 128 | step 9 | loss: 0.0029166704668818974\n",
      "epoch 128 | step 10 | loss: 0.0032323338448424404\n",
      "epoch 128 | step 11 | loss: 0.0035198328616694178\n",
      "epoch 129 | step 0 | loss: 0.0002750716996221034\n",
      "epoch 129 | step 1 | loss: 0.0005952339597514603\n",
      "epoch 129 | step 2 | loss: 0.0008797921417693678\n",
      "epoch 129 | step 3 | loss: 0.00117305929072007\n",
      "epoch 129 | step 4 | loss: 0.0015020999901641983\n",
      "epoch 129 | step 5 | loss: 0.0017700154237241523\n",
      "epoch 129 | step 6 | loss: 0.002074989457434665\n",
      "epoch 129 | step 7 | loss: 0.0023510308963967342\n",
      "epoch 129 | step 8 | loss: 0.0026411089361598767\n",
      "epoch 129 | step 9 | loss: 0.002928817024244509\n",
      "epoch 129 | step 10 | loss: 0.0032281884030426282\n",
      "epoch 129 | step 11 | loss: 0.0035214152423995886\n",
      "epoch 130 | step 0 | loss: 0.00026646511193275995\n",
      "epoch 130 | step 1 | loss: 0.0005692498216627201\n",
      "epoch 130 | step 2 | loss: 0.0008656575794467779\n",
      "epoch 130 | step 3 | loss: 0.0011667940901385384\n",
      "epoch 130 | step 4 | loss: 0.0014415555715881292\n",
      "epoch 130 | step 5 | loss: 0.0017672228401072528\n",
      "epoch 130 | step 6 | loss: 0.0020442076593908515\n",
      "epoch 130 | step 7 | loss: 0.002331535641524599\n",
      "epoch 130 | step 8 | loss: 0.0026297255235453185\n",
      "epoch 130 | step 9 | loss: 0.0029157740934903547\n",
      "epoch 130 | step 10 | loss: 0.003240702625434966\n",
      "epoch 130 | step 11 | loss: 0.0035164717890102887\n",
      "epoch 131 | step 0 | loss: 0.00032591671046173256\n",
      "epoch 131 | step 1 | loss: 0.0006198628131346558\n",
      "epoch 131 | step 2 | loss: 0.0009244958052260643\n",
      "epoch 131 | step 3 | loss: 0.001198166800777127\n",
      "epoch 131 | step 4 | loss: 0.00148456390990091\n",
      "epoch 131 | step 5 | loss: 0.0017817136371974652\n",
      "epoch 131 | step 6 | loss: 0.0020715085041421244\n",
      "epoch 131 | step 7 | loss: 0.002378336246841894\n",
      "epoch 131 | step 8 | loss: 0.0026575067548530473\n",
      "epoch 131 | step 9 | loss: 0.0029619858993811432\n",
      "epoch 131 | step 10 | loss: 0.00322245841028035\n",
      "epoch 131 | step 11 | loss: 0.0035236019102248316\n",
      "epoch 132 | step 0 | loss: 0.0003035993640042463\n",
      "epoch 132 | step 1 | loss: 0.0006183329798043601\n",
      "epoch 132 | step 2 | loss: 0.0008940627719374414\n",
      "epoch 132 | step 3 | loss: 0.0011531617223800978\n",
      "epoch 132 | step 4 | loss: 0.0014551795991398393\n",
      "epoch 132 | step 5 | loss: 0.0017596199744012754\n",
      "epoch 132 | step 6 | loss: 0.0020601285547748025\n",
      "epoch 132 | step 7 | loss: 0.0023436422555081437\n",
      "epoch 132 | step 8 | loss: 0.002622753985060921\n",
      "epoch 132 | step 9 | loss: 0.0029088957043525267\n",
      "epoch 132 | step 10 | loss: 0.0032224822260227748\n",
      "epoch 132 | step 11 | loss: 0.003523548355164211\n",
      "epoch 133 | step 0 | loss: 0.00028918132017343215\n",
      "epoch 133 | step 1 | loss: 0.000567223656547628\n",
      "epoch 133 | step 2 | loss: 0.0008616330741497225\n",
      "epoch 133 | step 3 | loss: 0.0011492506637814381\n",
      "epoch 133 | step 4 | loss: 0.0014454055884353045\n",
      "epoch 133 | step 5 | loss: 0.0017470048453620518\n",
      "epoch 133 | step 6 | loss: 0.0020536193083659235\n",
      "epoch 133 | step 7 | loss: 0.0023524352113789915\n",
      "epoch 133 | step 8 | loss: 0.002634357473488631\n",
      "epoch 133 | step 9 | loss: 0.002926760883244871\n",
      "epoch 133 | step 10 | loss: 0.003237563150672521\n",
      "epoch 133 | step 11 | loss: 0.00351768170692839\n",
      "epoch 134 | step 0 | loss: 0.00028665541572421685\n",
      "epoch 134 | step 1 | loss: 0.0005834533950741644\n",
      "epoch 134 | step 2 | loss: 0.0008732097618360066\n",
      "epoch 134 | step 3 | loss: 0.0011737086769067428\n",
      "epoch 134 | step 4 | loss: 0.0014602091980796886\n",
      "epoch 134 | step 5 | loss: 0.001756641626602419\n",
      "epoch 134 | step 6 | loss: 0.0020082166176076275\n",
      "epoch 134 | step 7 | loss: 0.002312729241561292\n",
      "epoch 134 | step 8 | loss: 0.0026333603369258383\n",
      "epoch 134 | step 9 | loss: 0.0029217870402409485\n",
      "epoch 134 | step 10 | loss: 0.0032091031382565603\n",
      "epoch 134 | step 11 | loss: 0.003528731559863983\n",
      "epoch 135 | step 0 | loss: 0.0002997221836022825\n",
      "epoch 135 | step 1 | loss: 0.0005675281157285938\n",
      "epoch 135 | step 2 | loss: 0.0008591510183906066\n",
      "epoch 135 | step 3 | loss: 0.0011781181528094764\n",
      "epoch 135 | step 4 | loss: 0.0014521053891527688\n",
      "epoch 135 | step 5 | loss: 0.0017398732872611663\n",
      "epoch 135 | step 6 | loss: 0.002032736409196206\n",
      "epoch 135 | step 7 | loss: 0.0023006392149367326\n",
      "epoch 135 | step 8 | loss: 0.0026023877618214534\n",
      "epoch 135 | step 9 | loss: 0.0029226418527653615\n",
      "epoch 135 | step 10 | loss: 0.003221197179720285\n",
      "epoch 135 | step 11 | loss: 0.003524194443055027\n",
      "epoch 136 | step 0 | loss: 0.0002717328526893271\n",
      "epoch 136 | step 1 | loss: 0.0005962801858187955\n",
      "epoch 136 | step 2 | loss: 0.0008718131727159791\n",
      "epoch 136 | step 3 | loss: 0.001166094483851747\n",
      "epoch 136 | step 4 | loss: 0.0014624993055689284\n",
      "epoch 136 | step 5 | loss: 0.0017827063615663964\n",
      "epoch 136 | step 6 | loss: 0.0021040345411165867\n",
      "epoch 136 | step 7 | loss: 0.002353563578750605\n",
      "epoch 136 | step 8 | loss: 0.0026509554163155207\n",
      "epoch 136 | step 9 | loss: 0.0029545166646484933\n",
      "epoch 136 | step 10 | loss: 0.0032304719189493744\n",
      "epoch 136 | step 11 | loss: 0.0035204031170108273\n",
      "epoch 137 | step 0 | loss: 0.0003041896987427615\n",
      "epoch 137 | step 1 | loss: 0.000580959982187569\n",
      "epoch 137 | step 2 | loss: 0.0008567530067092619\n",
      "epoch 137 | step 3 | loss: 0.0011230757836142799\n",
      "epoch 137 | step 4 | loss: 0.0014238572234504172\n",
      "epoch 137 | step 5 | loss: 0.0017042103478648572\n",
      "epoch 137 | step 6 | loss: 0.002000605633329571\n",
      "epoch 137 | step 7 | loss: 0.0023113384855654507\n",
      "epoch 137 | step 8 | loss: 0.0026308225275044387\n",
      "epoch 137 | step 9 | loss: 0.00292762152805211\n",
      "epoch 137 | step 10 | loss: 0.0032160868035998823\n",
      "epoch 137 | step 11 | loss: 0.0035259370392373606\n",
      "epoch 138 | step 0 | loss: 0.0002847937716487556\n",
      "epoch 138 | step 1 | loss: 0.0005803050754574913\n",
      "epoch 138 | step 2 | loss: 0.0008667794417057149\n",
      "epoch 138 | step 3 | loss: 0.0011796001046104686\n",
      "epoch 138 | step 4 | loss: 0.0014687580348681424\n",
      "epoch 138 | step 5 | loss: 0.0017745122683477364\n",
      "epoch 138 | step 6 | loss: 0.002055235534901466\n",
      "epoch 138 | step 7 | loss: 0.0023563044408908165\n",
      "epoch 138 | step 8 | loss: 0.002633958562012299\n",
      "epoch 138 | step 9 | loss: 0.002924707075282488\n",
      "epoch 138 | step 10 | loss: 0.0032272361688942828\n",
      "epoch 138 | step 11 | loss: 0.0035216266235816086\n",
      "epoch 139 | step 0 | loss: 0.00030198552513189083\n",
      "epoch 139 | step 1 | loss: 0.0006118766370063359\n",
      "epoch 139 | step 2 | loss: 0.0008882957193408869\n",
      "epoch 139 | step 3 | loss: 0.0011928495579747133\n",
      "epoch 139 | step 4 | loss: 0.0014957496184595793\n",
      "epoch 139 | step 5 | loss: 0.0017896015406889173\n",
      "epoch 139 | step 6 | loss: 0.002078039160675566\n",
      "epoch 139 | step 7 | loss: 0.0023564116497842157\n",
      "epoch 139 | step 8 | loss: 0.0026585274206054614\n",
      "epoch 139 | step 9 | loss: 0.0029395768330576695\n",
      "epoch 139 | step 10 | loss: 0.0032392266282011057\n",
      "epoch 139 | step 11 | loss: 0.0035169040890608767\n",
      "epoch 140 | step 0 | loss: 0.0002952485139773861\n",
      "epoch 140 | step 1 | loss: 0.0005837707604397174\n",
      "epoch 140 | step 2 | loss: 0.0008825710462957569\n",
      "epoch 140 | step 3 | loss: 0.0011892408846082263\n",
      "epoch 140 | step 4 | loss: 0.0014916840004332705\n",
      "epoch 140 | step 5 | loss: 0.0017680707208917105\n",
      "epoch 140 | step 6 | loss: 0.0020540648280686588\n",
      "epoch 140 | step 7 | loss: 0.0023622597560652935\n",
      "epoch 140 | step 8 | loss: 0.002656803801831286\n",
      "epoch 140 | step 9 | loss: 0.0029548363156360925\n",
      "epoch 140 | step 10 | loss: 0.0032257771975096076\n",
      "epoch 140 | step 11 | loss: 0.003522073562242686\n",
      "epoch 141 | step 0 | loss: 0.00029379731991899586\n",
      "epoch 141 | step 1 | loss: 0.0005772149288903654\n",
      "epoch 141 | step 2 | loss: 0.0008773685155773244\n",
      "epoch 141 | step 3 | loss: 0.0011546351625388944\n",
      "epoch 141 | step 4 | loss: 0.00144355982675064\n",
      "epoch 141 | step 5 | loss: 0.001744130695617453\n",
      "epoch 141 | step 6 | loss: 0.002031033345451147\n",
      "epoch 141 | step 7 | loss: 0.0023351932729424015\n",
      "epoch 141 | step 8 | loss: 0.002630022379210132\n",
      "epoch 141 | step 9 | loss: 0.0029301878315775667\n",
      "epoch 141 | step 10 | loss: 0.0032222201840057018\n",
      "epoch 141 | step 11 | loss: 0.00352343388467573\n",
      "epoch 142 | step 0 | loss: 0.00028579675513870027\n",
      "epoch 142 | step 1 | loss: 0.0005683027481423637\n",
      "epoch 142 | step 2 | loss: 0.000873126385428147\n",
      "epoch 142 | step 3 | loss: 0.0011743323243709933\n",
      "epoch 142 | step 4 | loss: 0.0014876632867926465\n",
      "epoch 142 | step 5 | loss: 0.0017870000267611805\n",
      "epoch 142 | step 6 | loss: 0.0020528690182906764\n",
      "epoch 142 | step 7 | loss: 0.0023404394511550683\n",
      "epoch 142 | step 8 | loss: 0.0026358570434659337\n",
      "epoch 142 | step 9 | loss: 0.0029139606334223052\n",
      "epoch 142 | step 10 | loss: 0.003220114052651043\n",
      "epoch 142 | step 11 | loss: 0.003524243427460423\n",
      "epoch 143 | step 0 | loss: 0.0002702331657819377\n",
      "epoch 143 | step 1 | loss: 0.0005585723201116377\n",
      "epoch 143 | step 2 | loss: 0.0008853985667261683\n",
      "epoch 143 | step 3 | loss: 0.0011674002647650127\n",
      "epoch 143 | step 4 | loss: 0.001462441325011663\n",
      "epoch 143 | step 5 | loss: 0.0017357531749458\n",
      "epoch 143 | step 6 | loss: 0.002040749706929014\n",
      "epoch 143 | step 7 | loss: 0.0023678832402857356\n",
      "epoch 143 | step 8 | loss: 0.002633420645097548\n",
      "epoch 143 | step 9 | loss: 0.002915149034532172\n",
      "epoch 143 | step 10 | loss: 0.0032365238791676036\n",
      "epoch 143 | step 11 | loss: 0.003517805511631551\n",
      "epoch 144 | step 0 | loss: 0.00028086988335316134\n",
      "epoch 144 | step 1 | loss: 0.0005758585050811668\n",
      "epoch 144 | step 2 | loss: 0.0008733941833873812\n",
      "epoch 144 | step 3 | loss: 0.0011334531410468866\n",
      "epoch 144 | step 4 | loss: 0.0014293850550443463\n",
      "epoch 144 | step 5 | loss: 0.001710424904668288\n",
      "epoch 144 | step 6 | loss: 0.002010553747910767\n",
      "epoch 144 | step 7 | loss: 0.0023259334133258155\n",
      "epoch 144 | step 8 | loss: 0.002637125311225814\n",
      "epoch 144 | step 9 | loss: 0.002928867022451825\n",
      "epoch 144 | step 10 | loss: 0.0032118576200803493\n",
      "epoch 144 | step 11 | loss: 0.0035274007560060125\n",
      "epoch 145 | step 0 | loss: 0.0002948735214232533\n",
      "epoch 145 | step 1 | loss: 0.0005644766580518659\n",
      "epoch 145 | step 2 | loss: 0.000864759754966976\n",
      "epoch 145 | step 3 | loss: 0.0011764998156352274\n",
      "epoch 145 | step 4 | loss: 0.0014577359480791043\n",
      "epoch 145 | step 5 | loss: 0.0017458808440342726\n",
      "epoch 145 | step 6 | loss: 0.0020621519756552284\n",
      "epoch 145 | step 7 | loss: 0.0023504293288867705\n",
      "epoch 145 | step 8 | loss: 0.0026416652948235884\n",
      "epoch 145 | step 9 | loss: 0.0029276476227245865\n",
      "epoch 145 | step 10 | loss: 0.0032196794976794\n",
      "epoch 145 | step 11 | loss: 0.0035243609619898136\n",
      "epoch 146 | step 0 | loss: 0.00027981736932340304\n",
      "epoch 146 | step 1 | loss: 0.0005754371448499007\n",
      "epoch 146 | step 2 | loss: 0.0008853447431844878\n",
      "epoch 146 | step 3 | loss: 0.0011933950822784647\n",
      "epoch 146 | step 4 | loss: 0.0014719260076380611\n",
      "epoch 146 | step 5 | loss: 0.0017424114417513774\n",
      "epoch 146 | step 6 | loss: 0.0020219653155908526\n",
      "epoch 146 | step 7 | loss: 0.0023222419823630924\n",
      "epoch 146 | step 8 | loss: 0.002608456822124342\n",
      "epoch 146 | step 9 | loss: 0.0029334964779299416\n",
      "epoch 146 | step 10 | loss: 0.003241060183904558\n",
      "epoch 146 | step 11 | loss: 0.0035159755334888366\n",
      "epoch 147 | step 0 | loss: 0.00028956214632463594\n",
      "epoch 147 | step 1 | loss: 0.0006114464256670955\n",
      "epoch 147 | step 2 | loss: 0.0008747045683473321\n",
      "epoch 147 | step 3 | loss: 0.0011780987030998585\n",
      "epoch 147 | step 4 | loss: 0.0014763720097677316\n",
      "epoch 147 | step 5 | loss: 0.0017606414717093494\n",
      "epoch 147 | step 6 | loss: 0.002036150769623729\n",
      "epoch 147 | step 7 | loss: 0.002339097914305173\n",
      "epoch 147 | step 8 | loss: 0.0026293514261011056\n",
      "epoch 147 | step 9 | loss: 0.002940699825818446\n",
      "epoch 147 | step 10 | loss: 0.003238130207187673\n",
      "epoch 147 | step 11 | loss: 0.0035171019386147583\n",
      "epoch 148 | step 0 | loss: 0.0002963771582020317\n",
      "epoch 148 | step 1 | loss: 0.0005813761527532707\n",
      "epoch 148 | step 2 | loss: 0.0009043209054482658\n",
      "epoch 148 | step 3 | loss: 0.0012038709883725727\n",
      "epoch 148 | step 4 | loss: 0.001463799789096679\n",
      "epoch 148 | step 5 | loss: 0.0017700360046129606\n",
      "epoch 148 | step 6 | loss: 0.0020456365928170667\n",
      "epoch 148 | step 7 | loss: 0.0023710735227166053\n",
      "epoch 148 | step 8 | loss: 0.002656815886187272\n",
      "epoch 148 | step 9 | loss: 0.0029114372733149083\n",
      "epoch 148 | step 10 | loss: 0.0032379508251178498\n",
      "epoch 148 | step 11 | loss: 0.0035171040576034915\n",
      "epoch 149 | step 0 | loss: 0.0003038431941820607\n",
      "epoch 149 | step 1 | loss: 0.0006206564979534965\n",
      "epoch 149 | step 2 | loss: 0.0009192429995992761\n",
      "epoch 149 | step 3 | loss: 0.0011913580547512719\n",
      "epoch 149 | step 4 | loss: 0.001490711661916235\n",
      "epoch 149 | step 5 | loss: 0.0017820502718314233\n",
      "epoch 149 | step 6 | loss: 0.0020883598532993597\n",
      "epoch 149 | step 7 | loss: 0.0023691790698562665\n",
      "epoch 149 | step 8 | loss: 0.0026513218495050964\n",
      "epoch 149 | step 9 | loss: 0.00295211719528364\n",
      "epoch 149 | step 10 | loss: 0.003226564841344857\n",
      "epoch 149 | step 11 | loss: 0.00352153304690251\n",
      "epoch 150 | step 0 | loss: 0.0003128799063622983\n",
      "epoch 150 | step 1 | loss: 0.0006246391838547374\n",
      "epoch 150 | step 2 | loss: 0.0009192026902712311\n",
      "epoch 150 | step 3 | loss: 0.0012413696914421216\n",
      "epoch 150 | step 4 | loss: 0.001562451006525647\n",
      "epoch 150 | step 5 | loss: 0.0018809361706795384\n",
      "epoch 150 | step 6 | loss: 0.0021535152196220293\n",
      "epoch 150 | step 7 | loss: 0.0024393800726106103\n",
      "epoch 150 | step 8 | loss: 0.0027175116545699062\n",
      "epoch 150 | step 9 | loss: 0.0029579235180105974\n",
      "epoch 150 | step 10 | loss: 0.003236914712498269\n",
      "epoch 150 | step 11 | loss: 0.003517429428444795\n",
      "epoch 151 | step 0 | loss: 0.00025900859598712193\n",
      "epoch 151 | step 1 | loss: 0.0005493817027178369\n",
      "epoch 151 | step 2 | loss: 0.0008549630277671766\n",
      "epoch 151 | step 3 | loss: 0.0011581683877600918\n",
      "epoch 151 | step 4 | loss: 0.001455598899866343\n",
      "epoch 151 | step 5 | loss: 0.0017410068631929097\n",
      "epoch 151 | step 6 | loss: 0.002043460260518248\n",
      "epoch 151 | step 7 | loss: 0.0023489093597214177\n",
      "epoch 151 | step 8 | loss: 0.002626781436456449\n",
      "epoch 151 | step 9 | loss: 0.0029189912345475464\n",
      "epoch 151 | step 10 | loss: 0.0032261860288764544\n",
      "epoch 151 | step 11 | loss: 0.0035215992459967176\n",
      "epoch 152 | step 0 | loss: 0.0002963885019375549\n",
      "epoch 152 | step 1 | loss: 0.0005899863011309453\n",
      "epoch 152 | step 2 | loss: 0.0008901620473460421\n",
      "epoch 152 | step 3 | loss: 0.0011625834682376592\n",
      "epoch 152 | step 4 | loss: 0.0014530200869687707\n",
      "epoch 152 | step 5 | loss: 0.0017519307935764336\n",
      "epoch 152 | step 6 | loss: 0.0020793198495688203\n",
      "epoch 152 | step 7 | loss: 0.0023808002291297497\n",
      "epoch 152 | step 8 | loss: 0.002669282572814929\n",
      "epoch 152 | step 9 | loss: 0.0029461074715041308\n",
      "epoch 152 | step 10 | loss: 0.0032230353106032146\n",
      "epoch 152 | step 11 | loss: 0.003522890852081766\n",
      "epoch 153 | step 0 | loss: 0.00029047825944715353\n",
      "epoch 153 | step 1 | loss: 0.0006042632688279514\n",
      "epoch 153 | step 2 | loss: 0.000879025088562501\n",
      "epoch 153 | step 3 | loss: 0.0011806827129719607\n",
      "epoch 153 | step 4 | loss: 0.001488460946941887\n",
      "epoch 153 | step 5 | loss: 0.0017994275235460964\n",
      "epoch 153 | step 6 | loss: 0.0020806147188765973\n",
      "epoch 153 | step 7 | loss: 0.0023814793294993043\n",
      "epoch 153 | step 8 | loss: 0.002657881463535843\n",
      "epoch 153 | step 9 | loss: 0.0029578284216819246\n",
      "epoch 153 | step 10 | loss: 0.0032264893737043144\n",
      "epoch 153 | step 11 | loss: 0.003521405033516574\n",
      "epoch 154 | step 0 | loss: 0.0002900667686708689\n",
      "epoch 154 | step 1 | loss: 0.0005732006635693793\n",
      "epoch 154 | step 2 | loss: 0.000881190648842817\n",
      "epoch 154 | step 3 | loss: 0.0011697938794297418\n",
      "epoch 154 | step 4 | loss: 0.0014657901369186962\n",
      "epoch 154 | step 5 | loss: 0.0017710585271963509\n",
      "epoch 154 | step 6 | loss: 0.0020541614141746044\n",
      "epoch 154 | step 7 | loss: 0.0023625246826156554\n",
      "epoch 154 | step 8 | loss: 0.0026680537470877264\n",
      "epoch 154 | step 9 | loss: 0.0029697713338676266\n",
      "epoch 154 | step 10 | loss: 0.0032531742681142584\n",
      "epoch 154 | step 11 | loss: 0.0035109762556332696\n",
      "epoch 155 | step 0 | loss: 0.0002879341813392034\n",
      "epoch 155 | step 1 | loss: 0.0005679395604146028\n",
      "epoch 155 | step 2 | loss: 0.0008622183444424213\n",
      "epoch 155 | step 3 | loss: 0.001130821387742929\n",
      "epoch 155 | step 4 | loss: 0.0014139910392639567\n",
      "epoch 155 | step 5 | loss: 0.0017030796635473851\n",
      "epoch 155 | step 6 | loss: 0.0019955214161040498\n",
      "epoch 155 | step 7 | loss: 0.0023114965602226705\n",
      "epoch 155 | step 8 | loss: 0.00262433733739148\n",
      "epoch 155 | step 9 | loss: 0.0029103111263459073\n",
      "epoch 155 | step 10 | loss: 0.0032114939475410874\n",
      "epoch 155 | step 11 | loss: 0.003527250161179131\n",
      "epoch 156 | step 0 | loss: 0.00028288863421703384\n",
      "epoch 156 | step 1 | loss: 0.0005690111235232198\n",
      "epoch 156 | step 2 | loss: 0.000870359927327443\n",
      "epoch 156 | step 3 | loss: 0.0011745347802458016\n",
      "epoch 156 | step 4 | loss: 0.0014919272607621739\n",
      "epoch 156 | step 5 | loss: 0.0017707729260021236\n",
      "epoch 156 | step 6 | loss: 0.0020808969068840328\n",
      "epoch 156 | step 7 | loss: 0.002384849296571379\n",
      "epoch 156 | step 8 | loss: 0.002662354804856172\n",
      "epoch 156 | step 9 | loss: 0.0029398602471878187\n",
      "epoch 156 | step 10 | loss: 0.0032359297692291984\n",
      "epoch 156 | step 11 | loss: 0.003517684817956926\n",
      "epoch 157 | step 0 | loss: 0.00029089200980718214\n",
      "epoch 157 | step 1 | loss: 0.0005970381503062174\n",
      "epoch 157 | step 2 | loss: 0.0008972420541703824\n",
      "epoch 157 | step 3 | loss: 0.0011701397387893372\n",
      "epoch 157 | step 4 | loss: 0.0014673036568818875\n",
      "epoch 157 | step 5 | loss: 0.0017602860965896313\n",
      "epoch 157 | step 6 | loss: 0.002074786601061606\n",
      "epoch 157 | step 7 | loss: 0.002348274336428359\n",
      "epoch 157 | step 8 | loss: 0.002626776637362278\n",
      "epoch 157 | step 9 | loss: 0.0029192308388830326\n",
      "epoch 157 | step 10 | loss: 0.003236820811442885\n",
      "epoch 157 | step 11 | loss: 0.0035173051496657864\n",
      "epoch 158 | step 0 | loss: 0.0003055850407241488\n",
      "epoch 158 | step 1 | loss: 0.0005896113772199484\n",
      "epoch 158 | step 2 | loss: 0.0008807628760603397\n",
      "epoch 158 | step 3 | loss: 0.00117471975505882\n",
      "epoch 158 | step 4 | loss: 0.0014657504259227745\n",
      "epoch 158 | step 5 | loss: 0.0017610319958891708\n",
      "epoch 158 | step 6 | loss: 0.002073751361736406\n",
      "epoch 158 | step 7 | loss: 0.0023551020611213185\n",
      "epoch 158 | step 8 | loss: 0.0026903452149866225\n",
      "epoch 158 | step 9 | loss: 0.0029708091790019257\n",
      "epoch 158 | step 10 | loss: 0.003238011491975945\n",
      "epoch 158 | step 11 | loss: 0.003516842064544836\n",
      "epoch 159 | step 0 | loss: 0.0002944612465168137\n",
      "epoch 159 | step 1 | loss: 0.0005639512451396545\n",
      "epoch 159 | step 2 | loss: 0.0008508926185721172\n",
      "epoch 159 | step 3 | loss: 0.0011777196829217463\n",
      "epoch 159 | step 4 | loss: 0.00151958025009489\n",
      "epoch 159 | step 5 | loss: 0.0018111346661441857\n",
      "epoch 159 | step 6 | loss: 0.002084244124103767\n",
      "epoch 159 | step 7 | loss: 0.002390170191052511\n",
      "epoch 159 | step 8 | loss: 0.0026785930607242025\n",
      "epoch 159 | step 9 | loss: 0.002948683779659162\n",
      "epoch 159 | step 10 | loss: 0.0032429578752137256\n",
      "epoch 159 | step 11 | loss: 0.003514846024374684\n",
      "epoch 160 | step 0 | loss: 0.0002865893806387646\n",
      "epoch 160 | step 1 | loss: 0.0005856355642314485\n",
      "epoch 160 | step 2 | loss: 0.0008907300069954541\n",
      "epoch 160 | step 3 | loss: 0.0011944041008952393\n",
      "epoch 160 | step 4 | loss: 0.001481278552184816\n",
      "epoch 160 | step 5 | loss: 0.0017625596628360121\n",
      "epoch 160 | step 6 | loss: 0.002077069811711745\n",
      "epoch 160 | step 7 | loss: 0.002372755049486461\n",
      "epoch 160 | step 8 | loss: 0.0026621071323792503\n",
      "epoch 160 | step 9 | loss: 0.0029422438826310865\n",
      "epoch 160 | step 10 | loss: 0.003246642282106816\n",
      "epoch 160 | step 11 | loss: 0.003513331210143344\n",
      "epoch 161 | step 0 | loss: 0.0002780333834578035\n",
      "epoch 161 | step 1 | loss: 0.0005709960025128025\n",
      "epoch 161 | step 2 | loss: 0.0008474797685491459\n",
      "epoch 161 | step 3 | loss: 0.0011221486558720705\n",
      "epoch 161 | step 4 | loss: 0.0014144877754164256\n",
      "epoch 161 | step 5 | loss: 0.001719361046999418\n",
      "epoch 161 | step 6 | loss: 0.0020306975123405317\n",
      "epoch 161 | step 7 | loss: 0.002297985916335248\n",
      "epoch 161 | step 8 | loss: 0.0026090050181902563\n",
      "epoch 161 | step 9 | loss: 0.00291539938361311\n",
      "epoch 161 | step 10 | loss: 0.0032248949972415236\n",
      "epoch 161 | step 11 | loss: 0.0035218487193437913\n",
      "epoch 162 | step 0 | loss: 0.00030563317147038946\n",
      "epoch 162 | step 1 | loss: 0.0005951936490902096\n",
      "epoch 162 | step 2 | loss: 0.0008603452037094245\n",
      "epoch 162 | step 3 | loss: 0.001145804301647195\n",
      "epoch 162 | step 4 | loss: 0.0014226930889416167\n",
      "epoch 162 | step 5 | loss: 0.0017088478991597906\n",
      "epoch 162 | step 6 | loss: 0.0020054680634336314\n",
      "epoch 162 | step 7 | loss: 0.0023014693135316074\n",
      "epoch 162 | step 8 | loss: 0.00262730777754802\n",
      "epoch 162 | step 9 | loss: 0.002927953776885196\n",
      "epoch 162 | step 10 | loss: 0.003221164460336456\n",
      "epoch 162 | step 11 | loss: 0.0035232971198335627\n",
      "epoch 163 | step 0 | loss: 0.0002926091588005668\n",
      "epoch 163 | step 1 | loss: 0.0005973065197925381\n",
      "epoch 163 | step 2 | loss: 0.0009011568047545493\n",
      "epoch 163 | step 3 | loss: 0.001175067882025045\n",
      "epoch 163 | step 4 | loss: 0.0014724383739158364\n",
      "epoch 163 | step 5 | loss: 0.0017907768452564677\n",
      "epoch 163 | step 6 | loss: 0.002089359683873612\n",
      "epoch 163 | step 7 | loss: 0.0023680615317750197\n",
      "epoch 163 | step 8 | loss: 0.002663542197613341\n",
      "epoch 163 | step 9 | loss: 0.0029394498387528343\n",
      "epoch 163 | step 10 | loss: 0.0032302442619482814\n",
      "epoch 163 | step 11 | loss: 0.0035197189465315066\n",
      "epoch 164 | step 0 | loss: 0.0003135772114035792\n",
      "epoch 164 | step 1 | loss: 0.000579143889874171\n",
      "epoch 164 | step 2 | loss: 0.0008633076063763658\n",
      "epoch 164 | step 3 | loss: 0.0011622380822359259\n",
      "epoch 164 | step 4 | loss: 0.0014404360598835287\n",
      "epoch 164 | step 5 | loss: 0.001732097988021863\n",
      "epoch 164 | step 6 | loss: 0.002039451869805729\n",
      "epoch 164 | step 7 | loss: 0.0023340688175940323\n",
      "epoch 164 | step 8 | loss: 0.0026102304767351724\n",
      "epoch 164 | step 9 | loss: 0.0029009972182087443\n",
      "epoch 164 | step 10 | loss: 0.0032030772298739145\n",
      "epoch 164 | step 11 | loss: 0.003530317750699681\n",
      "epoch 165 | step 0 | loss: 0.00028802373056703877\n",
      "epoch 165 | step 1 | loss: 0.0005779336458318579\n",
      "epoch 165 | step 2 | loss: 0.0008802696318158122\n",
      "epoch 165 | step 3 | loss: 0.0011550446448991343\n",
      "epoch 165 | step 4 | loss: 0.0014635462180657129\n",
      "epoch 165 | step 5 | loss: 0.0017542536271549127\n",
      "epoch 165 | step 6 | loss: 0.0020442513895024303\n",
      "epoch 165 | step 7 | loss: 0.0023277191701001473\n",
      "epoch 165 | step 8 | loss: 0.0026478643791969514\n",
      "epoch 165 | step 9 | loss: 0.002952100236318506\n",
      "epoch 165 | step 10 | loss: 0.003228410226802187\n",
      "epoch 165 | step 11 | loss: 0.003520336327381247\n",
      "epoch 166 | step 0 | loss: 0.0002955956533278605\n",
      "epoch 166 | step 1 | loss: 0.0005789601553805257\n",
      "epoch 166 | step 2 | loss: 0.0008765299443735273\n",
      "epoch 166 | step 3 | loss: 0.0011924978736188596\n",
      "epoch 166 | step 4 | loss: 0.0014979239339167325\n",
      "epoch 166 | step 5 | loss: 0.0017929238316349808\n",
      "epoch 166 | step 6 | loss: 0.002070231599008579\n",
      "epoch 166 | step 7 | loss: 0.002365509210570699\n",
      "epoch 166 | step 8 | loss: 0.0026624662436019594\n",
      "epoch 166 | step 9 | loss: 0.0029352860735734356\n",
      "epoch 166 | step 10 | loss: 0.003217757015063942\n",
      "epoch 166 | step 11 | loss: 0.0035245289903805634\n",
      "epoch 167 | step 0 | loss: 0.00028949661605461366\n",
      "epoch 167 | step 1 | loss: 0.0005469266164297929\n",
      "epoch 167 | step 2 | loss: 0.0008481611963656436\n",
      "epoch 167 | step 3 | loss: 0.0011564829363393497\n",
      "epoch 167 | step 4 | loss: 0.0014795474906336543\n",
      "epoch 167 | step 5 | loss: 0.0017950582736429095\n",
      "epoch 167 | step 6 | loss: 0.002062343815028131\n",
      "epoch 167 | step 7 | loss: 0.0023470857723921234\n",
      "epoch 167 | step 8 | loss: 0.0026476373369520466\n",
      "epoch 167 | step 9 | loss: 0.0029467446584929935\n",
      "epoch 167 | step 10 | loss: 0.003237023801335502\n",
      "epoch 167 | step 11 | loss: 0.0035169482336866605\n",
      "epoch 168 | step 0 | loss: 0.00028384869754246174\n",
      "epoch 168 | step 1 | loss: 0.0006021206099819452\n",
      "epoch 168 | step 2 | loss: 0.0008824720399147595\n",
      "epoch 168 | step 3 | loss: 0.0011471347226339732\n",
      "epoch 168 | step 4 | loss: 0.001435545696532301\n",
      "epoch 168 | step 5 | loss: 0.0017577779219034519\n",
      "epoch 168 | step 6 | loss: 0.0020611386407550234\n",
      "epoch 168 | step 7 | loss: 0.0023395341792184546\n",
      "epoch 168 | step 8 | loss: 0.0026496717375159677\n",
      "epoch 168 | step 9 | loss: 0.002942223042072566\n",
      "epoch 168 | step 10 | loss: 0.003233459464476162\n",
      "epoch 168 | step 11 | loss: 0.0035183066074160824\n",
      "epoch 169 | step 0 | loss: 0.0003168575940035242\n",
      "epoch 169 | step 1 | loss: 0.0006136595760924168\n",
      "epoch 169 | step 2 | loss: 0.0009118359169059256\n",
      "epoch 169 | step 3 | loss: 0.0012135836131213387\n",
      "epoch 169 | step 4 | loss: 0.0015109677452723057\n",
      "epoch 169 | step 5 | loss: 0.0018146444597224361\n",
      "epoch 169 | step 6 | loss: 0.002078946744739442\n",
      "epoch 169 | step 7 | loss: 0.0023499261915984155\n",
      "epoch 169 | step 8 | loss: 0.002626833446478945\n",
      "epoch 169 | step 9 | loss: 0.0029203068144066898\n",
      "epoch 169 | step 10 | loss: 0.003210836789298733\n",
      "epoch 169 | step 11 | loss: 0.0035271172323610967\n",
      "epoch 170 | step 0 | loss: 0.000299642078101275\n",
      "epoch 170 | step 1 | loss: 0.0005944840977946784\n",
      "epoch 170 | step 2 | loss: 0.0008887369458747962\n",
      "epoch 170 | step 3 | loss: 0.0012164295615770498\n",
      "epoch 170 | step 4 | loss: 0.0015149455265645044\n",
      "epoch 170 | step 5 | loss: 0.0017761141304941288\n",
      "epoch 170 | step 6 | loss: 0.0020542828694076485\n",
      "epoch 170 | step 7 | loss: 0.0023623076200680825\n",
      "epoch 170 | step 8 | loss: 0.00264437246150427\n",
      "epoch 170 | step 9 | loss: 0.0029400573670349704\n",
      "epoch 170 | step 10 | loss: 0.0032160114200752536\n",
      "epoch 170 | step 11 | loss: 0.0035250241712705597\n",
      "epoch 171 | step 0 | loss: 0.00031031713680078327\n",
      "epoch 171 | step 1 | loss: 0.0005970870470594625\n",
      "epoch 171 | step 2 | loss: 0.0009128640125378104\n",
      "epoch 171 | step 3 | loss: 0.0012076800331893376\n",
      "epoch 171 | step 4 | loss: 0.0014951668915796704\n",
      "epoch 171 | step 5 | loss: 0.0017696661993956068\n",
      "epoch 171 | step 6 | loss: 0.002081359461464702\n",
      "epoch 171 | step 7 | loss: 0.0023684374486909543\n",
      "epoch 171 | step 8 | loss: 0.0026666544736897667\n",
      "epoch 171 | step 9 | loss: 0.0029408397393802673\n",
      "epoch 171 | step 10 | loss: 0.0032225258972113807\n",
      "epoch 171 | step 11 | loss: 0.003522412500746472\n",
      "epoch 172 | step 0 | loss: 0.0003149832479457376\n",
      "epoch 172 | step 1 | loss: 0.0006092281160536445\n",
      "epoch 172 | step 2 | loss: 0.0009135126575629794\n",
      "epoch 172 | step 3 | loss: 0.0012103106450151164\n",
      "epoch 172 | step 4 | loss: 0.0015291412745722269\n",
      "epoch 172 | step 5 | loss: 0.0018154678739092558\n",
      "epoch 172 | step 6 | loss: 0.0020822141601989077\n",
      "epoch 172 | step 7 | loss: 0.0023688119588680242\n",
      "epoch 172 | step 8 | loss: 0.0026589296109290964\n",
      "epoch 172 | step 9 | loss: 0.0029540900024121455\n",
      "epoch 172 | step 10 | loss: 0.003231949863213495\n",
      "epoch 172 | step 11 | loss: 0.003518737244156988\n",
      "epoch 173 | step 0 | loss: 0.0003131540125668915\n",
      "epoch 173 | step 1 | loss: 0.0005997059427107805\n",
      "epoch 173 | step 2 | loss: 0.0008949909786278071\n",
      "epoch 173 | step 3 | loss: 0.0011850237235318428\n",
      "epoch 173 | step 4 | loss: 0.0014949445371247741\n",
      "epoch 173 | step 5 | loss: 0.001800463405229745\n",
      "epoch 173 | step 6 | loss: 0.002076889839296488\n",
      "epoch 173 | step 7 | loss: 0.002360529892687052\n",
      "epoch 173 | step 8 | loss: 0.0026436459316099567\n",
      "epoch 173 | step 9 | loss: 0.002918150706582939\n",
      "epoch 173 | step 10 | loss: 0.0032107322372123973\n",
      "epoch 173 | step 11 | loss: 0.003527018003360155\n",
      "epoch 174 | step 0 | loss: 0.0002917439141948852\n",
      "epoch 174 | step 1 | loss: 0.0005913125996594361\n",
      "epoch 174 | step 2 | loss: 0.0008668064072952193\n",
      "epoch 174 | step 3 | loss: 0.0011644108547138345\n",
      "epoch 174 | step 4 | loss: 0.0014727571676163872\n",
      "epoch 174 | step 5 | loss: 0.0017640733139977368\n",
      "epoch 174 | step 6 | loss: 0.002048166504193058\n",
      "epoch 174 | step 7 | loss: 0.002345411946881451\n",
      "epoch 174 | step 8 | loss: 0.0026381017966981488\n",
      "epoch 174 | step 9 | loss: 0.0029295249030760633\n",
      "epoch 174 | step 10 | loss: 0.003234116501282046\n",
      "epoch 174 | step 11 | loss: 0.0035177887673973245\n",
      "epoch 175 | step 0 | loss: 0.0002784302425096418\n",
      "epoch 175 | step 1 | loss: 0.0005785344285356807\n",
      "epoch 175 | step 2 | loss: 0.0008610093769798741\n",
      "epoch 175 | step 3 | loss: 0.0011888988270617805\n",
      "epoch 175 | step 4 | loss: 0.0014765433218970254\n",
      "epoch 175 | step 5 | loss: 0.0017662675303337274\n",
      "epoch 175 | step 6 | loss: 0.0020815049141961715\n",
      "epoch 175 | step 7 | loss: 0.0023853455403522058\n",
      "epoch 175 | step 8 | loss: 0.0026436165037297676\n",
      "epoch 175 | step 9 | loss: 0.002935306742202757\n",
      "epoch 175 | step 10 | loss: 0.003219084566085563\n",
      "epoch 175 | step 11 | loss: 0.0035236979626039216\n",
      "epoch 176 | step 0 | loss: 0.00028024025701135714\n",
      "epoch 176 | step 1 | loss: 0.0005717912539710301\n",
      "epoch 176 | step 2 | loss: 0.0008501734254410468\n",
      "epoch 176 | step 3 | loss: 0.0011582233842754891\n",
      "epoch 176 | step 4 | loss: 0.0014584968921346642\n",
      "epoch 176 | step 5 | loss: 0.0017796854065429686\n",
      "epoch 176 | step 6 | loss: 0.0020911119601407934\n",
      "epoch 176 | step 7 | loss: 0.0023749046730854304\n",
      "epoch 176 | step 8 | loss: 0.002683387777362401\n",
      "epoch 176 | step 9 | loss: 0.00295381434642403\n",
      "epoch 176 | step 10 | loss: 0.003240090550066986\n",
      "epoch 176 | step 11 | loss: 0.0035154474354626916\n",
      "epoch 177 | step 0 | loss: 0.00029183939476834725\n",
      "epoch 177 | step 1 | loss: 0.0006232364983353217\n",
      "epoch 177 | step 2 | loss: 0.0009077942920290933\n",
      "epoch 177 | step 3 | loss: 0.0011907437988499352\n",
      "epoch 177 | step 4 | loss: 0.0014449440233536555\n",
      "epoch 177 | step 5 | loss: 0.0017588146169216859\n",
      "epoch 177 | step 6 | loss: 0.002065235606421875\n",
      "epoch 177 | step 7 | loss: 0.0023564699748592384\n",
      "epoch 177 | step 8 | loss: 0.002639137861066018\n",
      "epoch 177 | step 9 | loss: 0.0029063373986025864\n",
      "epoch 177 | step 10 | loss: 0.0032218324627670443\n",
      "epoch 177 | step 11 | loss: 0.0035225863277945912\n",
      "epoch 178 | step 0 | loss: 0.0002724140319242677\n",
      "epoch 178 | step 1 | loss: 0.0005682698170439409\n",
      "epoch 178 | step 2 | loss: 0.0008562264586858829\n",
      "epoch 178 | step 3 | loss: 0.001180110483008416\n",
      "epoch 178 | step 4 | loss: 0.00147595281613701\n",
      "epoch 178 | step 5 | loss: 0.0017623396940839374\n",
      "epoch 178 | step 6 | loss: 0.0020319665857891104\n",
      "epoch 178 | step 7 | loss: 0.002336184956205544\n",
      "epoch 178 | step 8 | loss: 0.002643199509791224\n",
      "epoch 178 | step 9 | loss: 0.002944549722062036\n",
      "epoch 178 | step 10 | loss: 0.0032322094012963245\n",
      "epoch 178 | step 11 | loss: 0.003518525525050952\n",
      "epoch 179 | step 0 | loss: 0.0002824410586244186\n",
      "epoch 179 | step 1 | loss: 0.0005722323484471014\n",
      "epoch 179 | step 2 | loss: 0.000856482481731847\n",
      "epoch 179 | step 3 | loss: 0.0011533453079338937\n",
      "epoch 179 | step 4 | loss: 0.0014521838191721328\n",
      "epoch 179 | step 5 | loss: 0.001753605214071613\n",
      "epoch 179 | step 6 | loss: 0.002055019470097812\n",
      "epoch 179 | step 7 | loss: 0.002330497395124737\n",
      "epoch 179 | step 8 | loss: 0.0026267657781793687\n",
      "epoch 179 | step 9 | loss: 0.0029186797581230418\n",
      "epoch 179 | step 10 | loss: 0.003209339037598901\n",
      "epoch 179 | step 11 | loss: 0.003527427533800704\n",
      "epoch 180 | step 0 | loss: 0.0002974553337614657\n",
      "epoch 180 | step 1 | loss: 0.0006072378588753656\n",
      "epoch 180 | step 2 | loss: 0.0008996989920715609\n",
      "epoch 180 | step 3 | loss: 0.0011742535873099323\n",
      "epoch 180 | step 4 | loss: 0.0014795724981042375\n",
      "epoch 180 | step 5 | loss: 0.0017558085641284857\n",
      "epoch 180 | step 6 | loss: 0.002045845809801552\n",
      "epoch 180 | step 7 | loss: 0.002338156174318119\n",
      "epoch 180 | step 8 | loss: 0.0026296820471736265\n",
      "epoch 180 | step 9 | loss: 0.002907648588457018\n",
      "epoch 180 | step 10 | loss: 0.00319186630370792\n",
      "epoch 180 | step 11 | loss: 0.003534158354794479\n",
      "epoch 181 | step 0 | loss: 0.0002971482150628564\n",
      "epoch 181 | step 1 | loss: 0.0005797752013830398\n",
      "epoch 181 | step 2 | loss: 0.0008744129339475732\n",
      "epoch 181 | step 3 | loss: 0.0011547650109676772\n",
      "epoch 181 | step 4 | loss: 0.0014456670437034407\n",
      "epoch 181 | step 5 | loss: 0.0017312535201142831\n",
      "epoch 181 | step 6 | loss: 0.0020297533882906965\n",
      "epoch 181 | step 7 | loss: 0.002321997830064322\n",
      "epoch 181 | step 8 | loss: 0.0026117424271949333\n",
      "epoch 181 | step 9 | loss: 0.002909049328444204\n",
      "epoch 181 | step 10 | loss: 0.0032267026250397423\n",
      "epoch 181 | step 11 | loss: 0.0035205316623237886\n",
      "epoch 182 | step 0 | loss: 0.00028898015801541026\n",
      "epoch 182 | step 1 | loss: 0.0005789253787452655\n",
      "epoch 182 | step 2 | loss: 0.0008881021036599771\n",
      "epoch 182 | step 3 | loss: 0.00117593945335259\n",
      "epoch 182 | step 4 | loss: 0.0015016215596616335\n",
      "epoch 182 | step 5 | loss: 0.0017730518459732954\n",
      "epoch 182 | step 6 | loss: 0.0020606833727573183\n",
      "epoch 182 | step 7 | loss: 0.0023453804615434022\n",
      "epoch 182 | step 8 | loss: 0.0026630892583940106\n",
      "epoch 182 | step 9 | loss: 0.0029577926496741585\n",
      "epoch 182 | step 10 | loss: 0.0032340942434777253\n",
      "epoch 182 | step 11 | loss: 0.003517618189725407\n",
      "epoch 183 | step 0 | loss: 0.00027635121311305426\n",
      "epoch 183 | step 1 | loss: 0.0005669142244092652\n",
      "epoch 183 | step 2 | loss: 0.0008698430539005953\n",
      "epoch 183 | step 3 | loss: 0.0011830314374168271\n",
      "epoch 183 | step 4 | loss: 0.0014621446814285701\n",
      "epoch 183 | step 5 | loss: 0.0017505244892021658\n",
      "epoch 183 | step 6 | loss: 0.0020545794514443003\n",
      "epoch 183 | step 7 | loss: 0.002361816049190924\n",
      "epoch 183 | step 8 | loss: 0.0026167197004950626\n",
      "epoch 183 | step 9 | loss: 0.0029255833318899875\n",
      "epoch 183 | step 10 | loss: 0.003213226941014502\n",
      "epoch 183 | step 11 | loss: 0.003525740212385145\n",
      "epoch 184 | step 0 | loss: 0.0002829325734047384\n",
      "epoch 184 | step 1 | loss: 0.000601126085118437\n",
      "epoch 184 | step 2 | loss: 0.0008982829501788185\n",
      "epoch 184 | step 3 | loss: 0.001190546409787689\n",
      "epoch 184 | step 4 | loss: 0.0014914456781698394\n",
      "epoch 184 | step 5 | loss: 0.001819312588525298\n",
      "epoch 184 | step 6 | loss: 0.0020892018129959316\n",
      "epoch 184 | step 7 | loss: 0.002356627627874765\n",
      "epoch 184 | step 8 | loss: 0.0026395032545345255\n",
      "epoch 184 | step 9 | loss: 0.002916426902077545\n",
      "epoch 184 | step 10 | loss: 0.0032464304385987908\n",
      "epoch 184 | step 11 | loss: 0.0035127215007780257\n",
      "epoch 185 | step 0 | loss: 0.00028845038757591027\n",
      "epoch 185 | step 1 | loss: 0.0005662372936056303\n",
      "epoch 185 | step 2 | loss: 0.0008300392736026726\n",
      "epoch 185 | step 3 | loss: 0.001134049279854294\n",
      "epoch 185 | step 4 | loss: 0.0014264488150078371\n",
      "epoch 185 | step 5 | loss: 0.00169646862684982\n",
      "epoch 185 | step 6 | loss: 0.0020083985926775804\n",
      "epoch 185 | step 7 | loss: 0.002330148832421424\n",
      "epoch 185 | step 8 | loss: 0.002611921282432672\n",
      "epoch 185 | step 9 | loss: 0.002901099808109679\n",
      "epoch 185 | step 10 | loss: 0.0032316663664688785\n",
      "epoch 185 | step 11 | loss: 0.003518466014773218\n",
      "epoch 186 | step 0 | loss: 0.0002856387827118583\n",
      "epoch 186 | step 1 | loss: 0.000603023832263862\n",
      "epoch 186 | step 2 | loss: 0.0009148146412432127\n",
      "epoch 186 | step 3 | loss: 0.0012274962450291704\n",
      "epoch 186 | step 4 | loss: 0.0015046058794056663\n",
      "epoch 186 | step 5 | loss: 0.0017872509085867581\n",
      "epoch 186 | step 6 | loss: 0.0020538324622063294\n",
      "epoch 186 | step 7 | loss: 0.0023425485180979056\n",
      "epoch 186 | step 8 | loss: 0.0026356780666562853\n",
      "epoch 186 | step 9 | loss: 0.0029432600057222634\n",
      "epoch 186 | step 10 | loss: 0.0032503441892336622\n",
      "epoch 186 | step 11 | loss: 0.0035111356054171733\n",
      "epoch 187 | step 0 | loss: 0.00027155111823111595\n",
      "epoch 187 | step 1 | loss: 0.0005660903221535128\n",
      "epoch 187 | step 2 | loss: 0.0008552461239170414\n",
      "epoch 187 | step 3 | loss: 0.0011243671837707376\n",
      "epoch 187 | step 4 | loss: 0.001420575217424423\n",
      "epoch 187 | step 5 | loss: 0.0017186367173896758\n",
      "epoch 187 | step 6 | loss: 0.002003071646122956\n",
      "epoch 187 | step 7 | loss: 0.002322117267831182\n",
      "epoch 187 | step 8 | loss: 0.002609172062133561\n",
      "epoch 187 | step 9 | loss: 0.0029143001191329595\n",
      "epoch 187 | step 10 | loss: 0.003230415087111242\n",
      "epoch 187 | step 11 | loss: 0.0035188278913545804\n",
      "epoch 188 | step 0 | loss: 0.0002741578235417031\n",
      "epoch 188 | step 1 | loss: 0.0005747640556158226\n",
      "epoch 188 | step 2 | loss: 0.0008511785464248998\n",
      "epoch 188 | step 3 | loss: 0.0011454203937376094\n",
      "epoch 188 | step 4 | loss: 0.001445582572178903\n",
      "epoch 188 | step 5 | loss: 0.0017544171162651476\n",
      "epoch 188 | step 6 | loss: 0.0020500443251156514\n",
      "epoch 188 | step 7 | loss: 0.0023795393819839687\n",
      "epoch 188 | step 8 | loss: 0.0026531705901067493\n",
      "epoch 188 | step 9 | loss: 0.0029172490222433285\n",
      "epoch 188 | step 10 | loss: 0.0032121335707474747\n",
      "epoch 188 | step 11 | loss: 0.0035259978546592443\n",
      "epoch 189 | step 0 | loss: 0.00030572621159410875\n",
      "epoch 189 | step 1 | loss: 0.0005843867131404727\n",
      "epoch 189 | step 2 | loss: 0.000879730515557325\n",
      "epoch 189 | step 3 | loss: 0.001172179712831543\n",
      "epoch 189 | step 4 | loss: 0.0014526374681157847\n",
      "epoch 189 | step 5 | loss: 0.0017303315063048347\n",
      "epoch 189 | step 6 | loss: 0.002048551813577434\n",
      "epoch 189 | step 7 | loss: 0.0023552318456524115\n",
      "epoch 189 | step 8 | loss: 0.0026382265438831378\n",
      "epoch 189 | step 9 | loss: 0.002924100386985336\n",
      "epoch 189 | step 10 | loss: 0.003216125201939196\n",
      "epoch 189 | step 11 | loss: 0.003524370071879189\n",
      "epoch 190 | step 0 | loss: 0.0002947494577222153\n",
      "epoch 190 | step 1 | loss: 0.0005965505825123787\n",
      "epoch 190 | step 2 | loss: 0.0008910112905652223\n",
      "epoch 190 | step 3 | loss: 0.001201156070476213\n",
      "epoch 190 | step 4 | loss: 0.0014677030560843064\n",
      "epoch 190 | step 5 | loss: 0.0017509487562454805\n",
      "epoch 190 | step 6 | loss: 0.0020342338207605505\n",
      "epoch 190 | step 7 | loss: 0.002349428072980066\n",
      "epoch 190 | step 8 | loss: 0.002638392865195129\n",
      "epoch 190 | step 9 | loss: 0.002935425532948917\n",
      "epoch 190 | step 10 | loss: 0.00323901076214462\n",
      "epoch 190 | step 11 | loss: 0.0035154335550457863\n",
      "epoch 191 | step 0 | loss: 0.0002979814897922233\n",
      "epoch 191 | step 1 | loss: 0.0005739357174062053\n",
      "epoch 191 | step 2 | loss: 0.0008674771984840384\n",
      "epoch 191 | step 3 | loss: 0.0011745747754486574\n",
      "epoch 191 | step 4 | loss: 0.0014595048338123982\n",
      "epoch 191 | step 5 | loss: 0.0017531922039412044\n",
      "epoch 191 | step 6 | loss: 0.002067515337197726\n",
      "epoch 191 | step 7 | loss: 0.0023442564671954697\n",
      "epoch 191 | step 8 | loss: 0.0026399195071363517\n",
      "epoch 191 | step 9 | loss: 0.0029263512248000694\n",
      "epoch 191 | step 10 | loss: 0.0032359214325743202\n",
      "epoch 191 | step 11 | loss: 0.003516552386508764\n",
      "epoch 192 | step 0 | loss: 0.00031814992258021057\n",
      "epoch 192 | step 1 | loss: 0.0005929559282551519\n",
      "epoch 192 | step 2 | loss: 0.0009052490975474701\n",
      "epoch 192 | step 3 | loss: 0.0012150863647802754\n",
      "epoch 192 | step 4 | loss: 0.0014909518450816651\n",
      "epoch 192 | step 5 | loss: 0.0018007533870555508\n",
      "epoch 192 | step 6 | loss: 0.002082409070430396\n",
      "epoch 192 | step 7 | loss: 0.0023940011468471687\n",
      "epoch 192 | step 8 | loss: 0.002701972133864081\n",
      "epoch 192 | step 9 | loss: 0.0029827557652422716\n",
      "epoch 192 | step 10 | loss: 0.003258452525323416\n",
      "epoch 192 | step 11 | loss: 0.003507811994536992\n",
      "epoch 193 | step 0 | loss: 0.00032100832789429935\n",
      "epoch 193 | step 1 | loss: 0.0006246018844954318\n",
      "epoch 193 | step 2 | loss: 0.0008876139180038113\n",
      "epoch 193 | step 3 | loss: 0.0011825650437650302\n",
      "epoch 193 | step 4 | loss: 0.0014703762349109744\n",
      "epoch 193 | step 5 | loss: 0.0017537643352601943\n",
      "epoch 193 | step 6 | loss: 0.0020284901776473256\n",
      "epoch 193 | step 7 | loss: 0.0023213072542671376\n",
      "epoch 193 | step 8 | loss: 0.0026251049945343226\n",
      "epoch 193 | step 9 | loss: 0.0029341181972655887\n",
      "epoch 193 | step 10 | loss: 0.003229754792969054\n",
      "epoch 193 | step 11 | loss: 0.003518971824925933\n",
      "epoch 194 | step 0 | loss: 0.00029149439596993257\n",
      "epoch 194 | step 1 | loss: 0.0005544106422770451\n",
      "epoch 194 | step 2 | loss: 0.0008295527104733256\n",
      "epoch 194 | step 3 | loss: 0.0011070605340482717\n",
      "epoch 194 | step 4 | loss: 0.0013742250617806283\n",
      "epoch 194 | step 5 | loss: 0.0016645963724989624\n",
      "epoch 194 | step 6 | loss: 0.001967462700039541\n",
      "epoch 194 | step 7 | loss: 0.0022687569910735377\n",
      "epoch 194 | step 8 | loss: 0.0025770987830439985\n",
      "epoch 194 | step 9 | loss: 0.0029192806217875093\n",
      "epoch 194 | step 10 | loss: 0.0032146530665548394\n",
      "epoch 194 | step 11 | loss: 0.003525017833142254\n",
      "epoch 195 | step 0 | loss: 0.0002802120252817559\n",
      "epoch 195 | step 1 | loss: 0.0005911732897816123\n",
      "epoch 195 | step 2 | loss: 0.0008986469427978552\n",
      "epoch 195 | step 3 | loss: 0.0011674493706648862\n",
      "epoch 195 | step 4 | loss: 0.001477722931574011\n",
      "epoch 195 | step 5 | loss: 0.0017725493323207616\n",
      "epoch 195 | step 6 | loss: 0.0020453887685997524\n",
      "epoch 195 | step 7 | loss: 0.0023347539620272592\n",
      "epoch 195 | step 8 | loss: 0.002632467052798374\n",
      "epoch 195 | step 9 | loss: 0.0029333066773958726\n",
      "epoch 195 | step 10 | loss: 0.0032296805840110257\n",
      "epoch 195 | step 11 | loss: 0.0035188870242296873\n",
      "epoch 196 | step 0 | loss: 0.00028180550581967617\n",
      "epoch 196 | step 1 | loss: 0.0005893484978244531\n",
      "epoch 196 | step 2 | loss: 0.000892766745425088\n",
      "epoch 196 | step 3 | loss: 0.0011917411418546776\n",
      "epoch 196 | step 4 | loss: 0.0014949049388403774\n",
      "epoch 196 | step 5 | loss: 0.0017876691308267862\n",
      "epoch 196 | step 6 | loss: 0.00208403890444881\n",
      "epoch 196 | step 7 | loss: 0.0023581063051583604\n",
      "epoch 196 | step 8 | loss: 0.0026661773634626207\n",
      "epoch 196 | step 9 | loss: 0.0029621647662869876\n",
      "epoch 196 | step 10 | loss: 0.0032201024756429155\n",
      "epoch 196 | step 11 | loss: 0.0035226155877904307\n",
      "epoch 197 | step 0 | loss: 0.0003041809288546592\n",
      "epoch 197 | step 1 | loss: 0.0006071671119095896\n",
      "epoch 197 | step 2 | loss: 0.0008754233186666491\n",
      "epoch 197 | step 3 | loss: 0.0011679368371536917\n",
      "epoch 197 | step 4 | loss: 0.001458666682926723\n",
      "epoch 197 | step 5 | loss: 0.0017530044320492038\n",
      "epoch 197 | step 6 | loss: 0.0020419499454903023\n",
      "epoch 197 | step 7 | loss: 0.0023518669945676246\n",
      "epoch 197 | step 8 | loss: 0.002632906880238916\n",
      "epoch 197 | step 9 | loss: 0.002933030826053608\n",
      "epoch 197 | step 10 | loss: 0.003216590753393491\n",
      "epoch 197 | step 11 | loss: 0.0035239305389081605\n",
      "epoch 198 | step 0 | loss: 0.0002755734019918592\n",
      "epoch 198 | step 1 | loss: 0.0005729530223541231\n",
      "epoch 198 | step 2 | loss: 0.0008623701336397893\n",
      "epoch 198 | step 3 | loss: 0.0011442415152022737\n",
      "epoch 198 | step 4 | loss: 0.0014338469800181802\n",
      "epoch 198 | step 5 | loss: 0.001700995805581552\n",
      "epoch 198 | step 6 | loss: 0.0019853359908097385\n",
      "epoch 198 | step 7 | loss: 0.002285140540322613\n",
      "epoch 198 | step 8 | loss: 0.0025961921004434125\n",
      "epoch 198 | step 9 | loss: 0.0028863795550005318\n",
      "epoch 198 | step 10 | loss: 0.003213891889904541\n",
      "epoch 198 | step 11 | loss: 0.0035250439802647403\n",
      "epoch 199 | step 0 | loss: 0.00028804226030932343\n",
      "epoch 199 | step 1 | loss: 0.0005958651570770974\n",
      "epoch 199 | step 2 | loss: 0.0008771002263489579\n",
      "epoch 199 | step 3 | loss: 0.0011709606091864254\n",
      "epoch 199 | step 4 | loss: 0.001476962868010487\n",
      "epoch 199 | step 5 | loss: 0.0017611870356316467\n",
      "epoch 199 | step 6 | loss: 0.0020547825847589568\n",
      "epoch 199 | step 7 | loss: 0.002361104501450495\n",
      "epoch 199 | step 8 | loss: 0.0026367476229103024\n",
      "epoch 199 | step 9 | loss: 0.0029257061268109443\n",
      "epoch 199 | step 10 | loss: 0.0032199253406691554\n",
      "epoch 199 | step 11 | loss: 0.0035225915371596035\n",
      "epoch 200 | step 0 | loss: 0.00030063404140437546\n",
      "epoch 200 | step 1 | loss: 0.0006085300227361142\n",
      "epoch 200 | step 2 | loss: 0.0008716020404397052\n",
      "epoch 200 | step 3 | loss: 0.0011975960621837096\n",
      "epoch 200 | step 4 | loss: 0.0014683959268855134\n",
      "epoch 200 | step 5 | loss: 0.001759312237899529\n",
      "epoch 200 | step 6 | loss: 0.002046151175983578\n",
      "epoch 200 | step 7 | loss: 0.00230264821328691\n",
      "epoch 200 | step 8 | loss: 0.0026071332740291975\n",
      "epoch 200 | step 9 | loss: 0.0029166257123416834\n",
      "epoch 200 | step 10 | loss: 0.003205984443936656\n",
      "epoch 200 | step 11 | loss: 0.003528033079217614\n",
      "epoch 201 | step 0 | loss: 0.0003032637660624151\n",
      "epoch 201 | step 1 | loss: 0.0006219896775951243\n",
      "epoch 201 | step 2 | loss: 0.000900719100346524\n",
      "epoch 201 | step 3 | loss: 0.0011836487069984253\n",
      "epoch 201 | step 4 | loss: 0.0014853812617318582\n",
      "epoch 201 | step 5 | loss: 0.0017751922854437909\n",
      "epoch 201 | step 6 | loss: 0.0020795648821911833\n",
      "epoch 201 | step 7 | loss: 0.002364862205903161\n",
      "epoch 201 | step 8 | loss: 0.0026401702181764095\n",
      "epoch 201 | step 9 | loss: 0.0029465640123174973\n",
      "epoch 201 | step 10 | loss: 0.0032112957395915064\n",
      "epoch 201 | step 11 | loss: 0.0035259357135225414\n",
      "epoch 202 | step 0 | loss: 0.0002878363449829883\n",
      "epoch 202 | step 1 | loss: 0.0005833544698922277\n",
      "epoch 202 | step 2 | loss: 0.0008808502306432216\n",
      "epoch 202 | step 3 | loss: 0.0011813758893434172\n",
      "epoch 202 | step 4 | loss: 0.0014434306346001848\n",
      "epoch 202 | step 5 | loss: 0.0017638878424307343\n",
      "epoch 202 | step 6 | loss: 0.0020584741531375064\n",
      "epoch 202 | step 7 | loss: 0.002364348269679949\n",
      "epoch 202 | step 8 | loss: 0.002654366262094645\n",
      "epoch 202 | step 9 | loss: 0.0029291287516799705\n",
      "epoch 202 | step 10 | loss: 0.0032233092295917724\n",
      "epoch 202 | step 11 | loss: 0.0035213091455368177\n",
      "epoch 203 | step 0 | loss: 0.0003004101881408005\n",
      "epoch 203 | step 1 | loss: 0.0006184867553598644\n",
      "epoch 203 | step 2 | loss: 0.0009278522881664304\n",
      "epoch 203 | step 3 | loss: 0.0011899195830338896\n",
      "epoch 203 | step 4 | loss: 0.0014374987741483766\n",
      "epoch 203 | step 5 | loss: 0.0017180351322166297\n",
      "epoch 203 | step 6 | loss: 0.0020203976772163153\n",
      "epoch 203 | step 7 | loss: 0.002304774805466849\n",
      "epoch 203 | step 8 | loss: 0.0026290628328293677\n",
      "epoch 203 | step 9 | loss: 0.0029442598111829683\n",
      "epoch 203 | step 10 | loss: 0.0032254565539274757\n",
      "epoch 203 | step 11 | loss: 0.003520249780412897\n",
      "epoch 204 | step 0 | loss: 0.0002731834415509609\n",
      "epoch 204 | step 1 | loss: 0.0005753173619987336\n",
      "epoch 204 | step 2 | loss: 0.0008923848603718148\n",
      "epoch 204 | step 3 | loss: 0.0011678687198215406\n",
      "epoch 204 | step 4 | loss: 0.0014798363060676968\n",
      "epoch 204 | step 5 | loss: 0.0017617951626022837\n",
      "epoch 204 | step 6 | loss: 0.0020504036004037537\n",
      "epoch 204 | step 7 | loss: 0.0023581860921089723\n",
      "epoch 204 | step 8 | loss: 0.0026364769532637986\n",
      "epoch 204 | step 9 | loss: 0.0029147255258029935\n",
      "epoch 204 | step 10 | loss: 0.0032277678712953938\n",
      "epoch 204 | step 11 | loss: 0.003519319832019546\n",
      "epoch 205 | step 0 | loss: 0.0003174799397482804\n",
      "epoch 205 | step 1 | loss: 0.0006024799324514286\n",
      "epoch 205 | step 2 | loss: 0.0009378342602138618\n",
      "epoch 205 | step 3 | loss: 0.0012089500388158647\n",
      "epoch 205 | step 4 | loss: 0.0014764049863607333\n",
      "epoch 205 | step 5 | loss: 0.0017441013161940504\n",
      "epoch 205 | step 6 | loss: 0.002013744231465709\n",
      "epoch 205 | step 7 | loss: 0.0023299995105351533\n",
      "epoch 205 | step 8 | loss: 0.002626808192831718\n",
      "epoch 205 | step 9 | loss: 0.0029415703147590427\n",
      "epoch 205 | step 10 | loss: 0.0032260839680867204\n",
      "epoch 205 | step 11 | loss: 0.0035199388892150777\n",
      "epoch 206 | step 0 | loss: 0.0002911048021681629\n",
      "epoch 206 | step 1 | loss: 0.0005792088288193445\n",
      "epoch 206 | step 2 | loss: 0.0008868336312516871\n",
      "epoch 206 | step 3 | loss: 0.0011701701384878467\n",
      "epoch 206 | step 4 | loss: 0.0014605954785052726\n",
      "epoch 206 | step 5 | loss: 0.001766832611304747\n",
      "epoch 206 | step 6 | loss: 0.0020489398911575393\n",
      "epoch 206 | step 7 | loss: 0.0023663474801996548\n",
      "epoch 206 | step 8 | loss: 0.0026506160599997775\n",
      "epoch 206 | step 9 | loss: 0.002955321684549886\n",
      "epoch 206 | step 10 | loss: 0.0032343162288836873\n",
      "epoch 206 | step 11 | loss: 0.0035167023148773104\n",
      "epoch 207 | step 0 | loss: 0.00031312932920636355\n",
      "epoch 207 | step 1 | loss: 0.0006076028128753304\n",
      "epoch 207 | step 2 | loss: 0.0008963122391546591\n",
      "epoch 207 | step 3 | loss: 0.0011781618577801655\n",
      "epoch 207 | step 4 | loss: 0.00148184491760558\n",
      "epoch 207 | step 5 | loss: 0.0017565454914886141\n",
      "epoch 207 | step 6 | loss: 0.0020406674301005393\n",
      "epoch 207 | step 7 | loss: 0.002316504011839597\n",
      "epoch 207 | step 8 | loss: 0.0026313829558734826\n",
      "epoch 207 | step 9 | loss: 0.0029296608307475733\n",
      "epoch 207 | step 10 | loss: 0.003226291591150382\n",
      "epoch 207 | step 11 | loss: 0.00351982714647881\n",
      "epoch 208 | step 0 | loss: 0.00030385433283886324\n",
      "epoch 208 | step 1 | loss: 0.000605215425748957\n",
      "epoch 208 | step 2 | loss: 0.0008796301864063561\n",
      "epoch 208 | step 3 | loss: 0.0011755010634012773\n",
      "epoch 208 | step 4 | loss: 0.0014492523524526175\n",
      "epoch 208 | step 5 | loss: 0.001771353943732449\n",
      "epoch 208 | step 6 | loss: 0.0020553419165714507\n",
      "epoch 208 | step 7 | loss: 0.0023662126261528985\n",
      "epoch 208 | step 8 | loss: 0.002676949534588865\n",
      "epoch 208 | step 9 | loss: 0.0029462969524550616\n",
      "epoch 208 | step 10 | loss: 0.0032301043940079315\n",
      "epoch 208 | step 11 | loss: 0.0035182432961083617\n",
      "epoch 209 | step 0 | loss: 0.0002751922678803827\n",
      "epoch 209 | step 1 | loss: 0.0005915985026344313\n",
      "epoch 209 | step 2 | loss: 0.0008804723148295717\n",
      "epoch 209 | step 3 | loss: 0.0011585586856930519\n",
      "epoch 209 | step 4 | loss: 0.0014474882078012315\n",
      "epoch 209 | step 5 | loss: 0.0017504844868545693\n",
      "epoch 209 | step 6 | loss: 0.0020722018059276025\n",
      "epoch 209 | step 7 | loss: 0.0023559394199698393\n",
      "epoch 209 | step 8 | loss: 0.002646872376090527\n",
      "epoch 209 | step 9 | loss: 0.002916381301912422\n",
      "epoch 209 | step 10 | loss: 0.0032183483050962276\n",
      "epoch 209 | step 11 | loss: 0.0035227748789986462\n",
      "epoch 210 | step 0 | loss: 0.0002854166534935014\n",
      "epoch 210 | step 1 | loss: 0.0005779638064772265\n",
      "epoch 210 | step 2 | loss: 0.0008543996673258086\n",
      "epoch 210 | step 3 | loss: 0.0011685860711950816\n",
      "epoch 210 | step 4 | loss: 0.0014895878101167594\n",
      "epoch 210 | step 5 | loss: 0.0017646988748661013\n",
      "epoch 210 | step 6 | loss: 0.002064943842173206\n",
      "epoch 210 | step 7 | loss: 0.0023454802387559747\n",
      "epoch 210 | step 8 | loss: 0.0026510423157636957\n",
      "epoch 210 | step 9 | loss: 0.0029223798332052783\n",
      "epoch 210 | step 10 | loss: 0.003230273233649336\n",
      "epoch 210 | step 11 | loss: 0.0035181732275397616\n",
      "epoch 211 | step 0 | loss: 0.0002961323250982611\n",
      "epoch 211 | step 1 | loss: 0.0005830922310977607\n",
      "epoch 211 | step 2 | loss: 0.0008873591682341108\n",
      "epoch 211 | step 3 | loss: 0.001194199822998555\n",
      "epoch 211 | step 4 | loss: 0.0014973658280862533\n",
      "epoch 211 | step 5 | loss: 0.001777153615585976\n",
      "epoch 211 | step 6 | loss: 0.0020551663143645532\n",
      "epoch 211 | step 7 | loss: 0.0023395883327332074\n",
      "epoch 211 | step 8 | loss: 0.0026517133588934987\n",
      "epoch 211 | step 9 | loss: 0.002944667744914531\n",
      "epoch 211 | step 10 | loss: 0.0032168469573970624\n",
      "epoch 211 | step 11 | loss: 0.003523261236296233\n",
      "epoch 212 | step 0 | loss: 0.00029136073559833263\n",
      "epoch 212 | step 1 | loss: 0.0005955879186603453\n",
      "epoch 212 | step 2 | loss: 0.0008752279288887962\n",
      "epoch 212 | step 3 | loss: 0.0011568243936452283\n",
      "epoch 212 | step 4 | loss: 0.001436602698290974\n",
      "epoch 212 | step 5 | loss: 0.0017602955995860256\n",
      "epoch 212 | step 6 | loss: 0.0020112947959801263\n",
      "epoch 212 | step 7 | loss: 0.0022991858005802866\n",
      "epoch 212 | step 8 | loss: 0.0026211623527152826\n",
      "epoch 212 | step 9 | loss: 0.0029048380899758504\n",
      "epoch 212 | step 10 | loss: 0.0032245286041783783\n",
      "epoch 212 | step 11 | loss: 0.003520290487117772\n",
      "epoch 213 | step 0 | loss: 0.00031502828176270433\n",
      "epoch 213 | step 1 | loss: 0.0005954476062493408\n",
      "epoch 213 | step 2 | loss: 0.0008868446641374153\n",
      "epoch 213 | step 3 | loss: 0.0011769343882192113\n",
      "epoch 213 | step 4 | loss: 0.0014662998649989994\n",
      "epoch 213 | step 5 | loss: 0.0017569410398460088\n",
      "epoch 213 | step 6 | loss: 0.002078992527393608\n",
      "epoch 213 | step 7 | loss: 0.0023533968306077594\n",
      "epoch 213 | step 8 | loss: 0.0026176546426552516\n",
      "epoch 213 | step 9 | loss: 0.00290877644251288\n",
      "epoch 213 | step 10 | loss: 0.0032112086501384814\n",
      "epoch 213 | step 11 | loss: 0.0035254576988915784\n",
      "epoch 214 | step 0 | loss: 0.00028763840081100575\n",
      "epoch 214 | step 1 | loss: 0.0005875174309864213\n",
      "epoch 214 | step 2 | loss: 0.0008687274968562141\n",
      "epoch 214 | step 3 | loss: 0.0011437732893937401\n",
      "epoch 214 | step 4 | loss: 0.0014388082332784514\n",
      "epoch 214 | step 5 | loss: 0.0017506745896730773\n",
      "epoch 214 | step 6 | loss: 0.0020371461562395344\n",
      "epoch 214 | step 7 | loss: 0.0023281454035232476\n",
      "epoch 214 | step 8 | loss: 0.002645468943939395\n",
      "epoch 214 | step 9 | loss: 0.00292314362942767\n",
      "epoch 214 | step 10 | loss: 0.0032244590452616025\n",
      "epoch 214 | step 11 | loss: 0.003520243712010867\n",
      "epoch 215 | step 0 | loss: 0.00030422540813714586\n",
      "epoch 215 | step 1 | loss: 0.0006131497839550482\n",
      "epoch 215 | step 2 | loss: 0.0008787059994181288\n",
      "epoch 215 | step 3 | loss: 0.00115576244251587\n",
      "epoch 215 | step 4 | loss: 0.001424733548734414\n",
      "epoch 215 | step 5 | loss: 0.0017026986678762442\n",
      "epoch 215 | step 6 | loss: 0.0020045765626146685\n",
      "epoch 215 | step 7 | loss: 0.0023012297655219858\n",
      "epoch 215 | step 8 | loss: 0.002614515667498517\n",
      "epoch 215 | step 9 | loss: 0.00291282364874818\n",
      "epoch 215 | step 10 | loss: 0.0032180944859091175\n",
      "epoch 215 | step 11 | loss: 0.0035226609862689245\n",
      "epoch 216 | step 0 | loss: 0.0002933883606970907\n",
      "epoch 216 | step 1 | loss: 0.0005905243629831011\n",
      "epoch 216 | step 2 | loss: 0.0008807924745461663\n",
      "epoch 216 | step 3 | loss: 0.0011830993629211163\n",
      "epoch 216 | step 4 | loss: 0.001445520792826637\n",
      "epoch 216 | step 5 | loss: 0.0017165309205479922\n",
      "epoch 216 | step 6 | loss: 0.001982719904609559\n",
      "epoch 216 | step 7 | loss: 0.0022758668664737804\n",
      "epoch 216 | step 8 | loss: 0.0025656381641138513\n",
      "epoch 216 | step 9 | loss: 0.0028815642235406015\n",
      "epoch 216 | step 10 | loss: 0.0032115666911231135\n",
      "epoch 216 | step 11 | loss: 0.0035251725726756404\n",
      "epoch 217 | step 0 | loss: 0.00029576228424753207\n",
      "epoch 217 | step 1 | loss: 0.0006031778655340049\n",
      "epoch 217 | step 2 | loss: 0.0008814323175943513\n",
      "epoch 217 | step 3 | loss: 0.0011763125453647014\n",
      "epoch 217 | step 4 | loss: 0.0014924382718314178\n",
      "epoch 217 | step 5 | loss: 0.001790633064574078\n",
      "epoch 217 | step 6 | loss: 0.0020888342392101325\n",
      "epoch 217 | step 7 | loss: 0.0023696757357322034\n",
      "epoch 217 | step 8 | loss: 0.002637444231051716\n",
      "epoch 217 | step 9 | loss: 0.002917857255175681\n",
      "epoch 217 | step 10 | loss: 0.003209812522552755\n",
      "epoch 217 | step 11 | loss: 0.0035258890693726968\n",
      "epoch 218 | step 0 | loss: 0.0003083812789755845\n",
      "epoch 218 | step 1 | loss: 0.000600529419417681\n",
      "epoch 218 | step 2 | loss: 0.0009235880781376004\n",
      "epoch 218 | step 3 | loss: 0.001221926011603601\n",
      "epoch 218 | step 4 | loss: 0.001488887309262638\n",
      "epoch 218 | step 5 | loss: 0.0017825610959952766\n",
      "epoch 218 | step 6 | loss: 0.002072766732187994\n",
      "epoch 218 | step 7 | loss: 0.002344890449752674\n",
      "epoch 218 | step 8 | loss: 0.0026246763779923525\n",
      "epoch 218 | step 9 | loss: 0.0029056873679811613\n",
      "epoch 218 | step 10 | loss: 0.003234544103378266\n",
      "epoch 218 | step 11 | loss: 0.0035161481366323085\n",
      "epoch 219 | step 0 | loss: 0.0003015962916691848\n",
      "epoch 219 | step 1 | loss: 0.0006024730390282225\n",
      "epoch 219 | step 2 | loss: 0.0008934614762844944\n",
      "epoch 219 | step 3 | loss: 0.0011847208259169004\n",
      "epoch 219 | step 4 | loss: 0.0014738072438846535\n",
      "epoch 219 | step 5 | loss: 0.0017377632444570252\n",
      "epoch 219 | step 6 | loss: 0.002019670715744531\n",
      "epoch 219 | step 7 | loss: 0.002320011867302272\n",
      "epoch 219 | step 8 | loss: 0.002612731932153678\n",
      "epoch 219 | step 9 | loss: 0.0029290253366463996\n",
      "epoch 219 | step 10 | loss: 0.0032281153852684137\n",
      "epoch 219 | step 11 | loss: 0.0035187730861552143\n",
      "epoch 220 | step 0 | loss: 0.00030632573239673433\n",
      "epoch 220 | step 1 | loss: 0.0006004117241332129\n",
      "epoch 220 | step 2 | loss: 0.000906070862440157\n",
      "epoch 220 | step 3 | loss: 0.001185646451930037\n",
      "epoch 220 | step 4 | loss: 0.0015191720774520654\n",
      "epoch 220 | step 5 | loss: 0.0018074899875066275\n",
      "epoch 220 | step 6 | loss: 0.0020838579789821204\n",
      "epoch 220 | step 7 | loss: 0.0023829659966501473\n",
      "epoch 220 | step 8 | loss: 0.0026767281402982203\n",
      "epoch 220 | step 9 | loss: 0.0029681412561122955\n",
      "epoch 220 | step 10 | loss: 0.0032274052936080897\n",
      "epoch 220 | step 11 | loss: 0.003518823101238143\n",
      "epoch 221 | step 0 | loss: 0.0002835799710219208\n",
      "epoch 221 | step 1 | loss: 0.0005547113995011026\n",
      "epoch 221 | step 2 | loss: 0.000846774180288522\n",
      "epoch 221 | step 3 | loss: 0.0011414233437191102\n",
      "epoch 221 | step 4 | loss: 0.001464331200405867\n",
      "epoch 221 | step 5 | loss: 0.0017594049784013673\n",
      "epoch 221 | step 6 | loss: 0.0020627610167982457\n",
      "epoch 221 | step 7 | loss: 0.0023479827809990313\n",
      "epoch 221 | step 8 | loss: 0.002639026886755903\n",
      "epoch 221 | step 9 | loss: 0.0029544668245376554\n",
      "epoch 221 | step 10 | loss: 0.00323272321889453\n",
      "epoch 221 | step 11 | loss: 0.003516724147519744\n",
      "epoch 222 | step 0 | loss: 0.0002844914810581015\n",
      "epoch 222 | step 1 | loss: 0.0005833220489968708\n",
      "epoch 222 | step 2 | loss: 0.0008825877588032093\n",
      "epoch 222 | step 3 | loss: 0.001189304525044918\n",
      "epoch 222 | step 4 | loss: 0.0014592889753548082\n",
      "epoch 222 | step 5 | loss: 0.0017321807548594825\n",
      "epoch 222 | step 6 | loss: 0.0020278122944801867\n",
      "epoch 222 | step 7 | loss: 0.002328093488938299\n",
      "epoch 222 | step 8 | loss: 0.002612060748292408\n",
      "epoch 222 | step 9 | loss: 0.0029173945793377776\n",
      "epoch 222 | step 10 | loss: 0.0032109465115610094\n",
      "epoch 222 | step 11 | loss: 0.003525355214743386\n",
      "epoch 223 | step 0 | loss: 0.00027195156871478293\n",
      "epoch 223 | step 1 | loss: 0.0005692525900215884\n",
      "epoch 223 | step 2 | loss: 0.0008524847651396125\n",
      "epoch 223 | step 3 | loss: 0.001160264269804818\n",
      "epoch 223 | step 4 | loss: 0.0014647330306526572\n",
      "epoch 223 | step 5 | loss: 0.00174375607553927\n",
      "epoch 223 | step 6 | loss: 0.002028972635959013\n",
      "epoch 223 | step 7 | loss: 0.002333376390399135\n",
      "epoch 223 | step 8 | loss: 0.0026276529509397712\n",
      "epoch 223 | step 9 | loss: 0.0029116087258264736\n",
      "epoch 223 | step 10 | loss: 0.003209411362466874\n",
      "epoch 223 | step 11 | loss: 0.0035258821818094413\n",
      "epoch 224 | step 0 | loss: 0.0002620366194869462\n",
      "epoch 224 | step 1 | loss: 0.0005497950732093323\n",
      "epoch 224 | step 2 | loss: 0.0008605009922047636\n",
      "epoch 224 | step 3 | loss: 0.0011490085245534818\n",
      "epoch 224 | step 4 | loss: 0.0014523625802580343\n",
      "epoch 224 | step 5 | loss: 0.0017598911283237249\n",
      "epoch 224 | step 6 | loss: 0.0020495986960489525\n",
      "epoch 224 | step 7 | loss: 0.0023559892870438047\n",
      "epoch 224 | step 8 | loss: 0.0026512449240241056\n",
      "epoch 224 | step 9 | loss: 0.0029494232608100286\n",
      "epoch 224 | step 10 | loss: 0.003233846510917603\n",
      "epoch 224 | step 11 | loss: 0.003516381508883061\n",
      "epoch 225 | step 0 | loss: 0.0003022471900734002\n",
      "epoch 225 | step 1 | loss: 0.0006033457212637627\n",
      "epoch 225 | step 2 | loss: 0.0008970837509462839\n",
      "epoch 225 | step 3 | loss: 0.0011813011347551014\n",
      "epoch 225 | step 4 | loss: 0.00147314191092932\n",
      "epoch 225 | step 5 | loss: 0.001768658206313966\n",
      "epoch 225 | step 6 | loss: 0.002050229832224187\n",
      "epoch 225 | step 7 | loss: 0.0023753400999838884\n",
      "epoch 225 | step 8 | loss: 0.0026721759788090782\n",
      "epoch 225 | step 9 | loss: 0.0029529658022749523\n",
      "epoch 225 | step 10 | loss: 0.0032318865167271895\n",
      "epoch 225 | step 11 | loss: 0.0035169314588325805\n",
      "epoch 226 | step 0 | loss: 0.0003158604564439533\n",
      "epoch 226 | step 1 | loss: 0.0005921327911186036\n",
      "epoch 226 | step 2 | loss: 0.0008802017999594826\n",
      "epoch 226 | step 3 | loss: 0.0011722462725137586\n",
      "epoch 226 | step 4 | loss: 0.0014488248370649202\n",
      "epoch 226 | step 5 | loss: 0.0017359026023687136\n",
      "epoch 226 | step 6 | loss: 0.0020429451614830214\n",
      "epoch 226 | step 7 | loss: 0.0023399385749751935\n",
      "epoch 226 | step 8 | loss: 0.0026375374603848306\n",
      "epoch 226 | step 9 | loss: 0.00294528140187051\n",
      "epoch 226 | step 10 | loss: 0.0032299407481406876\n",
      "epoch 226 | step 11 | loss: 0.003517594303421279\n",
      "epoch 227 | step 0 | loss: 0.0003027849131852993\n",
      "epoch 227 | step 1 | loss: 0.0005712277531142092\n",
      "epoch 227 | step 2 | loss: 0.0008853829702645196\n",
      "epoch 227 | step 3 | loss: 0.0011920903140990112\n",
      "epoch 227 | step 4 | loss: 0.00150869384173517\n",
      "epoch 227 | step 5 | loss: 0.0017852858681991842\n",
      "epoch 227 | step 6 | loss: 0.0020742659707915197\n",
      "epoch 227 | step 7 | loss: 0.002363104963622209\n",
      "epoch 227 | step 8 | loss: 0.0026526214380913887\n",
      "epoch 227 | step 9 | loss: 0.0029313426326597483\n",
      "epoch 227 | step 10 | loss: 0.003225591111655709\n",
      "epoch 227 | step 11 | loss: 0.003519265598587879\n",
      "epoch 228 | step 0 | loss: 0.0002996897411815788\n",
      "epoch 228 | step 1 | loss: 0.0005729407067200657\n",
      "epoch 228 | step 2 | loss: 0.0008606508507712416\n",
      "epoch 228 | step 3 | loss: 0.00113620464077912\n",
      "epoch 228 | step 4 | loss: 0.0014534605653089376\n",
      "epoch 228 | step 5 | loss: 0.0017554046250648735\n",
      "epoch 228 | step 6 | loss: 0.002054669193250501\n",
      "epoch 228 | step 7 | loss: 0.002343993689849358\n",
      "epoch 228 | step 8 | loss: 0.0026469726062509323\n",
      "epoch 228 | step 9 | loss: 0.002930007743858699\n",
      "epoch 228 | step 10 | loss: 0.0032206437482013963\n",
      "epoch 228 | step 11 | loss: 0.003521195583503373\n",
      "epoch 229 | step 0 | loss: 0.0002803895638292196\n",
      "epoch 229 | step 1 | loss: 0.0005844119778959479\n",
      "epoch 229 | step 2 | loss: 0.0008725310002238109\n",
      "epoch 229 | step 3 | loss: 0.001152158225296979\n",
      "epoch 229 | step 4 | loss: 0.0014517918204506573\n",
      "epoch 229 | step 5 | loss: 0.001748149709892741\n",
      "epoch 229 | step 6 | loss: 0.002054497580053369\n",
      "epoch 229 | step 7 | loss: 0.002361678714601291\n",
      "epoch 229 | step 8 | loss: 0.002654115508462792\n",
      "epoch 229 | step 9 | loss: 0.002963475648763899\n",
      "epoch 229 | step 10 | loss: 0.0032367487673895983\n",
      "epoch 229 | step 11 | loss: 0.003514919173076195\n",
      "epoch 230 | step 0 | loss: 0.00029743106091173536\n",
      "epoch 230 | step 1 | loss: 0.0005827949536902264\n",
      "epoch 230 | step 2 | loss: 0.000889341422566568\n",
      "epoch 230 | step 3 | loss: 0.0011888761924352428\n",
      "epoch 230 | step 4 | loss: 0.0014899899742049783\n",
      "epoch 230 | step 5 | loss: 0.0017948406610453576\n",
      "epoch 230 | step 6 | loss: 0.002102635263340723\n",
      "epoch 230 | step 7 | loss: 0.0023723697775777917\n",
      "epoch 230 | step 8 | loss: 0.0026612082660125895\n",
      "epoch 230 | step 9 | loss: 0.0029689873706433617\n",
      "epoch 230 | step 10 | loss: 0.0032418131883060947\n",
      "epoch 230 | step 11 | loss: 0.003512801194734307\n",
      "epoch 231 | step 0 | loss: 0.0002815902338972252\n",
      "epoch 231 | step 1 | loss: 0.0005712366892738303\n",
      "epoch 231 | step 2 | loss: 0.0008652546729695527\n",
      "epoch 231 | step 3 | loss: 0.00114829767250977\n",
      "epoch 231 | step 4 | loss: 0.0014269816587848514\n",
      "epoch 231 | step 5 | loss: 0.00174567544749111\n",
      "epoch 231 | step 6 | loss: 0.0020464924744640805\n",
      "epoch 231 | step 7 | loss: 0.0023345747561913303\n",
      "epoch 231 | step 8 | loss: 0.002633859380291338\n",
      "epoch 231 | step 9 | loss: 0.0029205302023788517\n",
      "epoch 231 | step 10 | loss: 0.003230330872747323\n",
      "epoch 231 | step 11 | loss: 0.003517361306067862\n",
      "epoch 232 | step 0 | loss: 0.0003055013899245918\n",
      "epoch 232 | step 1 | loss: 0.0005942520878035469\n",
      "epoch 232 | step 2 | loss: 0.0008674006989626483\n",
      "epoch 232 | step 3 | loss: 0.001161422109610178\n",
      "epoch 232 | step 4 | loss: 0.0014522685597057432\n",
      "epoch 232 | step 5 | loss: 0.0017558944396008434\n",
      "epoch 232 | step 6 | loss: 0.002043949836888605\n",
      "epoch 232 | step 7 | loss: 0.0023624890114553442\n",
      "epoch 232 | step 8 | loss: 0.0026525594758303192\n",
      "epoch 232 | step 9 | loss: 0.0029316413677967896\n",
      "epoch 232 | step 10 | loss: 0.003217450312703885\n",
      "epoch 232 | step 11 | loss: 0.003522238490168624\n",
      "epoch 233 | step 0 | loss: 0.00032883613222582815\n",
      "epoch 233 | step 1 | loss: 0.0006090992329674953\n",
      "epoch 233 | step 2 | loss: 0.0008913088395462252\n",
      "epoch 233 | step 3 | loss: 0.0012071828610612891\n",
      "epoch 233 | step 4 | loss: 0.0014889754190277227\n",
      "epoch 233 | step 5 | loss: 0.0018150499239092122\n",
      "epoch 233 | step 6 | loss: 0.0021072599382341442\n",
      "epoch 233 | step 7 | loss: 0.002391914777848474\n",
      "epoch 233 | step 8 | loss: 0.0026594471368041462\n",
      "epoch 233 | step 9 | loss: 0.002949682148930279\n",
      "epoch 233 | step 10 | loss: 0.003255203087687579\n",
      "epoch 233 | step 11 | loss: 0.0035075362286788015\n",
      "epoch 234 | step 0 | loss: 0.0002827689824191165\n",
      "epoch 234 | step 1 | loss: 0.0005693412995191122\n",
      "epoch 234 | step 2 | loss: 0.000881441934629464\n",
      "epoch 234 | step 3 | loss: 0.0011746725238746894\n",
      "epoch 234 | step 4 | loss: 0.0014784314981230128\n",
      "epoch 234 | step 5 | loss: 0.001794189014572221\n",
      "epoch 234 | step 6 | loss: 0.0020806001467003577\n",
      "epoch 234 | step 7 | loss: 0.002357301927176444\n",
      "epoch 234 | step 8 | loss: 0.002633291629937538\n",
      "epoch 234 | step 9 | loss: 0.002932630146488649\n",
      "epoch 234 | step 10 | loss: 0.003216207465382146\n",
      "epoch 234 | step 11 | loss: 0.0035226725531537473\n",
      "epoch 235 | step 0 | loss: 0.00031641068772551926\n",
      "epoch 235 | step 1 | loss: 0.0006153546667990359\n",
      "epoch 235 | step 2 | loss: 0.0008983663511799286\n",
      "epoch 235 | step 3 | loss: 0.0011966668352101706\n",
      "epoch 235 | step 4 | loss: 0.0014728495575677433\n",
      "epoch 235 | step 5 | loss: 0.0017575438095514422\n",
      "epoch 235 | step 6 | loss: 0.002048707543654058\n",
      "epoch 235 | step 7 | loss: 0.002332238883650712\n",
      "epoch 235 | step 8 | loss: 0.002657948858187718\n",
      "epoch 235 | step 9 | loss: 0.002956399660828358\n",
      "epoch 235 | step 10 | loss: 0.003237621057176684\n",
      "epoch 235 | step 11 | loss: 0.0035142492401321174\n",
      "epoch 236 | step 0 | loss: 0.00029029267044606357\n",
      "epoch 236 | step 1 | loss: 0.0005536574070321579\n",
      "epoch 236 | step 2 | loss: 0.000863215016884143\n",
      "epoch 236 | step 3 | loss: 0.001137237083255123\n",
      "epoch 236 | step 4 | loss: 0.0014510354837667624\n",
      "epoch 236 | step 5 | loss: 0.001782232546148214\n",
      "epoch 236 | step 6 | loss: 0.002057917922732248\n",
      "epoch 236 | step 7 | loss: 0.002376480915828132\n",
      "epoch 236 | step 8 | loss: 0.0026597730409086756\n",
      "epoch 236 | step 9 | loss: 0.002946576465121206\n",
      "epoch 236 | step 10 | loss: 0.0032311937113907495\n",
      "epoch 236 | step 11 | loss: 0.0035167426697894242\n",
      "epoch 237 | step 0 | loss: 0.0003005435917709403\n",
      "epoch 237 | step 1 | loss: 0.0005821963874779157\n",
      "epoch 237 | step 2 | loss: 0.0008595500715327199\n",
      "epoch 237 | step 3 | loss: 0.0011273893066438459\n",
      "epoch 237 | step 4 | loss: 0.0014085059482432647\n",
      "epoch 237 | step 5 | loss: 0.001705520491760008\n",
      "epoch 237 | step 6 | loss: 0.0020051029493236474\n",
      "epoch 237 | step 7 | loss: 0.002344844156646669\n",
      "epoch 237 | step 8 | loss: 0.0026228153123737203\n",
      "epoch 237 | step 9 | loss: 0.00293081362399703\n",
      "epoch 237 | step 10 | loss: 0.0032308356471268105\n",
      "epoch 237 | step 11 | loss: 0.003516887809623227\n",
      "epoch 238 | step 0 | loss: 0.0002924028992606445\n",
      "epoch 238 | step 1 | loss: 0.000561323621818081\n",
      "epoch 238 | step 2 | loss: 0.0008464693038397076\n",
      "epoch 238 | step 3 | loss: 0.0011417592038149103\n",
      "epoch 238 | step 4 | loss: 0.0014247486100858059\n",
      "epoch 238 | step 5 | loss: 0.0017256172160251017\n",
      "epoch 238 | step 6 | loss: 0.0020104004378632455\n",
      "epoch 238 | step 7 | loss: 0.002282881923769118\n",
      "epoch 238 | step 8 | loss: 0.002582524891734839\n",
      "epoch 238 | step 9 | loss: 0.0028905268739268576\n",
      "epoch 238 | step 10 | loss: 0.0032037299566121\n",
      "epoch 238 | step 11 | loss: 0.0035273439950060633\n",
      "epoch 239 | step 0 | loss: 0.0003120143945601877\n",
      "epoch 239 | step 1 | loss: 0.000599287177505164\n",
      "epoch 239 | step 2 | loss: 0.0008793154284602627\n",
      "epoch 239 | step 3 | loss: 0.0011544870428957979\n",
      "epoch 239 | step 4 | loss: 0.001436564562612107\n",
      "epoch 239 | step 5 | loss: 0.0017191643145521886\n",
      "epoch 239 | step 6 | loss: 0.0020211651915030622\n",
      "epoch 239 | step 7 | loss: 0.002307843478846882\n",
      "epoch 239 | step 8 | loss: 0.0026058577294127286\n",
      "epoch 239 | step 9 | loss: 0.0029053568020985686\n",
      "epoch 239 | step 10 | loss: 0.003227544110866139\n",
      "epoch 239 | step 11 | loss: 0.0035179979809950835\n",
      "epoch 240 | step 0 | loss: 0.0002924279947770595\n",
      "epoch 240 | step 1 | loss: 0.0006163799869536328\n",
      "epoch 240 | step 2 | loss: 0.000899665435475977\n",
      "epoch 240 | step 3 | loss: 0.0011829233712356661\n",
      "epoch 240 | step 4 | loss: 0.0014535810018352596\n",
      "epoch 240 | step 5 | loss: 0.0017564136158355718\n",
      "epoch 240 | step 6 | loss: 0.002042045842230501\n",
      "epoch 240 | step 7 | loss: 0.0023302410940404883\n",
      "epoch 240 | step 8 | loss: 0.0026243905417960138\n",
      "epoch 240 | step 9 | loss: 0.002917103557759515\n",
      "epoch 240 | step 10 | loss: 0.0032177871139512765\n",
      "epoch 240 | step 11 | loss: 0.003521825273940469\n",
      "epoch 241 | step 0 | loss: 0.00029870220307816645\n",
      "epoch 241 | step 1 | loss: 0.0005927263283326508\n",
      "epoch 241 | step 2 | loss: 0.0008955907380468371\n",
      "epoch 241 | step 3 | loss: 0.0012117358990597652\n",
      "epoch 241 | step 4 | loss: 0.0014835601087970652\n",
      "epoch 241 | step 5 | loss: 0.0017872545080035301\n",
      "epoch 241 | step 6 | loss: 0.0020644673041667243\n",
      "epoch 241 | step 7 | loss: 0.0023375018793469504\n",
      "epoch 241 | step 8 | loss: 0.002623515603710097\n",
      "epoch 241 | step 9 | loss: 0.002931609282933787\n",
      "epoch 241 | step 10 | loss: 0.0032156091777926603\n",
      "epoch 241 | step 11 | loss: 0.0035226575638655708\n",
      "epoch 242 | step 0 | loss: 0.0002898468775610596\n",
      "epoch 242 | step 1 | loss: 0.0005836188049109176\n",
      "epoch 242 | step 2 | loss: 0.0008831763619249373\n",
      "epoch 242 | step 3 | loss: 0.0011631961383252742\n",
      "epoch 242 | step 4 | loss: 0.0014471130840426724\n",
      "epoch 242 | step 5 | loss: 0.0017357189793534122\n",
      "epoch 242 | step 6 | loss: 0.0020777180209879723\n",
      "epoch 242 | step 7 | loss: 0.0023916195105168907\n",
      "epoch 242 | step 8 | loss: 0.0026354453929954467\n",
      "epoch 242 | step 9 | loss: 0.00292731134238694\n",
      "epoch 242 | step 10 | loss: 0.003222652543372387\n",
      "epoch 242 | step 11 | loss: 0.003519933715280058\n",
      "epoch 243 | step 0 | loss: 0.0002961200684955665\n",
      "epoch 243 | step 1 | loss: 0.0005643405626894055\n",
      "epoch 243 | step 2 | loss: 0.0008337186450878581\n",
      "epoch 243 | step 3 | loss: 0.0011459197882198735\n",
      "epoch 243 | step 4 | loss: 0.0014291185817238328\n",
      "epoch 243 | step 5 | loss: 0.0017264934874010966\n",
      "epoch 243 | step 6 | loss: 0.0020227448719549197\n",
      "epoch 243 | step 7 | loss: 0.00231862911273668\n",
      "epoch 243 | step 8 | loss: 0.002615771911111469\n",
      "epoch 243 | step 9 | loss: 0.0029184935612379796\n",
      "epoch 243 | step 10 | loss: 0.0032207507424289587\n",
      "epoch 243 | step 11 | loss: 0.003520510173704427\n",
      "epoch 244 | step 0 | loss: 0.0003029909447921211\n",
      "epoch 244 | step 1 | loss: 0.0006032301315987611\n",
      "epoch 244 | step 2 | loss: 0.0008692357899327116\n",
      "epoch 244 | step 3 | loss: 0.0011744806289427443\n",
      "epoch 244 | step 4 | loss: 0.0014785858505692358\n",
      "epoch 244 | step 5 | loss: 0.0017937075863264924\n",
      "epoch 244 | step 6 | loss: 0.0020708663095605805\n",
      "epoch 244 | step 7 | loss: 0.002363389975948314\n",
      "epoch 244 | step 8 | loss: 0.00267091412138112\n",
      "epoch 244 | step 9 | loss: 0.002957118071994546\n",
      "epoch 244 | step 10 | loss: 0.0032482068458918037\n",
      "epoch 244 | step 11 | loss: 0.0035097504806312248\n",
      "epoch 245 | step 0 | loss: 0.0003073239354260743\n",
      "epoch 245 | step 1 | loss: 0.0005906520246965849\n",
      "epoch 245 | step 2 | loss: 0.0008831098957901441\n",
      "epoch 245 | step 3 | loss: 0.0011684721220460424\n",
      "epoch 245 | step 4 | loss: 0.0014500597727336076\n",
      "epoch 245 | step 5 | loss: 0.0017251998013966363\n",
      "epoch 245 | step 6 | loss: 0.002036355111143304\n",
      "epoch 245 | step 7 | loss: 0.002334348168269456\n",
      "epoch 245 | step 8 | loss: 0.002632256919910153\n",
      "epoch 245 | step 9 | loss: 0.0029243346827511714\n",
      "epoch 245 | step 10 | loss: 0.0032342097198423863\n",
      "epoch 245 | step 11 | loss: 0.003515160924586724\n",
      "epoch 246 | step 0 | loss: 0.00029615256691528495\n",
      "epoch 246 | step 1 | loss: 0.0005970687190208329\n",
      "epoch 246 | step 2 | loss: 0.0009084522340952839\n",
      "epoch 246 | step 3 | loss: 0.001220216516958595\n",
      "epoch 246 | step 4 | loss: 0.0015213672250755817\n",
      "epoch 246 | step 5 | loss: 0.0017875390342167933\n",
      "epoch 246 | step 6 | loss: 0.002062801592068623\n",
      "epoch 246 | step 7 | loss: 0.0023511457244615565\n",
      "epoch 246 | step 8 | loss: 0.0026262759281002472\n",
      "epoch 246 | step 9 | loss: 0.0029454263240037476\n",
      "epoch 246 | step 10 | loss: 0.0032244788383470223\n",
      "epoch 246 | step 11 | loss: 0.0035189126610005762\n",
      "epoch 247 | step 0 | loss: 0.0002978157729731126\n",
      "epoch 247 | step 1 | loss: 0.0006091144439260265\n",
      "epoch 247 | step 2 | loss: 0.0009188477462309441\n",
      "epoch 247 | step 3 | loss: 0.0011887446657906314\n",
      "epoch 247 | step 4 | loss: 0.0014939530447662876\n",
      "epoch 247 | step 5 | loss: 0.0017943836776799515\n",
      "epoch 247 | step 6 | loss: 0.0020839430533048623\n",
      "epoch 247 | step 7 | loss: 0.0023745982416909905\n",
      "epoch 247 | step 8 | loss: 0.0026510952870800316\n",
      "epoch 247 | step 9 | loss: 0.0029426021226794537\n",
      "epoch 247 | step 10 | loss: 0.0032364415309571832\n",
      "epoch 247 | step 11 | loss: 0.003514141881512673\n",
      "epoch 248 | step 0 | loss: 0.0002835460567109689\n",
      "epoch 248 | step 1 | loss: 0.0005705276835029314\n",
      "epoch 248 | step 2 | loss: 0.0008676058601593088\n",
      "epoch 248 | step 3 | loss: 0.0011432499694774927\n",
      "epoch 248 | step 4 | loss: 0.0014716700979565717\n",
      "epoch 248 | step 5 | loss: 0.0017639882786275242\n",
      "epoch 248 | step 6 | loss: 0.0020503323463136273\n",
      "epoch 248 | step 7 | loss: 0.002313647516957151\n",
      "epoch 248 | step 8 | loss: 0.002590170880079943\n",
      "epoch 248 | step 9 | loss: 0.00289902770637407\n",
      "epoch 248 | step 10 | loss: 0.0031870748006355767\n",
      "epoch 248 | step 11 | loss: 0.003533432152980049\n",
      "epoch 249 | step 0 | loss: 0.0002922095402712479\n",
      "epoch 249 | step 1 | loss: 0.000571461710255936\n",
      "epoch 249 | step 2 | loss: 0.0008587918786131362\n",
      "epoch 249 | step 3 | loss: 0.0011798250249300992\n",
      "epoch 249 | step 4 | loss: 0.0014813831468762754\n",
      "epoch 249 | step 5 | loss: 0.0017766781797242716\n",
      "epoch 249 | step 6 | loss: 0.002061270863008665\n",
      "epoch 249 | step 7 | loss: 0.002370297787386832\n",
      "epoch 249 | step 8 | loss: 0.0026683084902813484\n",
      "epoch 249 | step 9 | loss: 0.0029523109048415707\n",
      "epoch 249 | step 10 | loss: 0.0032323744336658564\n",
      "epoch 249 | step 11 | loss: 0.0035157496378942048\n",
      "epoch 250 | step 0 | loss: 0.0003181765197758478\n",
      "epoch 250 | step 1 | loss: 0.0006133043971308284\n",
      "epoch 250 | step 2 | loss: 0.0008966071727995659\n",
      "epoch 250 | step 3 | loss: 0.001177144843291106\n",
      "epoch 250 | step 4 | loss: 0.001491759504532323\n",
      "epoch 250 | step 5 | loss: 0.0017959553295457359\n",
      "epoch 250 | step 6 | loss: 0.00207195567184247\n",
      "epoch 250 | step 7 | loss: 0.0023744634631777855\n",
      "epoch 250 | step 8 | loss: 0.0026652368397330197\n",
      "epoch 250 | step 9 | loss: 0.0029461550265755683\n",
      "epoch 250 | step 10 | loss: 0.0032266647103726407\n",
      "epoch 250 | step 11 | loss: 0.003517871583273216\n",
      "epoch 251 | step 0 | loss: 0.00031727735049898946\n",
      "epoch 251 | step 1 | loss: 0.0006229278871154568\n",
      "epoch 251 | step 2 | loss: 0.0009168191561275109\n",
      "epoch 251 | step 3 | loss: 0.001193277297480057\n",
      "epoch 251 | step 4 | loss: 0.0014817090207110374\n",
      "epoch 251 | step 5 | loss: 0.0017695994917878133\n",
      "epoch 251 | step 6 | loss: 0.00206265365694529\n",
      "epoch 251 | step 7 | loss: 0.0023536783307287847\n",
      "epoch 251 | step 8 | loss: 0.0026619757955360203\n",
      "epoch 251 | step 9 | loss: 0.0029377286285503035\n",
      "epoch 251 | step 10 | loss: 0.003227269241253705\n",
      "epoch 251 | step 11 | loss: 0.0035177474415687623\n",
      "epoch 252 | step 0 | loss: 0.00028841577213975945\n",
      "epoch 252 | step 1 | loss: 0.0005971454445863539\n",
      "epoch 252 | step 2 | loss: 0.0008714018859972847\n",
      "epoch 252 | step 3 | loss: 0.0011454535846767923\n",
      "epoch 252 | step 4 | loss: 0.0014320618522798453\n",
      "epoch 252 | step 5 | loss: 0.0017497540319847966\n",
      "epoch 252 | step 6 | loss: 0.00204218906244603\n",
      "epoch 252 | step 7 | loss: 0.0023059807196912094\n",
      "epoch 252 | step 8 | loss: 0.0026312298411853468\n",
      "epoch 252 | step 9 | loss: 0.0029244378217863614\n",
      "epoch 252 | step 10 | loss: 0.003220934240269574\n",
      "epoch 252 | step 11 | loss: 0.003520070754393151\n",
      "epoch 253 | step 0 | loss: 0.00029876903523905223\n",
      "epoch 253 | step 1 | loss: 0.000613824230979751\n",
      "epoch 253 | step 2 | loss: 0.0008977578463464395\n",
      "epoch 253 | step 3 | loss: 0.0012090296500225223\n",
      "epoch 253 | step 4 | loss: 0.0014927342775730278\n",
      "epoch 253 | step 5 | loss: 0.0017713570876992132\n",
      "epoch 253 | step 6 | loss: 0.0020533950519192276\n",
      "epoch 253 | step 7 | loss: 0.0023388936797385153\n",
      "epoch 253 | step 8 | loss: 0.002645295344524972\n",
      "epoch 253 | step 9 | loss: 0.0029386413350282635\n",
      "epoch 253 | step 10 | loss: 0.003234306913269533\n",
      "epoch 253 | step 11 | loss: 0.003514866937284348\n",
      "epoch 254 | step 0 | loss: 0.0003089634847565365\n",
      "epoch 254 | step 1 | loss: 0.0005988105135771617\n",
      "epoch 254 | step 2 | loss: 0.0008880255018013483\n",
      "epoch 254 | step 3 | loss: 0.0011787173212313437\n",
      "epoch 254 | step 4 | loss: 0.0014610656277525493\n",
      "epoch 254 | step 5 | loss: 0.0017371316412158686\n",
      "epoch 254 | step 6 | loss: 0.0020327368911116513\n",
      "epoch 254 | step 7 | loss: 0.002329277246366899\n",
      "epoch 254 | step 8 | loss: 0.0026356124188974283\n",
      "epoch 254 | step 9 | loss: 0.0029354975241722508\n",
      "epoch 254 | step 10 | loss: 0.003213028897514978\n",
      "epoch 254 | step 11 | loss: 0.003523104107575612\n",
      "epoch 255 | step 0 | loss: 0.00029137889190446404\n",
      "epoch 255 | step 1 | loss: 0.0005913490145328874\n",
      "epoch 255 | step 2 | loss: 0.0009039754720573031\n",
      "epoch 255 | step 3 | loss: 0.001208037319861388\n",
      "epoch 255 | step 4 | loss: 0.001507344861464643\n",
      "epoch 255 | step 5 | loss: 0.0018011706098674592\n",
      "epoch 255 | step 6 | loss: 0.002092596248991722\n",
      "epoch 255 | step 7 | loss: 0.0023888500700703415\n",
      "epoch 255 | step 8 | loss: 0.0026645790759394715\n",
      "epoch 255 | step 9 | loss: 0.0029354957811080174\n",
      "epoch 255 | step 10 | loss: 0.003228388359507002\n",
      "epoch 255 | step 11 | loss: 0.003516988551950751\n",
      "epoch 256 | step 0 | loss: 0.0002990814941887088\n",
      "epoch 256 | step 1 | loss: 0.0005680405463584224\n",
      "epoch 256 | step 2 | loss: 0.0008795028035981171\n",
      "epoch 256 | step 3 | loss: 0.0012008990631312128\n",
      "epoch 256 | step 4 | loss: 0.001503190078096527\n",
      "epoch 256 | step 5 | loss: 0.0018096654121316565\n",
      "epoch 256 | step 6 | loss: 0.0021096794005523957\n",
      "epoch 256 | step 7 | loss: 0.0023953222786482395\n",
      "epoch 256 | step 8 | loss: 0.0026635795910458938\n",
      "epoch 256 | step 9 | loss: 0.0029591482152707313\n",
      "epoch 256 | step 10 | loss: 0.0032203385286986796\n",
      "epoch 256 | step 11 | loss: 0.0035200817955359275\n",
      "epoch 257 | step 0 | loss: 0.0002871892703958802\n",
      "epoch 257 | step 1 | loss: 0.0005843422779135308\n",
      "epoch 257 | step 2 | loss: 0.000868417246367504\n",
      "epoch 257 | step 3 | loss: 0.0011554423108621995\n",
      "epoch 257 | step 4 | loss: 0.0014528038481109496\n",
      "epoch 257 | step 5 | loss: 0.001753530029808828\n",
      "epoch 257 | step 6 | loss: 0.002056606633627908\n",
      "epoch 257 | step 7 | loss: 0.0023369003670207214\n",
      "epoch 257 | step 8 | loss: 0.0026373288735479805\n",
      "epoch 257 | step 9 | loss: 0.00291762107186558\n",
      "epoch 257 | step 10 | loss: 0.003220879502942377\n",
      "epoch 257 | step 11 | loss: 0.003519850197201983\n",
      "epoch 258 | step 0 | loss: 0.00030599808225491434\n",
      "epoch 258 | step 1 | loss: 0.0006009125721158883\n",
      "epoch 258 | step 2 | loss: 0.0008835178802271398\n",
      "epoch 258 | step 3 | loss: 0.0011722053581666354\n",
      "epoch 258 | step 4 | loss: 0.0014346214328520702\n",
      "epoch 258 | step 5 | loss: 0.0017623078637683295\n",
      "epoch 258 | step 6 | loss: 0.002054305562613517\n",
      "epoch 258 | step 7 | loss: 0.002329649650147145\n",
      "epoch 258 | step 8 | loss: 0.0026591820579886616\n",
      "epoch 258 | step 9 | loss: 0.0029469642591950325\n",
      "epoch 258 | step 10 | loss: 0.0032306713236495326\n",
      "epoch 258 | step 11 | loss: 0.0035160430012845475\n",
      "epoch 259 | step 0 | loss: 0.0003281660863265056\n",
      "epoch 259 | step 1 | loss: 0.0006371127607512295\n",
      "epoch 259 | step 2 | loss: 0.0009143568433762636\n",
      "epoch 259 | step 3 | loss: 0.0012067618822388588\n",
      "epoch 259 | step 4 | loss: 0.0014812992939922719\n",
      "epoch 259 | step 5 | loss: 0.0017495034368596983\n",
      "epoch 259 | step 6 | loss: 0.002051200299216314\n",
      "epoch 259 | step 7 | loss: 0.0023497701273720796\n",
      "epoch 259 | step 8 | loss: 0.002674525232301785\n",
      "epoch 259 | step 9 | loss: 0.00296049645720508\n",
      "epoch 259 | step 10 | loss: 0.0032481051700305927\n",
      "epoch 259 | step 11 | loss: 0.0035094083947931594\n",
      "epoch 260 | step 0 | loss: 0.0002762875083153207\n",
      "epoch 260 | step 1 | loss: 0.0005591210388178062\n",
      "epoch 260 | step 2 | loss: 0.0008461186193073251\n",
      "epoch 260 | step 3 | loss: 0.0011434341402422829\n",
      "epoch 260 | step 4 | loss: 0.0014334234534541714\n",
      "epoch 260 | step 5 | loss: 0.0017225513281868485\n",
      "epoch 260 | step 6 | loss: 0.0019989213464326537\n",
      "epoch 260 | step 7 | loss: 0.0023359759994946226\n",
      "epoch 260 | step 8 | loss: 0.0026335404745046424\n",
      "epoch 260 | step 9 | loss: 0.0029219811007405437\n",
      "epoch 260 | step 10 | loss: 0.0032299129887099335\n",
      "epoch 260 | step 11 | loss: 0.003516138776678461\n",
      "epoch 261 | step 0 | loss: 0.0003064390891151554\n",
      "epoch 261 | step 1 | loss: 0.0005849679925852793\n",
      "epoch 261 | step 2 | loss: 0.0009056547415705496\n",
      "epoch 261 | step 3 | loss: 0.0011908153680390558\n",
      "epoch 261 | step 4 | loss: 0.001457550302817789\n",
      "epoch 261 | step 5 | loss: 0.001750862557421393\n",
      "epoch 261 | step 6 | loss: 0.0020794901076192987\n",
      "epoch 261 | step 7 | loss: 0.0023791233819842337\n",
      "epoch 261 | step 8 | loss: 0.002656325960130037\n",
      "epoch 261 | step 9 | loss: 0.0029391478189185185\n",
      "epoch 261 | step 10 | loss: 0.0032307854638103815\n",
      "epoch 261 | step 11 | loss: 0.0035158333429829635\n",
      "epoch 262 | step 0 | loss: 0.000285596847473278\n",
      "epoch 262 | step 1 | loss: 0.0005867627668861302\n",
      "epoch 262 | step 2 | loss: 0.0008808297335654084\n",
      "epoch 262 | step 3 | loss: 0.0011496906789187663\n",
      "epoch 262 | step 4 | loss: 0.0014295680751854939\n",
      "epoch 262 | step 5 | loss: 0.0017115565980003744\n",
      "epoch 262 | step 6 | loss: 0.0020016011084731397\n",
      "epoch 262 | step 7 | loss: 0.0022846103990710387\n",
      "epoch 262 | step 8 | loss: 0.0026267513870869156\n",
      "epoch 262 | step 9 | loss: 0.0029264553946919867\n",
      "epoch 262 | step 10 | loss: 0.0032446041796961025\n",
      "epoch 262 | step 11 | loss: 0.003510329825708278\n",
      "epoch 263 | step 0 | loss: 0.0002737802326728228\n",
      "epoch 263 | step 1 | loss: 0.0005731395632063731\n",
      "epoch 263 | step 2 | loss: 0.0008813890252102226\n",
      "epoch 263 | step 3 | loss: 0.0011542577686997821\n",
      "epoch 263 | step 4 | loss: 0.0014389786543984651\n",
      "epoch 263 | step 5 | loss: 0.0017050129099154799\n",
      "epoch 263 | step 6 | loss: 0.001999836091511583\n",
      "epoch 263 | step 7 | loss: 0.0022969859744169053\n",
      "epoch 263 | step 8 | loss: 0.00260940652756956\n",
      "epoch 263 | step 9 | loss: 0.0029013789284822048\n",
      "epoch 263 | step 10 | loss: 0.003201297315609373\n",
      "epoch 263 | step 11 | loss: 0.0035272898815312077\n",
      "epoch 264 | step 0 | loss: 0.00028538646613789895\n",
      "epoch 264 | step 1 | loss: 0.000564658620817164\n",
      "epoch 264 | step 2 | loss: 0.0008623994691329126\n",
      "epoch 264 | step 3 | loss: 0.0011463566476981799\n",
      "epoch 264 | step 4 | loss: 0.0014365460275814878\n",
      "epoch 264 | step 5 | loss: 0.0017142893469021079\n",
      "epoch 264 | step 6 | loss: 0.0020088928733586757\n",
      "epoch 264 | step 7 | loss: 0.00230571886697822\n",
      "epoch 264 | step 8 | loss: 0.0026226417561377646\n",
      "epoch 264 | step 9 | loss: 0.002910665314814014\n",
      "epoch 264 | step 10 | loss: 0.0032259452823508042\n",
      "epoch 264 | step 11 | loss: 0.0035176380924120077\n",
      "epoch 265 | step 0 | loss: 0.0003056452794649398\n",
      "epoch 265 | step 1 | loss: 0.0006173996646759355\n",
      "epoch 265 | step 2 | loss: 0.0009036163135186352\n",
      "epoch 265 | step 3 | loss: 0.001193221607587482\n",
      "epoch 265 | step 4 | loss: 0.0015036983370426429\n",
      "epoch 265 | step 5 | loss: 0.0017729269900408722\n",
      "epoch 265 | step 6 | loss: 0.002060572153338145\n",
      "epoch 265 | step 7 | loss: 0.0023379636551141745\n",
      "epoch 265 | step 8 | loss: 0.0026275659884442996\n",
      "epoch 265 | step 9 | loss: 0.002919921960206346\n",
      "epoch 265 | step 10 | loss: 0.0032051267700655634\n",
      "epoch 265 | step 11 | loss: 0.0035257525459873778\n",
      "epoch 266 | step 0 | loss: 0.0002712909625110888\n",
      "epoch 266 | step 1 | loss: 0.0006034396182916658\n",
      "epoch 266 | step 2 | loss: 0.000891096674993024\n",
      "epoch 266 | step 3 | loss: 0.0011823099239567222\n",
      "epoch 266 | step 4 | loss: 0.0014954887249242277\n",
      "epoch 266 | step 5 | loss: 0.0017536899816893957\n",
      "epoch 266 | step 6 | loss: 0.0020488460822895155\n",
      "epoch 266 | step 7 | loss: 0.0023563424597598777\n",
      "epoch 266 | step 8 | loss: 0.002654002887220579\n",
      "epoch 266 | step 9 | loss: 0.0029544682482934787\n",
      "epoch 266 | step 10 | loss: 0.0032274696558749817\n",
      "epoch 266 | step 11 | loss: 0.003516896389621331\n",
      "epoch 267 | step 0 | loss: 0.0002882220403578279\n",
      "epoch 267 | step 1 | loss: 0.0005845815091881934\n",
      "epoch 267 | step 2 | loss: 0.0008759803217093475\n",
      "epoch 267 | step 3 | loss: 0.0011391610344624573\n",
      "epoch 267 | step 4 | loss: 0.0014185893253172161\n",
      "epoch 267 | step 5 | loss: 0.001722133430047795\n",
      "epoch 267 | step 6 | loss: 0.002012769022278132\n",
      "epoch 267 | step 7 | loss: 0.0023044647181664703\n",
      "epoch 267 | step 8 | loss: 0.0026205120928191\n",
      "epoch 267 | step 9 | loss: 0.0029340727546613445\n",
      "epoch 267 | step 10 | loss: 0.0032205765660817736\n",
      "epoch 267 | step 11 | loss: 0.003519470007242168\n",
      "epoch 268 | step 0 | loss: 0.000301082642265927\n",
      "epoch 268 | step 1 | loss: 0.0005730271357195224\n",
      "epoch 268 | step 2 | loss: 0.0008585026110673282\n",
      "epoch 268 | step 3 | loss: 0.0011513212350327327\n",
      "epoch 268 | step 4 | loss: 0.0014538747190837344\n",
      "epoch 268 | step 5 | loss: 0.0017709596259688869\n",
      "epoch 268 | step 6 | loss: 0.002058041285131318\n",
      "epoch 268 | step 7 | loss: 0.002358208490654055\n",
      "epoch 268 | step 8 | loss: 0.0026614214061805065\n",
      "epoch 268 | step 9 | loss: 0.002934687383349199\n",
      "epoch 268 | step 10 | loss: 0.003228146178868041\n",
      "epoch 268 | step 11 | loss: 0.0035165430896118934\n",
      "epoch 269 | step 0 | loss: 0.0003071345949944495\n",
      "epoch 269 | step 1 | loss: 0.0006049714498066562\n",
      "epoch 269 | step 2 | loss: 0.0008911079232533381\n",
      "epoch 269 | step 3 | loss: 0.00118501508499\n",
      "epoch 269 | step 4 | loss: 0.001479747764778861\n",
      "epoch 269 | step 5 | loss: 0.0017584134147371165\n",
      "epoch 269 | step 6 | loss: 0.0020315333012319334\n",
      "epoch 269 | step 7 | loss: 0.002326589121589287\n",
      "epoch 269 | step 8 | loss: 0.0026148108620043116\n",
      "epoch 269 | step 9 | loss: 0.002912731911853218\n",
      "epoch 269 | step 10 | loss: 0.0032037337660299537\n",
      "epoch 269 | step 11 | loss: 0.0035259881425705733\n",
      "epoch 270 | step 0 | loss: 0.0002975265637270092\n",
      "epoch 270 | step 1 | loss: 0.0005903540819025045\n",
      "epoch 270 | step 2 | loss: 0.0008440135910671007\n",
      "epoch 270 | step 3 | loss: 0.0011542319826659414\n",
      "epoch 270 | step 4 | loss: 0.0014745602328270374\n",
      "epoch 270 | step 5 | loss: 0.0017596378597621956\n",
      "epoch 270 | step 6 | loss: 0.0020568836102878595\n",
      "epoch 270 | step 7 | loss: 0.0023379914450570457\n",
      "epoch 270 | step 8 | loss: 0.0026265368672288513\n",
      "epoch 270 | step 9 | loss: 0.002927240752609825\n",
      "epoch 270 | step 10 | loss: 0.0032227004784534568\n",
      "epoch 270 | step 11 | loss: 0.003518516186293899\n",
      "epoch 271 | step 0 | loss: 0.0003030744476145587\n",
      "epoch 271 | step 1 | loss: 0.000578261697974093\n",
      "epoch 271 | step 2 | loss: 0.0008678023276255262\n",
      "epoch 271 | step 3 | loss: 0.0011699210474037658\n",
      "epoch 271 | step 4 | loss: 0.0014487791775006187\n",
      "epoch 271 | step 5 | loss: 0.0017572597798923503\n",
      "epoch 271 | step 6 | loss: 0.002044746181236\n",
      "epoch 271 | step 7 | loss: 0.0023461753841746927\n",
      "epoch 271 | step 8 | loss: 0.0026491443467362196\n",
      "epoch 271 | step 9 | loss: 0.0029557134980222758\n",
      "epoch 271 | step 10 | loss: 0.003231252935171367\n",
      "epoch 271 | step 11 | loss: 0.0035152446131697774\n",
      "epoch 272 | step 0 | loss: 0.00030560305831937515\n",
      "epoch 272 | step 1 | loss: 0.0005631378896434736\n",
      "epoch 272 | step 2 | loss: 0.0008470204515718496\n",
      "epoch 272 | step 3 | loss: 0.001154905646356548\n",
      "epoch 272 | step 4 | loss: 0.0014958733155064844\n",
      "epoch 272 | step 5 | loss: 0.0017841749617018461\n",
      "epoch 272 | step 6 | loss: 0.0020855144974130106\n",
      "epoch 272 | step 7 | loss: 0.002371234163987453\n",
      "epoch 272 | step 8 | loss: 0.0026671633682683136\n",
      "epoch 272 | step 9 | loss: 0.0029477848055169463\n",
      "epoch 272 | step 10 | loss: 0.003249574028453489\n",
      "epoch 272 | step 11 | loss: 0.0035079172362999502\n",
      "epoch 273 | step 0 | loss: 0.00027891443292766356\n",
      "epoch 273 | step 1 | loss: 0.000562503551012114\n",
      "epoch 273 | step 2 | loss: 0.000838335715979083\n",
      "epoch 273 | step 3 | loss: 0.0011277870287104412\n",
      "epoch 273 | step 4 | loss: 0.0013932240734210511\n",
      "epoch 273 | step 5 | loss: 0.0016907411213810074\n",
      "epoch 273 | step 6 | loss: 0.002002768074234703\n",
      "epoch 273 | step 7 | loss: 0.002296211082948429\n",
      "epoch 273 | step 8 | loss: 0.0026016750720346325\n",
      "epoch 273 | step 9 | loss: 0.0029098088411115496\n",
      "epoch 273 | step 10 | loss: 0.0032236071380666357\n",
      "epoch 273 | step 11 | loss: 0.003518152834031475\n",
      "epoch 274 | step 0 | loss: 0.0002654614730944048\n",
      "epoch 274 | step 1 | loss: 0.0005628363601689452\n",
      "epoch 274 | step 2 | loss: 0.000841933337777038\n",
      "epoch 274 | step 3 | loss: 0.0011724603376714693\n",
      "epoch 274 | step 4 | loss: 0.0014639241239873687\n",
      "epoch 274 | step 5 | loss: 0.0017658111920531125\n",
      "epoch 274 | step 6 | loss: 0.0020785847794828104\n",
      "epoch 274 | step 7 | loss: 0.002354263383189096\n",
      "epoch 274 | step 8 | loss: 0.0026365809338028384\n",
      "epoch 274 | step 9 | loss: 0.0029334875901352604\n",
      "epoch 274 | step 10 | loss: 0.003236345289306438\n",
      "epoch 274 | step 11 | loss: 0.0035129716612653517\n",
      "epoch 275 | step 0 | loss: 0.00032806480195627204\n",
      "epoch 275 | step 1 | loss: 0.0006419531434075996\n",
      "epoch 275 | step 2 | loss: 0.0009251373188574046\n",
      "epoch 275 | step 3 | loss: 0.00121170013726569\n",
      "epoch 275 | step 4 | loss: 0.0015027410816170585\n",
      "epoch 275 | step 5 | loss: 0.0017862419103831712\n",
      "epoch 275 | step 6 | loss: 0.0020677936682633354\n",
      "epoch 275 | step 7 | loss: 0.0023708584824973667\n",
      "epoch 275 | step 8 | loss: 0.002654368926872015\n",
      "epoch 275 | step 9 | loss: 0.0029341158813963137\n",
      "epoch 275 | step 10 | loss: 0.00322866997267071\n",
      "epoch 275 | step 11 | loss: 0.0035160381751013227\n",
      "epoch 276 | step 0 | loss: 0.0002902864591093702\n",
      "epoch 276 | step 1 | loss: 0.0006015446758600697\n",
      "epoch 276 | step 2 | loss: 0.0008778615127075189\n",
      "epoch 276 | step 3 | loss: 0.0012026000798227633\n",
      "epoch 276 | step 4 | loss: 0.0014960720434385763\n",
      "epoch 276 | step 5 | loss: 0.0017628756366147454\n",
      "epoch 276 | step 6 | loss: 0.0020265739867593594\n",
      "epoch 276 | step 7 | loss: 0.0023225069912944348\n",
      "epoch 276 | step 8 | loss: 0.0026073957977008275\n",
      "epoch 276 | step 9 | loss: 0.0029210188913987322\n",
      "epoch 276 | step 10 | loss: 0.0032145355027303436\n",
      "epoch 276 | step 11 | loss: 0.003521473909302191\n",
      "epoch 277 | step 0 | loss: 0.0002715033298702176\n",
      "epoch 277 | step 1 | loss: 0.0005512395795649192\n",
      "epoch 277 | step 2 | loss: 0.0008793019842649706\n",
      "epoch 277 | step 3 | loss: 0.0011763950798558197\n",
      "epoch 277 | step 4 | loss: 0.001458899114552705\n",
      "epoch 277 | step 5 | loss: 0.0017573667620527581\n",
      "epoch 277 | step 6 | loss: 0.0020799842718068153\n",
      "epoch 277 | step 7 | loss: 0.002363694295721523\n",
      "epoch 277 | step 8 | loss: 0.0026189775975804723\n",
      "epoch 277 | step 9 | loss: 0.0028912390205822207\n",
      "epoch 277 | step 10 | loss: 0.003200951299405848\n",
      "epoch 277 | step 11 | loss: 0.0035267645390473826\n",
      "epoch 278 | step 0 | loss: 0.00029269478183170867\n",
      "epoch 278 | step 1 | loss: 0.0005800536258870374\n",
      "epoch 278 | step 2 | loss: 0.0008682917140810509\n",
      "epoch 278 | step 3 | loss: 0.0011767960416828583\n",
      "epoch 278 | step 4 | loss: 0.0014694949198748592\n",
      "epoch 278 | step 5 | loss: 0.0017655168299573767\n",
      "epoch 278 | step 6 | loss: 0.002075719426303329\n",
      "epoch 278 | step 7 | loss: 0.0023728343596699022\n",
      "epoch 278 | step 8 | loss: 0.0026456794250651955\n",
      "epoch 278 | step 9 | loss: 0.002925628872218081\n",
      "epoch 278 | step 10 | loss: 0.0032343926505873947\n",
      "epoch 278 | step 11 | loss: 0.003513575693239341\n",
      "epoch 279 | step 0 | loss: 0.00031757161615981414\n",
      "epoch 279 | step 1 | loss: 0.0005976555459769989\n",
      "epoch 279 | step 2 | loss: 0.0008795940705429745\n",
      "epoch 279 | step 3 | loss: 0.0011741943443779998\n",
      "epoch 279 | step 4 | loss: 0.0014849464908845513\n",
      "epoch 279 | step 5 | loss: 0.0017717729200673418\n",
      "epoch 279 | step 6 | loss: 0.0020717423650394302\n",
      "epoch 279 | step 7 | loss: 0.002388546274826179\n",
      "epoch 279 | step 8 | loss: 0.002696802260585944\n",
      "epoch 279 | step 9 | loss: 0.0029830213076499183\n",
      "epoch 279 | step 10 | loss: 0.003237581611510696\n",
      "epoch 279 | step 11 | loss: 0.0035123513620182725\n",
      "epoch 280 | step 0 | loss: 0.0002991101511940334\n",
      "epoch 280 | step 1 | loss: 0.0005992357073123465\n",
      "epoch 280 | step 2 | loss: 0.000870384073686585\n",
      "epoch 280 | step 3 | loss: 0.0011361625540335938\n",
      "epoch 280 | step 4 | loss: 0.0014305864705624518\n",
      "epoch 280 | step 5 | loss: 0.001739756696578792\n",
      "epoch 280 | step 6 | loss: 0.002036954870902304\n",
      "epoch 280 | step 7 | loss: 0.0023237787758259134\n",
      "epoch 280 | step 8 | loss: 0.0026018886155135712\n",
      "epoch 280 | step 9 | loss: 0.002893605429056165\n",
      "epoch 280 | step 10 | loss: 0.0031927386310705032\n",
      "epoch 280 | step 11 | loss: 0.003529734853137642\n",
      "epoch 281 | step 0 | loss: 0.0003093527232623271\n",
      "epoch 281 | step 1 | loss: 0.0005849195151404753\n",
      "epoch 281 | step 2 | loss: 0.0008884291253976589\n",
      "epoch 281 | step 3 | loss: 0.0011825050300020156\n",
      "epoch 281 | step 4 | loss: 0.0014680243203166986\n",
      "epoch 281 | step 5 | loss: 0.0017636802018978087\n",
      "epoch 281 | step 6 | loss: 0.0020586131918991233\n",
      "epoch 281 | step 7 | loss: 0.0023376554876575464\n",
      "epoch 281 | step 8 | loss: 0.0026131583845056877\n",
      "epoch 281 | step 9 | loss: 0.0029185655208253123\n",
      "epoch 281 | step 10 | loss: 0.0032221125622745516\n",
      "epoch 281 | step 11 | loss: 0.003518150611149263\n",
      "epoch 282 | step 0 | loss: 0.0003101295362420549\n",
      "epoch 282 | step 1 | loss: 0.000591785834723842\n",
      "epoch 282 | step 2 | loss: 0.0009147325057759517\n",
      "epoch 282 | step 3 | loss: 0.0012317799307948363\n",
      "epoch 282 | step 4 | loss: 0.0015174162099358807\n",
      "epoch 282 | step 5 | loss: 0.0018167068974295265\n",
      "epoch 282 | step 6 | loss: 0.002101884767212982\n",
      "epoch 282 | step 7 | loss: 0.0023895397702042463\n",
      "epoch 282 | step 8 | loss: 0.002679060834778938\n",
      "epoch 282 | step 9 | loss: 0.0029507488341867527\n",
      "epoch 282 | step 10 | loss: 0.0032276101465017735\n",
      "epoch 282 | step 11 | loss: 0.003516074431360851\n",
      "epoch 283 | step 0 | loss: 0.00029975405275605897\n",
      "epoch 283 | step 1 | loss: 0.0005955377649722411\n",
      "epoch 283 | step 2 | loss: 0.0008610104914926914\n",
      "epoch 283 | step 3 | loss: 0.0011539239863708277\n",
      "epoch 283 | step 4 | loss: 0.0014509723781521672\n",
      "epoch 283 | step 5 | loss: 0.0017307521761590293\n",
      "epoch 283 | step 6 | loss: 0.00205832667420115\n",
      "epoch 283 | step 7 | loss: 0.0023533078351293283\n",
      "epoch 283 | step 8 | loss: 0.002663669494475237\n",
      "epoch 283 | step 9 | loss: 0.002964002410159156\n",
      "epoch 283 | step 10 | loss: 0.0032379044352230436\n",
      "epoch 283 | step 11 | loss: 0.00351198742876199\n",
      "epoch 284 | step 0 | loss: 0.00027287619289676987\n",
      "epoch 284 | step 1 | loss: 0.0005950390325939769\n",
      "epoch 284 | step 2 | loss: 0.0008654305512067939\n",
      "epoch 284 | step 3 | loss: 0.00114849235668847\n",
      "epoch 284 | step 4 | loss: 0.0014364220408860933\n",
      "epoch 284 | step 5 | loss: 0.0017196018723533387\n",
      "epoch 284 | step 6 | loss: 0.002042424884715116\n",
      "epoch 284 | step 7 | loss: 0.0023419214072375786\n",
      "epoch 284 | step 8 | loss: 0.002617454248281648\n",
      "epoch 284 | step 9 | loss: 0.002916738230471952\n",
      "epoch 284 | step 10 | loss: 0.003227635063435414\n",
      "epoch 284 | step 11 | loss: 0.003515926617128543\n",
      "epoch 285 | step 0 | loss: 0.0002950027293717178\n",
      "epoch 285 | step 1 | loss: 0.0005790415937836546\n",
      "epoch 285 | step 2 | loss: 0.0008799585234086938\n",
      "epoch 285 | step 3 | loss: 0.0011543113877488299\n",
      "epoch 285 | step 4 | loss: 0.001458547332461302\n",
      "epoch 285 | step 5 | loss: 0.0017608171303316617\n",
      "epoch 285 | step 6 | loss: 0.0020351686856551273\n",
      "epoch 285 | step 7 | loss: 0.002328528550645835\n",
      "epoch 285 | step 8 | loss: 0.00260383369142766\n",
      "epoch 285 | step 9 | loss: 0.002926046859166676\n",
      "epoch 285 | step 10 | loss: 0.003208889294708938\n",
      "epoch 285 | step 11 | loss: 0.0035232153567896266\n",
      "epoch 286 | step 0 | loss: 0.0002927440938202023\n",
      "epoch 286 | step 1 | loss: 0.0005701909775782518\n",
      "epoch 286 | step 2 | loss: 0.0008861263285040204\n",
      "epoch 286 | step 3 | loss: 0.0011543750143836518\n",
      "epoch 286 | step 4 | loss: 0.0014469519979932622\n",
      "epoch 286 | step 5 | loss: 0.001743909145988354\n",
      "epoch 286 | step 6 | loss: 0.0020183572902605007\n",
      "epoch 286 | step 7 | loss: 0.00231774279978627\n",
      "epoch 286 | step 8 | loss: 0.0026244136197771115\n",
      "epoch 286 | step 9 | loss: 0.0029214036914222787\n",
      "epoch 286 | step 10 | loss: 0.0031994246101892942\n",
      "epoch 286 | step 11 | loss: 0.0035270306856811846\n",
      "epoch 287 | step 0 | loss: 0.0003011185372995233\n",
      "epoch 287 | step 1 | loss: 0.0006091575008975326\n",
      "epoch 287 | step 2 | loss: 0.0008809972737207058\n",
      "epoch 287 | step 3 | loss: 0.001185446420243898\n",
      "epoch 287 | step 4 | loss: 0.0014493505621835279\n",
      "epoch 287 | step 5 | loss: 0.0017603592571351853\n",
      "epoch 287 | step 6 | loss: 0.0020693332168099437\n",
      "epoch 287 | step 7 | loss: 0.00237451149624912\n",
      "epoch 287 | step 8 | loss: 0.0026863354741014666\n",
      "epoch 287 | step 9 | loss: 0.0029562938976073535\n",
      "epoch 287 | step 10 | loss: 0.0032396913230056034\n",
      "epoch 287 | step 11 | loss: 0.0035110930671609615\n",
      "epoch 288 | step 0 | loss: 0.00029942006476627806\n",
      "epoch 288 | step 1 | loss: 0.0005776417603946201\n",
      "epoch 288 | step 2 | loss: 0.0008545924741689751\n",
      "epoch 288 | step 3 | loss: 0.0011462370787499842\n",
      "epoch 288 | step 4 | loss: 0.0014322486808932544\n",
      "epoch 288 | step 5 | loss: 0.0017389013726643583\n",
      "epoch 288 | step 6 | loss: 0.0020450786560288437\n",
      "epoch 288 | step 7 | loss: 0.002335860226821069\n",
      "epoch 288 | step 8 | loss: 0.002651499071332425\n",
      "epoch 288 | step 9 | loss: 0.0029146240688640848\n",
      "epoch 288 | step 10 | loss: 0.0031908666059623702\n",
      "epoch 288 | step 11 | loss: 0.003530076708407926\n",
      "epoch 289 | step 0 | loss: 0.00034232555879083445\n",
      "epoch 289 | step 1 | loss: 0.0006288531933075585\n",
      "epoch 289 | step 2 | loss: 0.0009070271689315728\n",
      "epoch 289 | step 3 | loss: 0.0011882957513805392\n",
      "epoch 289 | step 4 | loss: 0.0014441655456795695\n",
      "epoch 289 | step 5 | loss: 0.0017448399662909377\n",
      "epoch 289 | step 6 | loss: 0.0020491146571893952\n",
      "epoch 289 | step 7 | loss: 0.0023493782819691525\n",
      "epoch 289 | step 8 | loss: 0.0026301060365187083\n",
      "epoch 289 | step 9 | loss: 0.002936933632209102\n",
      "epoch 289 | step 10 | loss: 0.0032153462115183267\n",
      "epoch 289 | step 11 | loss: 0.0035206001037617063\n",
      "epoch 290 | step 0 | loss: 0.0002966307922353186\n",
      "epoch 290 | step 1 | loss: 0.0005938839100760927\n",
      "epoch 290 | step 2 | loss: 0.0008897662922460806\n",
      "epoch 290 | step 3 | loss: 0.00117885187994299\n",
      "epoch 290 | step 4 | loss: 0.0014623755969284092\n",
      "epoch 290 | step 5 | loss: 0.0017591371014603972\n",
      "epoch 290 | step 6 | loss: 0.002044050590148268\n",
      "epoch 290 | step 7 | loss: 0.0023619694165144477\n",
      "epoch 290 | step 8 | loss: 0.0026318940584885804\n",
      "epoch 290 | step 9 | loss: 0.00291324856739831\n",
      "epoch 290 | step 10 | loss: 0.003210243567453394\n",
      "epoch 290 | step 11 | loss: 0.0035227392658335214\n",
      "epoch 291 | step 0 | loss: 0.0002844933590304414\n",
      "epoch 291 | step 1 | loss: 0.0005527730289153092\n",
      "epoch 291 | step 2 | loss: 0.0008307684353899291\n",
      "epoch 291 | step 3 | loss: 0.001125289762174334\n",
      "epoch 291 | step 4 | loss: 0.001425452541718169\n",
      "epoch 291 | step 5 | loss: 0.0017190009032445365\n",
      "epoch 291 | step 6 | loss: 0.002015162823698775\n",
      "epoch 291 | step 7 | loss: 0.0023559546342739497\n",
      "epoch 291 | step 8 | loss: 0.0026661053769917904\n",
      "epoch 291 | step 9 | loss: 0.002940924644877394\n",
      "epoch 291 | step 10 | loss: 0.0032357462095887204\n",
      "epoch 291 | step 11 | loss: 0.0035124039635542606\n",
      "epoch 292 | step 0 | loss: 0.0002913567155832739\n",
      "epoch 292 | step 1 | loss: 0.0005668511559747433\n",
      "epoch 292 | step 2 | loss: 0.0008465388754663472\n",
      "epoch 292 | step 3 | loss: 0.0011533622890001097\n",
      "epoch 292 | step 4 | loss: 0.0014575944642923407\n",
      "epoch 292 | step 5 | loss: 0.0017766486956309671\n",
      "epoch 292 | step 6 | loss: 0.002053650984994789\n",
      "epoch 292 | step 7 | loss: 0.0023214435285605775\n",
      "epoch 292 | step 8 | loss: 0.0026187603410802326\n",
      "epoch 292 | step 9 | loss: 0.0029243507766895925\n",
      "epoch 292 | step 10 | loss: 0.0032112947399631927\n",
      "epoch 292 | step 11 | loss: 0.0035219167881705205\n",
      "epoch 293 | step 0 | loss: 0.00030972669275648534\n",
      "epoch 293 | step 1 | loss: 0.0005858607822179552\n",
      "epoch 293 | step 2 | loss: 0.000862338109729874\n",
      "epoch 293 | step 3 | loss: 0.001141522158091669\n",
      "epoch 293 | step 4 | loss: 0.001477790559630179\n",
      "epoch 293 | step 5 | loss: 0.001766392094796607\n",
      "epoch 293 | step 6 | loss: 0.002051466082805134\n",
      "epoch 293 | step 7 | loss: 0.0023265477202577918\n",
      "epoch 293 | step 8 | loss: 0.00260538385985926\n",
      "epoch 293 | step 9 | loss: 0.002904794791944518\n",
      "epoch 293 | step 10 | loss: 0.0032069419458907907\n",
      "epoch 293 | step 11 | loss: 0.003523728119726082\n",
      "epoch 294 | step 0 | loss: 0.00029856173984787137\n",
      "epoch 294 | step 1 | loss: 0.0005882103096148224\n",
      "epoch 294 | step 2 | loss: 0.0008816101646257991\n",
      "epoch 294 | step 3 | loss: 0.0011796543522625143\n",
      "epoch 294 | step 4 | loss: 0.0014506291405064213\n",
      "epoch 294 | step 5 | loss: 0.0017400652221341215\n",
      "epoch 294 | step 6 | loss: 0.0020737707486381597\n",
      "epoch 294 | step 7 | loss: 0.002351594274596472\n",
      "epoch 294 | step 8 | loss: 0.002636154856162564\n",
      "epoch 294 | step 9 | loss: 0.00293151434615974\n",
      "epoch 294 | step 10 | loss: 0.0032349170392875215\n",
      "epoch 294 | step 11 | loss: 0.003512682606073856\n",
      "epoch 295 | step 0 | loss: 0.0002792125776465646\n",
      "epoch 295 | step 1 | loss: 0.000570076102177754\n",
      "epoch 295 | step 2 | loss: 0.0008398937553776512\n",
      "epoch 295 | step 3 | loss: 0.0011060644591139152\n",
      "epoch 295 | step 4 | loss: 0.00140433979550461\n",
      "epoch 295 | step 5 | loss: 0.0016941539692338498\n",
      "epoch 295 | step 6 | loss: 0.0020207441460051534\n",
      "epoch 295 | step 7 | loss: 0.002312959955631059\n",
      "epoch 295 | step 8 | loss: 0.0026241729670608634\n",
      "epoch 295 | step 9 | loss: 0.002917900633407483\n",
      "epoch 295 | step 10 | loss: 0.0032175277057852726\n",
      "epoch 295 | step 11 | loss: 0.0035194502363348644\n",
      "epoch 296 | step 0 | loss: 0.00030423602957584865\n",
      "epoch 296 | step 1 | loss: 0.0006165156509124437\n",
      "epoch 296 | step 2 | loss: 0.0009111010798850915\n",
      "epoch 296 | step 3 | loss: 0.0011941116477907129\n",
      "epoch 296 | step 4 | loss: 0.0015108906593339108\n",
      "epoch 296 | step 5 | loss: 0.0018095663639474745\n",
      "epoch 296 | step 6 | loss: 0.002102004299715403\n",
      "epoch 296 | step 7 | loss: 0.0023845314390349716\n",
      "epoch 296 | step 8 | loss: 0.002689588493202967\n",
      "epoch 296 | step 9 | loss: 0.002973637277233473\n",
      "epoch 296 | step 10 | loss: 0.003234698547141114\n",
      "epoch 296 | step 11 | loss: 0.003512433218722768\n",
      "epoch 297 | step 0 | loss: 0.00030690127937194864\n",
      "epoch 297 | step 1 | loss: 0.000582098894926887\n",
      "epoch 297 | step 2 | loss: 0.0008611952143142102\n",
      "epoch 297 | step 3 | loss: 0.00116685642979671\n",
      "epoch 297 | step 4 | loss: 0.001467290365748241\n",
      "epoch 297 | step 5 | loss: 0.0017476429037228704\n",
      "epoch 297 | step 6 | loss: 0.002051654328932893\n",
      "epoch 297 | step 7 | loss: 0.0023445241043978343\n",
      "epoch 297 | step 8 | loss: 0.002636807720484764\n",
      "epoch 297 | step 9 | loss: 0.0029122329717935517\n",
      "epoch 297 | step 10 | loss: 0.0032149251657538755\n",
      "epoch 297 | step 11 | loss: 0.003520314237255108\n",
      "epoch 298 | step 0 | loss: 0.0003028595150606912\n",
      "epoch 298 | step 1 | loss: 0.0005900267930053166\n",
      "epoch 298 | step 2 | loss: 0.0008891817771661551\n",
      "epoch 298 | step 3 | loss: 0.0011832085247901996\n",
      "epoch 298 | step 4 | loss: 0.0014605146032131138\n",
      "epoch 298 | step 5 | loss: 0.0017488784994512002\n",
      "epoch 298 | step 6 | loss: 0.002036448652974901\n",
      "epoch 298 | step 7 | loss: 0.002314464650272261\n",
      "epoch 298 | step 8 | loss: 0.002620190467894196\n",
      "epoch 298 | step 9 | loss: 0.002931118089782138\n",
      "epoch 298 | step 10 | loss: 0.0032346045135695422\n",
      "epoch 298 | step 11 | loss: 0.003512584564053972\n",
      "epoch 299 | step 0 | loss: 0.00029035094786705057\n",
      "epoch 299 | step 1 | loss: 0.0005854524513643837\n",
      "epoch 299 | step 2 | loss: 0.0008782976909677727\n",
      "epoch 299 | step 3 | loss: 0.0011652465825176457\n",
      "epoch 299 | step 4 | loss: 0.0014509435130532803\n",
      "epoch 299 | step 5 | loss: 0.0017481364217052815\n",
      "epoch 299 | step 6 | loss: 0.002056703835095958\n",
      "epoch 299 | step 7 | loss: 0.002366348190451802\n",
      "epoch 299 | step 8 | loss: 0.0026628785020883913\n",
      "epoch 299 | step 9 | loss: 0.0029381829608961935\n",
      "epoch 299 | step 10 | loss: 0.00321866636922727\n",
      "epoch 299 | step 11 | loss: 0.0035188824540799703\n",
      "epoch 300 | step 0 | loss: 0.00028416480674168463\n",
      "epoch 300 | step 1 | loss: 0.0005857477385245339\n",
      "epoch 300 | step 2 | loss: 0.0009154300043034021\n",
      "epoch 300 | step 3 | loss: 0.0012125090191292872\n",
      "epoch 300 | step 4 | loss: 0.001515933386124484\n",
      "epoch 300 | step 5 | loss: 0.0018032475606661387\n",
      "epoch 300 | step 6 | loss: 0.0020679774192644547\n",
      "epoch 300 | step 7 | loss: 0.0023412331161985662\n",
      "epoch 300 | step 8 | loss: 0.002644300859063006\n",
      "epoch 300 | step 9 | loss: 0.0029123928308473187\n",
      "epoch 300 | step 10 | loss: 0.0032276270740277226\n",
      "epoch 300 | step 11 | loss: 0.003515045093199067\n",
      "epoch 301 | step 0 | loss: 0.0002931877545420106\n",
      "epoch 301 | step 1 | loss: 0.0005758433873732426\n",
      "epoch 301 | step 2 | loss: 0.0008550005646283828\n",
      "epoch 301 | step 3 | loss: 0.0011391385825031421\n",
      "epoch 301 | step 4 | loss: 0.001461402533196045\n",
      "epoch 301 | step 5 | loss: 0.0017536467573668012\n",
      "epoch 301 | step 6 | loss: 0.002057744029305289\n",
      "epoch 301 | step 7 | loss: 0.00235885824064964\n",
      "epoch 301 | step 8 | loss: 0.002668858130092875\n",
      "epoch 301 | step 9 | loss: 0.0029389561845469016\n",
      "epoch 301 | step 10 | loss: 0.003225534593278943\n",
      "epoch 301 | step 11 | loss: 0.0035159090032490896\n",
      "epoch 302 | step 0 | loss: 0.0002923860624494224\n",
      "epoch 302 | step 1 | loss: 0.0005775191579624938\n",
      "epoch 302 | step 2 | loss: 0.0008805384685760555\n",
      "epoch 302 | step 3 | loss: 0.0011641201646378717\n",
      "epoch 302 | step 4 | loss: 0.001484192478457664\n",
      "epoch 302 | step 5 | loss: 0.0017784309612824257\n",
      "epoch 302 | step 6 | loss: 0.002077986102892618\n",
      "epoch 302 | step 7 | loss: 0.0023561067356344936\n",
      "epoch 302 | step 8 | loss: 0.0026414417089515195\n",
      "epoch 302 | step 9 | loss: 0.002952158203169095\n",
      "epoch 302 | step 10 | loss: 0.0031999065169198296\n",
      "epoch 302 | step 11 | loss: 0.0035259438094879885\n",
      "epoch 303 | step 0 | loss: 0.00030224353133357956\n",
      "epoch 303 | step 1 | loss: 0.0005980872100819071\n",
      "epoch 303 | step 2 | loss: 0.0008856757716123699\n",
      "epoch 303 | step 3 | loss: 0.001186528413048557\n",
      "epoch 303 | step 4 | loss: 0.0014782836411522562\n",
      "epoch 303 | step 5 | loss: 0.001764061728858709\n",
      "epoch 303 | step 6 | loss: 0.002070389349202762\n",
      "epoch 303 | step 7 | loss: 0.002359402018868859\n",
      "epoch 303 | step 8 | loss: 0.0026402332527178155\n",
      "epoch 303 | step 9 | loss: 0.002913926733865647\n",
      "epoch 303 | step 10 | loss: 0.003217618188028356\n",
      "epoch 303 | step 11 | loss: 0.0035189203873308957\n",
      "epoch 304 | step 0 | loss: 0.000305859603971316\n",
      "epoch 304 | step 1 | loss: 0.0006151219673928828\n",
      "epoch 304 | step 2 | loss: 0.000888580708525422\n",
      "epoch 304 | step 3 | loss: 0.0011810124161461305\n",
      "epoch 304 | step 4 | loss: 0.0015072148290117045\n",
      "epoch 304 | step 5 | loss: 0.0018002446772031836\n",
      "epoch 304 | step 6 | loss: 0.0021101516901668943\n",
      "epoch 304 | step 7 | loss: 0.0023977699882800916\n",
      "epoch 304 | step 8 | loss: 0.002654910274544438\n",
      "epoch 304 | step 9 | loss: 0.0029424519215465158\n",
      "epoch 304 | step 10 | loss: 0.0032159178165220405\n",
      "epoch 304 | step 11 | loss: 0.003519450219056314\n",
      "epoch 305 | step 0 | loss: 0.00028909647099072496\n",
      "epoch 305 | step 1 | loss: 0.00057914124347438\n",
      "epoch 305 | step 2 | loss: 0.0008814201760464454\n",
      "epoch 305 | step 3 | loss: 0.0011539415454745747\n",
      "epoch 305 | step 4 | loss: 0.00144270118971121\n",
      "epoch 305 | step 5 | loss: 0.00173323039325358\n",
      "epoch 305 | step 6 | loss: 0.0020301483299111694\n",
      "epoch 305 | step 7 | loss: 0.0023178238594681005\n",
      "epoch 305 | step 8 | loss: 0.002630145798905261\n",
      "epoch 305 | step 9 | loss: 0.002912947087937333\n",
      "epoch 305 | step 10 | loss: 0.0032028873471576375\n",
      "epoch 305 | step 11 | loss: 0.0035247169908378126\n",
      "epoch 306 | step 0 | loss: 0.00029122362124374515\n",
      "epoch 306 | step 1 | loss: 0.0005620969735712279\n",
      "epoch 306 | step 2 | loss: 0.000875564776141999\n",
      "epoch 306 | step 3 | loss: 0.001193218925280581\n",
      "epoch 306 | step 4 | loss: 0.001471663221438855\n",
      "epoch 306 | step 5 | loss: 0.0017602948477790582\n",
      "epoch 306 | step 6 | loss: 0.002100249615419704\n",
      "epoch 306 | step 7 | loss: 0.002397738307218666\n",
      "epoch 306 | step 8 | loss: 0.00266269037660291\n",
      "epoch 306 | step 9 | loss: 0.0029454621708810903\n",
      "epoch 306 | step 10 | loss: 0.0032361004794389386\n",
      "epoch 306 | step 11 | loss: 0.003511429967205325\n",
      "epoch 307 | step 0 | loss: 0.00030393900558267915\n",
      "epoch 307 | step 1 | loss: 0.0005951991060194129\n",
      "epoch 307 | step 2 | loss: 0.0008935043236427938\n",
      "epoch 307 | step 3 | loss: 0.0011710850768239646\n",
      "epoch 307 | step 4 | loss: 0.0014675560335066554\n",
      "epoch 307 | step 5 | loss: 0.0017365407710046195\n",
      "epoch 307 | step 6 | loss: 0.0020489345143706135\n",
      "epoch 307 | step 7 | loss: 0.002338343146312515\n",
      "epoch 307 | step 8 | loss: 0.002644722012020195\n",
      "epoch 307 | step 9 | loss: 0.002929926312749107\n",
      "epoch 307 | step 10 | loss: 0.0032195609855044513\n",
      "epoch 307 | step 11 | loss: 0.003517980794543485\n",
      "epoch 308 | step 0 | loss: 0.0002932885173294534\n",
      "epoch 308 | step 1 | loss: 0.0005625656020004024\n",
      "epoch 308 | step 2 | loss: 0.0008659478481293847\n",
      "epoch 308 | step 3 | loss: 0.0011401689635664764\n",
      "epoch 308 | step 4 | loss: 0.0014372750412340602\n",
      "epoch 308 | step 5 | loss: 0.001738837050029133\n",
      "epoch 308 | step 6 | loss: 0.002058723476626329\n",
      "epoch 308 | step 7 | loss: 0.002342532811651379\n",
      "epoch 308 | step 8 | loss: 0.002659128942384461\n",
      "epoch 308 | step 9 | loss: 0.0029300500182518037\n",
      "epoch 308 | step 10 | loss: 0.003237340713562335\n",
      "epoch 308 | step 11 | loss: 0.0035109529146863877\n",
      "epoch 309 | step 0 | loss: 0.00026652855566973845\n",
      "epoch 309 | step 1 | loss: 0.0005592459255189719\n",
      "epoch 309 | step 2 | loss: 0.0008334935414485084\n",
      "epoch 309 | step 3 | loss: 0.0011091632877343906\n",
      "epoch 309 | step 4 | loss: 0.0014165230944446039\n",
      "epoch 309 | step 5 | loss: 0.0017159537284610654\n",
      "epoch 309 | step 6 | loss: 0.0020124573350938438\n",
      "epoch 309 | step 7 | loss: 0.002310752489661617\n",
      "epoch 309 | step 8 | loss: 0.0026268413271087606\n",
      "epoch 309 | step 9 | loss: 0.0029347331667887826\n",
      "epoch 309 | step 10 | loss: 0.0032182722009646003\n",
      "epoch 309 | step 11 | loss: 0.003518265235052423\n",
      "epoch 310 | step 0 | loss: 0.00025724845481542974\n",
      "epoch 310 | step 1 | loss: 0.0005636427559738104\n",
      "epoch 310 | step 2 | loss: 0.0008509330999502669\n",
      "epoch 310 | step 3 | loss: 0.0011776374091824286\n",
      "epoch 310 | step 4 | loss: 0.001498761795068557\n",
      "epoch 310 | step 5 | loss: 0.001802702985507431\n",
      "epoch 310 | step 6 | loss: 0.0020966506073908093\n",
      "epoch 310 | step 7 | loss: 0.00238384696537734\n",
      "epoch 310 | step 8 | loss: 0.0026563298657499386\n",
      "epoch 310 | step 9 | loss: 0.002954820337018632\n",
      "epoch 310 | step 10 | loss: 0.0032477096911558145\n",
      "epoch 310 | step 11 | loss: 0.003506759244709169\n",
      "epoch 311 | step 0 | loss: 0.0002994649989814508\n",
      "epoch 311 | step 1 | loss: 0.0005946653765367315\n",
      "epoch 311 | step 2 | loss: 0.0008844906212180708\n",
      "epoch 311 | step 3 | loss: 0.0011690761690376255\n",
      "epoch 311 | step 4 | loss: 0.0014650971639839552\n",
      "epoch 311 | step 5 | loss: 0.0017736695278775313\n",
      "epoch 311 | step 6 | loss: 0.002069678164115653\n",
      "epoch 311 | step 7 | loss: 0.0023433885708247686\n",
      "epoch 311 | step 8 | loss: 0.0026448602785753067\n",
      "epoch 311 | step 9 | loss: 0.002903260854402079\n",
      "epoch 311 | step 10 | loss: 0.003218463883065975\n",
      "epoch 311 | step 11 | loss: 0.003518042733616683\n",
      "epoch 312 | step 0 | loss: 0.00026753170128699587\n",
      "epoch 312 | step 1 | loss: 0.0005548236701987643\n",
      "epoch 312 | step 2 | loss: 0.0008542381199088034\n",
      "epoch 312 | step 3 | loss: 0.001153030990208223\n",
      "epoch 312 | step 4 | loss: 0.001456622960920946\n",
      "epoch 312 | step 5 | loss: 0.0017487730534445217\n",
      "epoch 312 | step 6 | loss: 0.0020363504335170184\n",
      "epoch 312 | step 7 | loss: 0.002312965518601729\n",
      "epoch 312 | step 8 | loss: 0.0026129284577281055\n",
      "epoch 312 | step 9 | loss: 0.0029106489578876295\n",
      "epoch 312 | step 10 | loss: 0.00319884192208906\n",
      "epoch 312 | step 11 | loss: 0.003525644161506869\n",
      "epoch 313 | step 0 | loss: 0.0002750127899568685\n",
      "epoch 313 | step 1 | loss: 0.0005655528676959953\n",
      "epoch 313 | step 2 | loss: 0.0008606055670290382\n",
      "epoch 313 | step 3 | loss: 0.0011767878169573363\n",
      "epoch 313 | step 4 | loss: 0.0014690472223567498\n",
      "epoch 313 | step 5 | loss: 0.0017748420994893501\n",
      "epoch 313 | step 6 | loss: 0.0020906261493137523\n",
      "epoch 313 | step 7 | loss: 0.0024046698858174844\n",
      "epoch 313 | step 8 | loss: 0.002663800986343101\n",
      "epoch 313 | step 9 | loss: 0.0029466708626686416\n",
      "epoch 313 | step 10 | loss: 0.0032293696425174183\n",
      "epoch 313 | step 11 | loss: 0.003513710606018291\n",
      "epoch 314 | step 0 | loss: 0.0003083036883725828\n",
      "epoch 314 | step 1 | loss: 0.0006042371920517165\n",
      "epoch 314 | step 2 | loss: 0.0008988021885754695\n",
      "epoch 314 | step 3 | loss: 0.0011985446998545955\n",
      "epoch 314 | step 4 | loss: 0.0014789273124466867\n",
      "epoch 314 | step 5 | loss: 0.0017664084196548524\n",
      "epoch 314 | step 6 | loss: 0.002082438060464457\n",
      "epoch 314 | step 7 | loss: 0.0024054697859666055\n",
      "epoch 314 | step 8 | loss: 0.002660890582545964\n",
      "epoch 314 | step 9 | loss: 0.002939425860574764\n",
      "epoch 314 | step 10 | loss: 0.003218404206495995\n",
      "epoch 314 | step 11 | loss: 0.003518016899175794\n",
      "epoch 315 | step 0 | loss: 0.00029314864865845705\n",
      "epoch 315 | step 1 | loss: 0.0005984161925689067\n",
      "epoch 315 | step 2 | loss: 0.0008823956084668512\n",
      "epoch 315 | step 3 | loss: 0.0011566517686349951\n",
      "epoch 315 | step 4 | loss: 0.0014222130482049798\n",
      "epoch 315 | step 5 | loss: 0.0017273900269869247\n",
      "epoch 315 | step 6 | loss: 0.002012760123644131\n",
      "epoch 315 | step 7 | loss: 0.002330647165692729\n",
      "epoch 315 | step 8 | loss: 0.002620784424472475\n",
      "epoch 315 | step 9 | loss: 0.002910941177729787\n",
      "epoch 315 | step 10 | loss: 0.0032249266098520702\n",
      "epoch 315 | step 11 | loss: 0.0035153812297987726\n",
      "epoch 316 | step 0 | loss: 0.0002884590819736656\n",
      "epoch 316 | step 1 | loss: 0.0005654259815243817\n",
      "epoch 316 | step 2 | loss: 0.0008548376232702583\n",
      "epoch 316 | step 3 | loss: 0.0011598101792966743\n",
      "epoch 316 | step 4 | loss: 0.0014437450681394379\n",
      "epoch 316 | step 5 | loss: 0.0017102368980280054\n",
      "epoch 316 | step 6 | loss: 0.002038532318787504\n",
      "epoch 316 | step 7 | loss: 0.0023231794062786868\n",
      "epoch 316 | step 8 | loss: 0.0026221530557859743\n",
      "epoch 316 | step 9 | loss: 0.0029299036875179937\n",
      "epoch 316 | step 10 | loss: 0.0032136757711494794\n",
      "epoch 316 | step 11 | loss: 0.0035196743004577043\n",
      "epoch 317 | step 0 | loss: 0.00029787372172820876\n",
      "epoch 317 | step 1 | loss: 0.0005786350816606729\n",
      "epoch 317 | step 2 | loss: 0.0008614971563769301\n",
      "epoch 317 | step 3 | loss: 0.0011601195649796323\n",
      "epoch 317 | step 4 | loss: 0.001449202355557355\n",
      "epoch 317 | step 5 | loss: 0.0017679176397660187\n",
      "epoch 317 | step 6 | loss: 0.002066011211393763\n",
      "epoch 317 | step 7 | loss: 0.002375579125240113\n",
      "epoch 317 | step 8 | loss: 0.0026564769318120065\n",
      "epoch 317 | step 9 | loss: 0.002925451376629385\n",
      "epoch 317 | step 10 | loss: 0.003220220594581396\n",
      "epoch 317 | step 11 | loss: 0.003517212798458718\n",
      "epoch 318 | step 0 | loss: 0.000279359347601528\n",
      "epoch 318 | step 1 | loss: 0.000581271512619234\n",
      "epoch 318 | step 2 | loss: 0.0008788228143291846\n",
      "epoch 318 | step 3 | loss: 0.0011736682110369096\n",
      "epoch 318 | step 4 | loss: 0.0014611221577103951\n",
      "epoch 318 | step 5 | loss: 0.0017425439155224203\n",
      "epoch 318 | step 6 | loss: 0.002035256268370088\n",
      "epoch 318 | step 7 | loss: 0.002325579093009095\n",
      "epoch 318 | step 8 | loss: 0.002625757730805582\n",
      "epoch 318 | step 9 | loss: 0.0029185462949478836\n",
      "epoch 318 | step 10 | loss: 0.0032261038886432294\n",
      "epoch 318 | step 11 | loss: 0.003514966271673598\n",
      "epoch 319 | step 0 | loss: 0.00030291631078361225\n",
      "epoch 319 | step 1 | loss: 0.0005875630816132241\n",
      "epoch 319 | step 2 | loss: 0.0008966558697689703\n",
      "epoch 319 | step 3 | loss: 0.0011737897340269054\n",
      "epoch 319 | step 4 | loss: 0.001471809669344157\n",
      "epoch 319 | step 5 | loss: 0.0017799346766935703\n",
      "epoch 319 | step 6 | loss: 0.002079255937358713\n",
      "epoch 319 | step 7 | loss: 0.0023827146024670534\n",
      "epoch 319 | step 8 | loss: 0.0026599609042066937\n",
      "epoch 319 | step 9 | loss: 0.002936183476090852\n",
      "epoch 319 | step 10 | loss: 0.0032314384992347942\n",
      "epoch 319 | step 11 | loss: 0.0035125090045585125\n",
      "epoch 320 | step 0 | loss: 0.00030480994752227463\n",
      "epoch 320 | step 1 | loss: 0.0006179163899539041\n",
      "epoch 320 | step 2 | loss: 0.0009195856724482351\n",
      "epoch 320 | step 3 | loss: 0.0012230828591299977\n",
      "epoch 320 | step 4 | loss: 0.001510643627657129\n",
      "epoch 320 | step 5 | loss: 0.00179040081148902\n",
      "epoch 320 | step 6 | loss: 0.002070384233248064\n",
      "epoch 320 | step 7 | loss: 0.0023705126396238465\n",
      "epoch 320 | step 8 | loss: 0.0026540904683730916\n",
      "epoch 320 | step 9 | loss: 0.0029508312626764475\n",
      "epoch 320 | step 10 | loss: 0.0032385931255595564\n",
      "epoch 320 | step 11 | loss: 0.0035096851070971005\n",
      "epoch 321 | step 0 | loss: 0.00031392999451851274\n",
      "epoch 321 | step 1 | loss: 0.0006082133323132434\n",
      "epoch 321 | step 2 | loss: 0.0009420616503768277\n",
      "epoch 321 | step 3 | loss: 0.0012343657620363607\n",
      "epoch 321 | step 4 | loss: 0.0015133908485302945\n",
      "epoch 321 | step 5 | loss: 0.0018064693878790373\n",
      "epoch 321 | step 6 | loss: 0.002100807105011676\n",
      "epoch 321 | step 7 | loss: 0.0023838000478791565\n",
      "epoch 321 | step 8 | loss: 0.002665314664032261\n",
      "epoch 321 | step 9 | loss: 0.0029469821972087262\n",
      "epoch 321 | step 10 | loss: 0.0032192463302413562\n",
      "epoch 321 | step 11 | loss: 0.003517338524486318\n",
      "epoch 322 | step 0 | loss: 0.000288517543049814\n",
      "epoch 322 | step 1 | loss: 0.0005698824683377537\n",
      "epoch 322 | step 2 | loss: 0.0008483035741133132\n",
      "epoch 322 | step 3 | loss: 0.0011468161418286369\n",
      "epoch 322 | step 4 | loss: 0.001441916507142428\n",
      "epoch 322 | step 5 | loss: 0.0017749173310273119\n",
      "epoch 322 | step 6 | loss: 0.002064725950333362\n",
      "epoch 322 | step 7 | loss: 0.0023616829946272546\n",
      "epoch 322 | step 8 | loss: 0.002656546304700352\n",
      "epoch 322 | step 9 | loss: 0.0029494219312539262\n",
      "epoch 322 | step 10 | loss: 0.0032260604664009393\n",
      "epoch 322 | step 11 | loss: 0.0035145819688640655\n",
      "epoch 323 | step 0 | loss: 0.0002819927015466664\n",
      "epoch 323 | step 1 | loss: 0.0005774692185430626\n",
      "epoch 323 | step 2 | loss: 0.0008974838359482029\n",
      "epoch 323 | step 3 | loss: 0.0011950770597419593\n",
      "epoch 323 | step 4 | loss: 0.0014774090973311501\n",
      "epoch 323 | step 5 | loss: 0.0017624098097205848\n",
      "epoch 323 | step 6 | loss: 0.0020572544984274973\n",
      "epoch 323 | step 7 | loss: 0.0023430954978728375\n",
      "epoch 323 | step 8 | loss: 0.002645042908039851\n",
      "epoch 323 | step 9 | loss: 0.0029530620047967353\n",
      "epoch 323 | step 10 | loss: 0.003233722242294909\n",
      "epoch 323 | step 11 | loss: 0.0035114303593575384\n",
      "epoch 324 | step 0 | loss: 0.0002819725916684879\n",
      "epoch 324 | step 1 | loss: 0.0005871002672177743\n",
      "epoch 324 | step 2 | loss: 0.0008686956444960918\n",
      "epoch 324 | step 3 | loss: 0.001161250872023539\n",
      "epoch 324 | step 4 | loss: 0.0014110303410376768\n",
      "epoch 324 | step 5 | loss: 0.0017505856032727228\n",
      "epoch 324 | step 6 | loss: 0.00205886882974721\n",
      "epoch 324 | step 7 | loss: 0.0023586969534833647\n",
      "epoch 324 | step 8 | loss: 0.00264642916511881\n",
      "epoch 324 | step 9 | loss: 0.0029189445995240326\n",
      "epoch 324 | step 10 | loss: 0.0032095073191847276\n",
      "epoch 324 | step 11 | loss: 0.00352113878696447\n",
      "epoch 325 | step 0 | loss: 0.00029732594617669654\n",
      "epoch 325 | step 1 | loss: 0.0005831574193599623\n",
      "epoch 325 | step 2 | loss: 0.0008506462031538738\n",
      "epoch 325 | step 3 | loss: 0.0011388546134290304\n",
      "epoch 325 | step 4 | loss: 0.0014312743507640762\n",
      "epoch 325 | step 5 | loss: 0.0017270334497593493\n",
      "epoch 325 | step 6 | loss: 0.0020362419335849145\n",
      "epoch 325 | step 7 | loss: 0.0023314907255399964\n",
      "epoch 325 | step 8 | loss: 0.00262581655902709\n",
      "epoch 325 | step 9 | loss: 0.0029060604915809867\n",
      "epoch 325 | step 10 | loss: 0.0031930838205840965\n",
      "epoch 325 | step 11 | loss: 0.003527256869754223\n",
      "epoch 326 | step 0 | loss: 0.0002890083593471024\n",
      "epoch 326 | step 1 | loss: 0.0005788019304540709\n",
      "epoch 326 | step 2 | loss: 0.0008644408954974814\n",
      "epoch 326 | step 3 | loss: 0.0012025193553397069\n",
      "epoch 326 | step 4 | loss: 0.0014914910468027797\n",
      "epoch 326 | step 5 | loss: 0.0017704858784240718\n",
      "epoch 326 | step 6 | loss: 0.002027228452533469\n",
      "epoch 326 | step 7 | loss: 0.002324154296079117\n",
      "epoch 326 | step 8 | loss: 0.002620325121703457\n",
      "epoch 326 | step 9 | loss: 0.002947893709428894\n",
      "epoch 326 | step 10 | loss: 0.0032347206068271592\n",
      "epoch 326 | step 11 | loss: 0.003510877095696683\n",
      "epoch 327 | step 0 | loss: 0.00028535094052476486\n",
      "epoch 327 | step 1 | loss: 0.0005891297302228276\n",
      "epoch 327 | step 2 | loss: 0.0008987288038119752\n",
      "epoch 327 | step 3 | loss: 0.001203169091119422\n",
      "epoch 327 | step 4 | loss: 0.0014992930561221938\n",
      "epoch 327 | step 5 | loss: 0.0018091001986548932\n",
      "epoch 327 | step 6 | loss: 0.0020844643550338403\n",
      "epoch 327 | step 7 | loss: 0.002392817139935838\n",
      "epoch 327 | step 8 | loss: 0.0026830819250318262\n",
      "epoch 327 | step 9 | loss: 0.0029554294124466934\n",
      "epoch 327 | step 10 | loss: 0.003223623594032607\n",
      "epoch 327 | step 11 | loss: 0.0035151401774318235\n",
      "epoch 328 | step 0 | loss: 0.0003010370786011006\n",
      "epoch 328 | step 1 | loss: 0.000583667746438649\n",
      "epoch 328 | step 2 | loss: 0.0008482351924169196\n",
      "epoch 328 | step 3 | loss: 0.0011457170525823995\n",
      "epoch 328 | step 4 | loss: 0.0014486730915932125\n",
      "epoch 328 | step 5 | loss: 0.001710053796777938\n",
      "epoch 328 | step 6 | loss: 0.0020271490688107685\n",
      "epoch 328 | step 7 | loss: 0.0023109709827692218\n",
      "epoch 328 | step 8 | loss: 0.0026069022646573767\n",
      "epoch 328 | step 9 | loss: 0.0029132289609100368\n",
      "epoch 328 | step 10 | loss: 0.003227084703412369\n",
      "epoch 328 | step 11 | loss: 0.0035137735375202336\n",
      "epoch 329 | step 0 | loss: 0.0002935864487771404\n",
      "epoch 329 | step 1 | loss: 0.000580806376373578\n",
      "epoch 329 | step 2 | loss: 0.0008684264613895593\n",
      "epoch 329 | step 3 | loss: 0.0011518747382610953\n",
      "epoch 329 | step 4 | loss: 0.0014319540256723605\n",
      "epoch 329 | step 5 | loss: 0.0017243351007450597\n",
      "epoch 329 | step 6 | loss: 0.0020032968230431227\n",
      "epoch 329 | step 7 | loss: 0.002304658502977455\n",
      "epoch 329 | step 8 | loss: 0.0026063611660597182\n",
      "epoch 329 | step 9 | loss: 0.0029141753796136332\n",
      "epoch 329 | step 10 | loss: 0.003244149601373692\n",
      "epoch 329 | step 11 | loss: 0.003507175196973712\n",
      "epoch 330 | step 0 | loss: 0.00029714103974659426\n",
      "epoch 330 | step 1 | loss: 0.0005727806546574268\n",
      "epoch 330 | step 2 | loss: 0.0009025970941647941\n",
      "epoch 330 | step 3 | loss: 0.0011718539807839732\n",
      "epoch 330 | step 4 | loss: 0.0014680674358101774\n",
      "epoch 330 | step 5 | loss: 0.001756408721629367\n",
      "epoch 330 | step 6 | loss: 0.002077086479808407\n",
      "epoch 330 | step 7 | loss: 0.0023793242831538592\n",
      "epoch 330 | step 8 | loss: 0.0026724993253218325\n",
      "epoch 330 | step 9 | loss: 0.002950860742783892\n",
      "epoch 330 | step 10 | loss: 0.003235079149172257\n",
      "epoch 330 | step 11 | loss: 0.003510616534464844\n",
      "epoch 331 | step 0 | loss: 0.000306739292191139\n",
      "epoch 331 | step 1 | loss: 0.0005845483116495096\n",
      "epoch 331 | step 2 | loss: 0.0008756987819144438\n",
      "epoch 331 | step 3 | loss: 0.0011788074588737491\n",
      "epoch 331 | step 4 | loss: 0.0014541184128744583\n",
      "epoch 331 | step 5 | loss: 0.0017532662955474314\n",
      "epoch 331 | step 6 | loss: 0.0020625036374643164\n",
      "epoch 331 | step 7 | loss: 0.0023698570671438816\n",
      "epoch 331 | step 8 | loss: 0.002661960137770055\n",
      "epoch 331 | step 9 | loss: 0.0029442465832408755\n",
      "epoch 331 | step 10 | loss: 0.0032305319841379994\n",
      "epoch 331 | step 11 | loss: 0.00351228753183616\n",
      "epoch 332 | step 0 | loss: 0.00030228794897398396\n",
      "epoch 332 | step 1 | loss: 0.0005851210174814839\n",
      "epoch 332 | step 2 | loss: 0.0008779053319009466\n",
      "epoch 332 | step 3 | loss: 0.0011652062396037444\n",
      "epoch 332 | step 4 | loss: 0.0014622310411066598\n",
      "epoch 332 | step 5 | loss: 0.001767726009333634\n",
      "epoch 332 | step 6 | loss: 0.0020755910198475393\n",
      "epoch 332 | step 7 | loss: 0.0023599705828575754\n",
      "epoch 332 | step 8 | loss: 0.0026618594114748333\n",
      "epoch 332 | step 9 | loss: 0.0029327251247054584\n",
      "epoch 332 | step 10 | loss: 0.0032218207092510457\n",
      "epoch 332 | step 11 | loss: 0.0035156230555265025\n",
      "epoch 333 | step 0 | loss: 0.0002790823914600703\n",
      "epoch 333 | step 1 | loss: 0.0005406629333269194\n",
      "epoch 333 | step 2 | loss: 0.0008475157793507834\n",
      "epoch 333 | step 3 | loss: 0.0011845912629934237\n",
      "epoch 333 | step 4 | loss: 0.001465364471514809\n",
      "epoch 333 | step 5 | loss: 0.0017530004638512068\n",
      "epoch 333 | step 6 | loss: 0.002018850436474788\n",
      "epoch 333 | step 7 | loss: 0.0023270849325362913\n",
      "epoch 333 | step 8 | loss: 0.002630555867305514\n",
      "epoch 333 | step 9 | loss: 0.0029301579480274114\n",
      "epoch 333 | step 10 | loss: 0.0032218480675204947\n",
      "epoch 333 | step 11 | loss: 0.003515540481210201\n",
      "epoch 334 | step 0 | loss: 0.0002995839006146945\n",
      "epoch 334 | step 1 | loss: 0.0006018412817438997\n",
      "epoch 334 | step 2 | loss: 0.0008972783781192886\n",
      "epoch 334 | step 3 | loss: 0.0011889491777112438\n",
      "epoch 334 | step 4 | loss: 0.0014798461856997448\n",
      "epoch 334 | step 5 | loss: 0.0017830237131837838\n",
      "epoch 334 | step 6 | loss: 0.002056134421860573\n",
      "epoch 334 | step 7 | loss: 0.0023393147208956456\n",
      "epoch 334 | step 8 | loss: 0.002646101424450918\n",
      "epoch 334 | step 9 | loss: 0.0029432591310250855\n",
      "epoch 334 | step 10 | loss: 0.0032230580862499325\n",
      "epoch 334 | step 11 | loss: 0.003515117070021221\n",
      "epoch 335 | step 0 | loss: 0.0002966920409497982\n",
      "epoch 335 | step 1 | loss: 0.00059035700233128\n",
      "epoch 335 | step 2 | loss: 0.0008901985003224592\n",
      "epoch 335 | step 3 | loss: 0.0011659981693356402\n",
      "epoch 335 | step 4 | loss: 0.0014675096044219655\n",
      "epoch 335 | step 5 | loss: 0.001750117034716116\n",
      "epoch 335 | step 6 | loss: 0.002032215343063881\n",
      "epoch 335 | step 7 | loss: 0.0023213327362235185\n",
      "epoch 335 | step 8 | loss: 0.0026082657055010185\n",
      "epoch 335 | step 9 | loss: 0.002891798700255236\n",
      "epoch 335 | step 10 | loss: 0.003218443412938688\n",
      "epoch 335 | step 11 | loss: 0.0035167727952528103\n",
      "epoch 336 | step 0 | loss: 0.00027787009538544476\n",
      "epoch 336 | step 1 | loss: 0.0005679170065211108\n",
      "epoch 336 | step 2 | loss: 0.0008584581261623557\n",
      "epoch 336 | step 3 | loss: 0.0011430052266479082\n",
      "epoch 336 | step 4 | loss: 0.0014510983609140413\n",
      "epoch 336 | step 5 | loss: 0.0017334385075983244\n",
      "epoch 336 | step 6 | loss: 0.0020335908278297423\n",
      "epoch 336 | step 7 | loss: 0.002345926255324581\n",
      "epoch 336 | step 8 | loss: 0.0026123188842279285\n",
      "epoch 336 | step 9 | loss: 0.002900051245439746\n",
      "epoch 336 | step 10 | loss: 0.003201434093284774\n",
      "epoch 336 | step 11 | loss: 0.003523364332976609\n",
      "epoch 337 | step 0 | loss: 0.0002860377628137945\n",
      "epoch 337 | step 1 | loss: 0.0006120945285872236\n",
      "epoch 337 | step 2 | loss: 0.0009102584432581345\n",
      "epoch 337 | step 3 | loss: 0.001195663155403951\n",
      "epoch 337 | step 4 | loss: 0.0014875001535071862\n",
      "epoch 337 | step 5 | loss: 0.0017906455859793995\n",
      "epoch 337 | step 6 | loss: 0.002066173023238142\n",
      "epoch 337 | step 7 | loss: 0.0023521372369113675\n",
      "epoch 337 | step 8 | loss: 0.0026400969093176396\n",
      "epoch 337 | step 9 | loss: 0.0029481705791606434\n",
      "epoch 337 | step 10 | loss: 0.003211227985527536\n",
      "epoch 337 | step 11 | loss: 0.0035194568240400894\n",
      "epoch 338 | step 0 | loss: 0.00026292059153313957\n",
      "epoch 338 | step 1 | loss: 0.0005814868000974885\n",
      "epoch 338 | step 2 | loss: 0.0008696918622781628\n",
      "epoch 338 | step 3 | loss: 0.0011748412066222018\n",
      "epoch 338 | step 4 | loss: 0.0014827672396708108\n",
      "epoch 338 | step 5 | loss: 0.001777030561347276\n",
      "epoch 338 | step 6 | loss: 0.0020473245260649066\n",
      "epoch 338 | step 7 | loss: 0.0023336263490222925\n",
      "epoch 338 | step 8 | loss: 0.0026461030979891996\n",
      "epoch 338 | step 9 | loss: 0.00294227255025684\n",
      "epoch 338 | step 10 | loss: 0.0032107603289751596\n",
      "epoch 338 | step 11 | loss: 0.0035197157072915\n",
      "epoch 339 | step 0 | loss: 0.0002944964873145066\n",
      "epoch 339 | step 1 | loss: 0.0005708105538815583\n",
      "epoch 339 | step 2 | loss: 0.0008646850094740303\n",
      "epoch 339 | step 3 | loss: 0.0011601510724733074\n",
      "epoch 339 | step 4 | loss: 0.0014451238095736629\n",
      "epoch 339 | step 5 | loss: 0.001734048935603795\n",
      "epoch 339 | step 6 | loss: 0.0020203785871910737\n",
      "epoch 339 | step 7 | loss: 0.0023195374416322074\n",
      "epoch 339 | step 8 | loss: 0.0026178643465078455\n",
      "epoch 339 | step 9 | loss: 0.0029417412670949368\n",
      "epoch 339 | step 10 | loss: 0.0032280932365555412\n",
      "epoch 339 | step 11 | loss: 0.0035127137468619943\n",
      "epoch 340 | step 0 | loss: 0.00031655313192907846\n",
      "epoch 340 | step 1 | loss: 0.0005997741960957361\n",
      "epoch 340 | step 2 | loss: 0.0009043820016101999\n",
      "epoch 340 | step 3 | loss: 0.0011964451418325778\n",
      "epoch 340 | step 4 | loss: 0.0014951837748506034\n",
      "epoch 340 | step 5 | loss: 0.0018003377127425474\n",
      "epoch 340 | step 6 | loss: 0.0020679531120465925\n",
      "epoch 340 | step 7 | loss: 0.0023687619620116874\n",
      "epoch 340 | step 8 | loss: 0.0026578454174656105\n",
      "epoch 340 | step 9 | loss: 0.0029500890384656993\n",
      "epoch 340 | step 10 | loss: 0.0032420279754553985\n",
      "epoch 340 | step 11 | loss: 0.003507293379755902\n",
      "epoch 341 | step 0 | loss: 0.00029426340590699704\n",
      "epoch 341 | step 1 | loss: 0.0005893868095494705\n",
      "epoch 341 | step 2 | loss: 0.0008759596379489268\n",
      "epoch 341 | step 3 | loss: 0.0011690860390508424\n",
      "epoch 341 | step 4 | loss: 0.0014809952416021104\n",
      "epoch 341 | step 5 | loss: 0.0017808813961028083\n",
      "epoch 341 | step 6 | loss: 0.0020909480765494617\n",
      "epoch 341 | step 7 | loss: 0.002397388679360733\n",
      "epoch 341 | step 8 | loss: 0.002673705658788688\n",
      "epoch 341 | step 9 | loss: 0.002944705243241064\n",
      "epoch 341 | step 10 | loss: 0.0032341801380833617\n",
      "epoch 341 | step 11 | loss: 0.003510302051830496\n",
      "epoch 342 | step 0 | loss: 0.000295979241798326\n",
      "epoch 342 | step 1 | loss: 0.0005995485558796643\n",
      "epoch 342 | step 2 | loss: 0.0008938911385064148\n",
      "epoch 342 | step 3 | loss: 0.0012047776381018062\n",
      "epoch 342 | step 4 | loss: 0.0014773983203988559\n",
      "epoch 342 | step 5 | loss: 0.0017480944860670936\n",
      "epoch 342 | step 6 | loss: 0.002042141839990685\n",
      "epoch 342 | step 7 | loss: 0.002346733971316927\n",
      "epoch 342 | step 8 | loss: 0.0026593005768348697\n",
      "epoch 342 | step 9 | loss: 0.002918286567955738\n",
      "epoch 342 | step 10 | loss: 0.003223233009821899\n",
      "epoch 342 | step 11 | loss: 0.0035145799868397053\n",
      "epoch 343 | step 0 | loss: 0.0002661395233682434\n",
      "epoch 343 | step 1 | loss: 0.0005779681369655489\n",
      "epoch 343 | step 2 | loss: 0.0008645171409375091\n",
      "epoch 343 | step 3 | loss: 0.0011361848120628308\n",
      "epoch 343 | step 4 | loss: 0.0014425095241838686\n",
      "epoch 343 | step 5 | loss: 0.001742954964251127\n",
      "epoch 343 | step 6 | loss: 0.0020495319596195174\n",
      "epoch 343 | step 7 | loss: 0.0023519408161200225\n",
      "epoch 343 | step 8 | loss: 0.0026615280998275176\n",
      "epoch 343 | step 9 | loss: 0.0029401895731017624\n",
      "epoch 343 | step 10 | loss: 0.0032260248783138193\n",
      "epoch 343 | step 11 | loss: 0.0035134641006465987\n",
      "epoch 344 | step 0 | loss: 0.0002825970058034003\n",
      "epoch 344 | step 1 | loss: 0.0005624172797221571\n",
      "epoch 344 | step 2 | loss: 0.0008490033982148541\n",
      "epoch 344 | step 3 | loss: 0.001172984156059611\n",
      "epoch 344 | step 4 | loss: 0.0014344152858011099\n",
      "epoch 344 | step 5 | loss: 0.0017316648127612788\n",
      "epoch 344 | step 6 | loss: 0.0020497432389273837\n",
      "epoch 344 | step 7 | loss: 0.002331387222813447\n",
      "epoch 344 | step 8 | loss: 0.0026306291713421815\n",
      "epoch 344 | step 9 | loss: 0.0029381643427235167\n",
      "epoch 344 | step 10 | loss: 0.003233617969811022\n",
      "epoch 344 | step 11 | loss: 0.0035102567363226914\n",
      "epoch 345 | step 0 | loss: 0.00029368265898919267\n",
      "epoch 345 | step 1 | loss: 0.0005746847936848222\n",
      "epoch 345 | step 2 | loss: 0.0008638771749186012\n",
      "epoch 345 | step 3 | loss: 0.0011737060824829793\n",
      "epoch 345 | step 4 | loss: 0.0014840748398562253\n",
      "epoch 345 | step 5 | loss: 0.0017971411004241698\n",
      "epoch 345 | step 6 | loss: 0.0020817430751226313\n",
      "epoch 345 | step 7 | loss: 0.0023582054364959925\n",
      "epoch 345 | step 8 | loss: 0.0026684474633424955\n",
      "epoch 345 | step 9 | loss: 0.0029453400694223423\n",
      "epoch 345 | step 10 | loss: 0.0032145765293590976\n",
      "epoch 345 | step 11 | loss: 0.0035175689987447605\n",
      "epoch 346 | step 0 | loss: 0.0002686501314863732\n",
      "epoch 346 | step 1 | loss: 0.0005857340866713001\n",
      "epoch 346 | step 2 | loss: 0.0008809777793509584\n",
      "epoch 346 | step 3 | loss: 0.0011603422813107015\n",
      "epoch 346 | step 4 | loss: 0.0014605744182695365\n",
      "epoch 346 | step 5 | loss: 0.001753351649962451\n",
      "epoch 346 | step 6 | loss: 0.002040935314817491\n",
      "epoch 346 | step 7 | loss: 0.002337537779912206\n",
      "epoch 346 | step 8 | loss: 0.0026194312712926474\n",
      "epoch 346 | step 9 | loss: 0.002917934734920255\n",
      "epoch 346 | step 10 | loss: 0.0032049775924199502\n",
      "epoch 346 | step 11 | loss: 0.003521314235060766\n",
      "epoch 347 | step 0 | loss: 0.0002997482169928657\n",
      "epoch 347 | step 1 | loss: 0.0005840289311875071\n",
      "epoch 347 | step 2 | loss: 0.0008856951232143877\n",
      "epoch 347 | step 3 | loss: 0.0011676890948017155\n",
      "epoch 347 | step 4 | loss: 0.0014632769754556103\n",
      "epoch 347 | step 5 | loss: 0.001762400699842515\n",
      "epoch 347 | step 6 | loss: 0.002055999579789531\n",
      "epoch 347 | step 7 | loss: 0.00235999290968925\n",
      "epoch 347 | step 8 | loss: 0.002671666880405341\n",
      "epoch 347 | step 9 | loss: 0.002968857294247986\n",
      "epoch 347 | step 10 | loss: 0.00323426471401363\n",
      "epoch 347 | step 11 | loss: 0.0035099758892652745\n",
      "epoch 348 | step 0 | loss: 0.0003012531433186797\n",
      "epoch 348 | step 1 | loss: 0.0005729239066196632\n",
      "epoch 348 | step 2 | loss: 0.0008521263589992851\n",
      "epoch 348 | step 3 | loss: 0.001166040911660383\n",
      "epoch 348 | step 4 | loss: 0.001433381149499601\n",
      "epoch 348 | step 5 | loss: 0.0017514090827948493\n",
      "epoch 348 | step 6 | loss: 0.002052426665631601\n",
      "epoch 348 | step 7 | loss: 0.0023618660781807767\n",
      "epoch 348 | step 8 | loss: 0.0026682837518323785\n",
      "epoch 348 | step 9 | loss: 0.0029495465638093776\n",
      "epoch 348 | step 10 | loss: 0.0031997197456680912\n",
      "epoch 348 | step 11 | loss: 0.0035232338730217215\n",
      "epoch 349 | step 0 | loss: 0.00030369321615933013\n",
      "epoch 349 | step 1 | loss: 0.000565435789984555\n",
      "epoch 349 | step 2 | loss: 0.0008496412109772634\n",
      "epoch 349 | step 3 | loss: 0.0011483709415539812\n",
      "epoch 349 | step 4 | loss: 0.0014452063621557716\n",
      "epoch 349 | step 5 | loss: 0.0017401627853786863\n",
      "epoch 349 | step 6 | loss: 0.002021197061349555\n",
      "epoch 349 | step 7 | loss: 0.002335150868580961\n",
      "epoch 349 | step 8 | loss: 0.002639697596005404\n",
      "epoch 349 | step 9 | loss: 0.00293352367341302\n",
      "epoch 349 | step 10 | loss: 0.003238273552102113\n",
      "epoch 349 | step 11 | loss: 0.0035082575676022238\n",
      "epoch 350 | step 0 | loss: 0.00026622060857548065\n",
      "epoch 350 | step 1 | loss: 0.0005578695872784729\n",
      "epoch 350 | step 2 | loss: 0.0008418984311232067\n",
      "epoch 350 | step 3 | loss: 0.0011430913288230766\n",
      "epoch 350 | step 4 | loss: 0.0014494305600091476\n",
      "epoch 350 | step 5 | loss: 0.001764359399422257\n",
      "epoch 350 | step 6 | loss: 0.0020640977403994358\n",
      "epoch 350 | step 7 | loss: 0.002349537027123944\n",
      "epoch 350 | step 8 | loss: 0.002643824235168529\n",
      "epoch 350 | step 9 | loss: 0.002933249203517766\n",
      "epoch 350 | step 10 | loss: 0.003220493697366946\n",
      "epoch 350 | step 11 | loss: 0.0035151085809591266\n",
      "epoch 351 | step 0 | loss: 0.00030469452375067683\n",
      "epoch 351 | step 1 | loss: 0.000601479992019295\n",
      "epoch 351 | step 2 | loss: 0.0008965467998343173\n",
      "epoch 351 | step 3 | loss: 0.0011982577368017551\n",
      "epoch 351 | step 4 | loss: 0.0014824414946519179\n",
      "epoch 351 | step 5 | loss: 0.0017785507478648445\n",
      "epoch 351 | step 6 | loss: 0.002060241428675405\n",
      "epoch 351 | step 7 | loss: 0.002361605636799451\n",
      "epoch 351 | step 8 | loss: 0.0026427811491124336\n",
      "epoch 351 | step 9 | loss: 0.0029230096368278256\n",
      "epoch 351 | step 10 | loss: 0.003217476552791307\n",
      "epoch 351 | step 11 | loss: 0.0035162142623667314\n",
      "epoch 352 | step 0 | loss: 0.00027789869020378676\n",
      "epoch 352 | step 1 | loss: 0.000599931529930001\n",
      "epoch 352 | step 2 | loss: 0.0008583931135858275\n",
      "epoch 352 | step 3 | loss: 0.001148844048193697\n",
      "epoch 352 | step 4 | loss: 0.0014587121623222553\n",
      "epoch 352 | step 5 | loss: 0.0017726095447640898\n",
      "epoch 352 | step 6 | loss: 0.002072446348870305\n",
      "epoch 352 | step 7 | loss: 0.002345228390472727\n",
      "epoch 352 | step 8 | loss: 0.00263677040782547\n",
      "epoch 352 | step 9 | loss: 0.002922893509367276\n",
      "epoch 352 | step 10 | loss: 0.0032245559107982637\n",
      "epoch 352 | step 11 | loss: 0.003513354632314166\n",
      "epoch 353 | step 0 | loss: 0.0002915482196374312\n",
      "epoch 353 | step 1 | loss: 0.0005592106335897595\n",
      "epoch 353 | step 2 | loss: 0.0008684234007555061\n",
      "epoch 353 | step 3 | loss: 0.0011468422642837658\n",
      "epoch 353 | step 4 | loss: 0.001472064488928148\n",
      "epoch 353 | step 5 | loss: 0.0017854772394530114\n",
      "epoch 353 | step 6 | loss: 0.002053886861997753\n",
      "epoch 353 | step 7 | loss: 0.0023703195069231255\n",
      "epoch 353 | step 8 | loss: 0.002631541350328632\n",
      "epoch 353 | step 9 | loss: 0.002923706352450926\n",
      "epoch 353 | step 10 | loss: 0.003211871793124217\n",
      "epoch 353 | step 11 | loss: 0.0035184125753478686\n",
      "epoch 354 | step 0 | loss: 0.000288574056093332\n",
      "epoch 354 | step 1 | loss: 0.000586309525148419\n",
      "epoch 354 | step 2 | loss: 0.0008832816212239174\n",
      "epoch 354 | step 3 | loss: 0.001159740088562328\n",
      "epoch 354 | step 4 | loss: 0.0014881819917438753\n",
      "epoch 354 | step 5 | loss: 0.0017763719565298472\n",
      "epoch 354 | step 6 | loss: 0.0020559368352331127\n",
      "epoch 354 | step 7 | loss: 0.002369031147554107\n",
      "epoch 354 | step 8 | loss: 0.0026702887160307553\n",
      "epoch 354 | step 9 | loss: 0.002944056635027164\n",
      "epoch 354 | step 10 | loss: 0.0032309085262912708\n",
      "epoch 354 | step 11 | loss: 0.0035107266412778915\n",
      "epoch 355 | step 0 | loss: 0.00028245008654637473\n",
      "epoch 355 | step 1 | loss: 0.0005980309448279607\n",
      "epoch 355 | step 2 | loss: 0.000883566882429954\n",
      "epoch 355 | step 3 | loss: 0.0011884314476330756\n",
      "epoch 355 | step 4 | loss: 0.00147421473418286\n",
      "epoch 355 | step 5 | loss: 0.0017686003521967682\n",
      "epoch 355 | step 6 | loss: 0.0020547532161556245\n",
      "epoch 355 | step 7 | loss: 0.0023809713319889497\n",
      "epoch 355 | step 8 | loss: 0.002677221259793318\n",
      "epoch 355 | step 9 | loss: 0.002942340582525376\n",
      "epoch 355 | step 10 | loss: 0.0032286941505494924\n",
      "epoch 355 | step 11 | loss: 0.0035117324598620708\n",
      "epoch 356 | step 0 | loss: 0.0002873079321264317\n",
      "epoch 356 | step 1 | loss: 0.0006038902619231922\n",
      "epoch 356 | step 2 | loss: 0.0008920787845922033\n",
      "epoch 356 | step 3 | loss: 0.0011687029045417579\n",
      "epoch 356 | step 4 | loss: 0.0014906806551846356\n",
      "epoch 356 | step 5 | loss: 0.0017871449725615292\n",
      "epoch 356 | step 6 | loss: 0.0020560853429885486\n",
      "epoch 356 | step 7 | loss: 0.0023501623814286307\n",
      "epoch 356 | step 8 | loss: 0.002640151423162274\n",
      "epoch 356 | step 9 | loss: 0.002914358259466621\n",
      "epoch 356 | step 10 | loss: 0.0032049061827407006\n",
      "epoch 356 | step 11 | loss: 0.003520843331889602\n",
      "epoch 357 | step 0 | loss: 0.00030168854851003713\n",
      "epoch 357 | step 1 | loss: 0.0006087346317804603\n",
      "epoch 357 | step 2 | loss: 0.000904393958589212\n",
      "epoch 357 | step 3 | loss: 0.0011827355529835395\n",
      "epoch 357 | step 4 | loss: 0.0014698284491471835\n",
      "epoch 357 | step 5 | loss: 0.001746132857469971\n",
      "epoch 357 | step 6 | loss: 0.0020428085216682977\n",
      "epoch 357 | step 7 | loss: 0.0023171146755296885\n",
      "epoch 357 | step 8 | loss: 0.0026573249600295597\n",
      "epoch 357 | step 9 | loss: 0.002948622744009177\n",
      "epoch 357 | step 10 | loss: 0.003214867948837874\n",
      "epoch 357 | step 11 | loss: 0.00351698642509867\n",
      "epoch 358 | step 0 | loss: 0.0002988900224377976\n",
      "epoch 358 | step 1 | loss: 0.0005777571555333999\n",
      "epoch 358 | step 2 | loss: 0.0008752416955635868\n",
      "epoch 358 | step 3 | loss: 0.0011497584932999316\n",
      "epoch 358 | step 4 | loss: 0.0014593730351207725\n",
      "epoch 358 | step 5 | loss: 0.0017652269882955994\n",
      "epoch 358 | step 6 | loss: 0.002044437292383235\n",
      "epoch 358 | step 7 | loss: 0.002347525881488995\n",
      "epoch 358 | step 8 | loss: 0.0026359974528214402\n",
      "epoch 358 | step 9 | loss: 0.0029356631921229302\n",
      "epoch 358 | step 10 | loss: 0.0032024454318834578\n",
      "epoch 358 | step 11 | loss: 0.003521772936197325\n",
      "epoch 359 | step 0 | loss: 0.00029138212127755156\n",
      "epoch 359 | step 1 | loss: 0.0005868562246896453\n",
      "epoch 359 | step 2 | loss: 0.0008643678551274088\n",
      "epoch 359 | step 3 | loss: 0.0011599176813714134\n",
      "epoch 359 | step 4 | loss: 0.001442158258562255\n",
      "epoch 359 | step 5 | loss: 0.0017489624933108756\n",
      "epoch 359 | step 6 | loss: 0.0020428732618861115\n",
      "epoch 359 | step 7 | loss: 0.002360445307987536\n",
      "epoch 359 | step 8 | loss: 0.0026572062903850342\n",
      "epoch 359 | step 9 | loss: 0.0029490062294817506\n",
      "epoch 359 | step 10 | loss: 0.00323020017864772\n",
      "epoch 359 | step 11 | loss: 0.0035109785603967937\n",
      "epoch 360 | step 0 | loss: 0.00030578971080910114\n",
      "epoch 360 | step 1 | loss: 0.0006108763865878475\n",
      "epoch 360 | step 2 | loss: 0.0009038448389920815\n",
      "epoch 360 | step 3 | loss: 0.0012048035388285412\n",
      "epoch 360 | step 4 | loss: 0.0014909735253029028\n",
      "epoch 360 | step 5 | loss: 0.0017856360781311753\n",
      "epoch 360 | step 6 | loss: 0.002046272118690123\n",
      "epoch 360 | step 7 | loss: 0.0023671889756622195\n",
      "epoch 360 | step 8 | loss: 0.002632693542701813\n",
      "epoch 360 | step 9 | loss: 0.0029470248841737567\n",
      "epoch 360 | step 10 | loss: 0.0032450448972534637\n",
      "epoch 360 | step 11 | loss: 0.003504813208199211\n",
      "epoch 361 | step 0 | loss: 0.0003128667931524175\n",
      "epoch 361 | step 1 | loss: 0.0006276854139772155\n",
      "epoch 361 | step 2 | loss: 0.0009341821279423082\n",
      "epoch 361 | step 3 | loss: 0.0012104386175620452\n",
      "epoch 361 | step 4 | loss: 0.001494759023186149\n",
      "epoch 361 | step 5 | loss: 0.0017886544831325142\n",
      "epoch 361 | step 6 | loss: 0.0020782266860167\n",
      "epoch 361 | step 7 | loss: 0.0023470062619623524\n",
      "epoch 361 | step 8 | loss: 0.0026203136985287192\n",
      "epoch 361 | step 9 | loss: 0.0029136259891985486\n",
      "epoch 361 | step 10 | loss: 0.0032186313781776476\n",
      "epoch 361 | step 11 | loss: 0.003515092971118382\n",
      "epoch 362 | step 0 | loss: 0.0002803690832297109\n",
      "epoch 362 | step 1 | loss: 0.0005845696761896703\n",
      "epoch 362 | step 2 | loss: 0.0008607623702339877\n",
      "epoch 362 | step 3 | loss: 0.001150479632748082\n",
      "epoch 362 | step 4 | loss: 0.0014422008873743997\n",
      "epoch 362 | step 5 | loss: 0.0017216771857547434\n",
      "epoch 362 | step 6 | loss: 0.002021950754792162\n",
      "epoch 362 | step 7 | loss: 0.0023137343888216077\n",
      "epoch 362 | step 8 | loss: 0.002603553114482069\n",
      "epoch 362 | step 9 | loss: 0.0029413378827973435\n",
      "epoch 362 | step 10 | loss: 0.003240047776137193\n",
      "epoch 362 | step 11 | loss: 0.0035067094568547847\n",
      "epoch 363 | step 0 | loss: 0.0003040482864570649\n",
      "epoch 363 | step 1 | loss: 0.0006147315139255446\n",
      "epoch 363 | step 2 | loss: 0.0008801590970937629\n",
      "epoch 363 | step 3 | loss: 0.001181195754979551\n",
      "epoch 363 | step 4 | loss: 0.0014390319254938275\n",
      "epoch 363 | step 5 | loss: 0.0016841705508446267\n",
      "epoch 363 | step 6 | loss: 0.002031020760585799\n",
      "epoch 363 | step 7 | loss: 0.002331115522768973\n",
      "epoch 363 | step 8 | loss: 0.0026186884216857966\n",
      "epoch 363 | step 9 | loss: 0.0029126992544752915\n",
      "epoch 363 | step 10 | loss: 0.0032185883407577394\n",
      "epoch 363 | step 11 | loss: 0.003515160052226117\n",
      "epoch 364 | step 0 | loss: 0.0003113308478812866\n",
      "epoch 364 | step 1 | loss: 0.0006125616619776291\n",
      "epoch 364 | step 2 | loss: 0.0008786854400790993\n",
      "epoch 364 | step 3 | loss: 0.001175058042604481\n",
      "epoch 364 | step 4 | loss: 0.0014486022186726345\n",
      "epoch 364 | step 5 | loss: 0.0017628077433139745\n",
      "epoch 364 | step 6 | loss: 0.0020772500041977156\n",
      "epoch 364 | step 7 | loss: 0.0023418311913687667\n",
      "epoch 364 | step 8 | loss: 0.0026441925157829394\n",
      "epoch 364 | step 9 | loss: 0.0029507077969952366\n",
      "epoch 364 | step 10 | loss: 0.003233263995706373\n",
      "epoch 364 | step 11 | loss: 0.0035091630868304045\n",
      "epoch 365 | step 0 | loss: 0.00028211105323532775\n",
      "epoch 365 | step 1 | loss: 0.0005702630118942867\n",
      "epoch 365 | step 2 | loss: 0.0008715857885303999\n",
      "epoch 365 | step 3 | loss: 0.0011537664523977825\n",
      "epoch 365 | step 4 | loss: 0.001440068786670526\n",
      "epoch 365 | step 5 | loss: 0.0017288921576708266\n",
      "epoch 365 | step 6 | loss: 0.0020413752407746647\n",
      "epoch 365 | step 7 | loss: 0.0023351898998402684\n",
      "epoch 365 | step 8 | loss: 0.0026286474936026974\n",
      "epoch 365 | step 9 | loss: 0.0028918919536115007\n",
      "epoch 365 | step 10 | loss: 0.0032019422432488427\n",
      "epoch 365 | step 11 | loss: 0.0035214451116332202\n",
      "epoch 366 | step 0 | loss: 0.00027962357914187666\n",
      "epoch 366 | step 1 | loss: 0.0005864701642911729\n",
      "epoch 366 | step 2 | loss: 0.0008640761535258312\n",
      "epoch 366 | step 3 | loss: 0.0011762635377595304\n",
      "epoch 366 | step 4 | loss: 0.0014873704187825007\n",
      "epoch 366 | step 5 | loss: 0.0017607671837018967\n",
      "epoch 366 | step 6 | loss: 0.0020521394292375944\n",
      "epoch 366 | step 7 | loss: 0.0023298279234044727\n",
      "epoch 366 | step 8 | loss: 0.0026329361544131335\n",
      "epoch 366 | step 9 | loss: 0.002944914450063521\n",
      "epoch 366 | step 10 | loss: 0.0032363026859534073\n",
      "epoch 366 | step 11 | loss: 0.0035080105669405394\n",
      "epoch 367 | step 0 | loss: 0.0002791419777070159\n",
      "epoch 367 | step 1 | loss: 0.0005926016535315297\n",
      "epoch 367 | step 2 | loss: 0.000886558316165904\n",
      "epoch 367 | step 3 | loss: 0.0011752571081082256\n",
      "epoch 367 | step 4 | loss: 0.001449359178931226\n",
      "epoch 367 | step 5 | loss: 0.0017301483153176298\n",
      "epoch 367 | step 6 | loss: 0.0020382180605438113\n",
      "epoch 367 | step 7 | loss: 0.0023229536208994573\n",
      "epoch 367 | step 8 | loss: 0.0026009365721525587\n",
      "epoch 367 | step 9 | loss: 0.0029085298523057776\n",
      "epoch 367 | step 10 | loss: 0.0032026379059223375\n",
      "epoch 367 | step 11 | loss: 0.0035209484786493444\n",
      "epoch 368 | step 0 | loss: 0.00028622259625811307\n",
      "epoch 368 | step 1 | loss: 0.000576808452352457\n",
      "epoch 368 | step 2 | loss: 0.0009001291516553062\n",
      "epoch 368 | step 3 | loss: 0.0011524586948522236\n",
      "epoch 368 | step 4 | loss: 0.001436325892430132\n",
      "epoch 368 | step 5 | loss: 0.0017191460094583791\n",
      "epoch 368 | step 6 | loss: 0.001996725206750797\n",
      "epoch 368 | step 7 | loss: 0.002287010793983067\n",
      "epoch 368 | step 8 | loss: 0.002593811858472763\n",
      "epoch 368 | step 9 | loss: 0.0029020305022474354\n",
      "epoch 368 | step 10 | loss: 0.0032180924734194066\n",
      "epoch 368 | step 11 | loss: 0.003514914693943319\n",
      "epoch 369 | step 0 | loss: 0.00029414439744917387\n",
      "epoch 369 | step 1 | loss: 0.0005806049394282281\n",
      "epoch 369 | step 2 | loss: 0.0008671731903435061\n",
      "epoch 369 | step 3 | loss: 0.0011700921404651054\n",
      "epoch 369 | step 4 | loss: 0.0014592007763132107\n",
      "epoch 369 | step 5 | loss: 0.001744580349535612\n",
      "epoch 369 | step 6 | loss: 0.0020411771040970846\n",
      "epoch 369 | step 7 | loss: 0.002338658391440621\n",
      "epoch 369 | step 8 | loss: 0.002638413317452634\n",
      "epoch 369 | step 9 | loss: 0.0029275233704375457\n",
      "epoch 369 | step 10 | loss: 0.0032319603907988185\n",
      "epoch 369 | step 11 | loss: 0.0035093405236860916\n",
      "epoch 370 | step 0 | loss: 0.0002932449709422359\n",
      "epoch 370 | step 1 | loss: 0.0005918131902734203\n",
      "epoch 370 | step 2 | loss: 0.0009138272953794619\n",
      "epoch 370 | step 3 | loss: 0.0011986868096016525\n",
      "epoch 370 | step 4 | loss: 0.001502928269907206\n",
      "epoch 370 | step 5 | loss: 0.001804209509345287\n",
      "epoch 370 | step 6 | loss: 0.0020828699235712644\n",
      "epoch 370 | step 7 | loss: 0.0023652851675143175\n",
      "epoch 370 | step 8 | loss: 0.0026696177734448793\n",
      "epoch 370 | step 9 | loss: 0.0029624502211277855\n",
      "epoch 370 | step 10 | loss: 0.003241603519958878\n",
      "epoch 370 | step 11 | loss: 0.003505613049345986\n",
      "epoch 371 | step 0 | loss: 0.00029520206215081287\n",
      "epoch 371 | step 1 | loss: 0.0005778731935260154\n",
      "epoch 371 | step 2 | loss: 0.000865147725303847\n",
      "epoch 371 | step 3 | loss: 0.001148364685271586\n",
      "epoch 371 | step 4 | loss: 0.0014604961391343906\n",
      "epoch 371 | step 5 | loss: 0.0017466251262623974\n",
      "epoch 371 | step 6 | loss: 0.0020595227163486066\n",
      "epoch 371 | step 7 | loss: 0.002344210748352102\n",
      "epoch 371 | step 8 | loss: 0.0026241910627880587\n",
      "epoch 371 | step 9 | loss: 0.0029414590020415956\n",
      "epoch 371 | step 10 | loss: 0.0032173749276151672\n",
      "epoch 371 | step 11 | loss: 0.0035150054772930804\n",
      "epoch 372 | step 0 | loss: 0.0003101325904689983\n",
      "epoch 372 | step 1 | loss: 0.0006108677897712642\n",
      "epoch 372 | step 2 | loss: 0.0009068487315206805\n",
      "epoch 372 | step 3 | loss: 0.0012297385450024197\n",
      "epoch 372 | step 4 | loss: 0.0015354864942565734\n",
      "epoch 372 | step 5 | loss: 0.0018135916549517322\n",
      "epoch 372 | step 6 | loss: 0.0020979152593615987\n",
      "epoch 372 | step 7 | loss: 0.002385231664415863\n",
      "epoch 372 | step 8 | loss: 0.0026554948868758735\n",
      "epoch 372 | step 9 | loss: 0.002944770577450943\n",
      "epoch 372 | step 10 | loss: 0.0032226218818887007\n",
      "epoch 372 | step 11 | loss: 0.0035128113083108476\n",
      "epoch 373 | step 0 | loss: 0.00030090568718347334\n",
      "epoch 373 | step 1 | loss: 0.0005921773376147555\n",
      "epoch 373 | step 2 | loss: 0.0008993518827340371\n",
      "epoch 373 | step 3 | loss: 0.001226224151425789\n",
      "epoch 373 | step 4 | loss: 0.001506372471501116\n",
      "epoch 373 | step 5 | loss: 0.0017860899638116382\n",
      "epoch 373 | step 6 | loss: 0.002086005684987003\n",
      "epoch 373 | step 7 | loss: 0.0023710668050294093\n",
      "epoch 373 | step 8 | loss: 0.0026381934169380696\n",
      "epoch 373 | step 9 | loss: 0.0029350442327494455\n",
      "epoch 373 | step 10 | loss: 0.003214383285586855\n",
      "epoch 373 | step 11 | loss: 0.003516077678825652\n",
      "epoch 374 | step 0 | loss: 0.00028719241959253836\n",
      "epoch 374 | step 1 | loss: 0.0005839761413707434\n",
      "epoch 374 | step 2 | loss: 0.0008697723090526132\n",
      "epoch 374 | step 3 | loss: 0.0011727386734897377\n",
      "epoch 374 | step 4 | loss: 0.0014665050828431359\n",
      "epoch 374 | step 5 | loss: 0.001741150835345059\n",
      "epoch 374 | step 6 | loss: 0.0020443121053406793\n",
      "epoch 374 | step 7 | loss: 0.0023326736204387654\n",
      "epoch 374 | step 8 | loss: 0.0026121250072327832\n",
      "epoch 374 | step 9 | loss: 0.0029034364410128015\n",
      "epoch 374 | step 10 | loss: 0.0032022598197768114\n",
      "epoch 374 | step 11 | loss: 0.0035206808575112326\n",
      "epoch 375 | step 0 | loss: 0.00030132547409724624\n",
      "epoch 375 | step 1 | loss: 0.000590892564354361\n",
      "epoch 375 | step 2 | loss: 0.0009034196763571588\n",
      "epoch 375 | step 3 | loss: 0.0011796451589038675\n",
      "epoch 375 | step 4 | loss: 0.0014779510723799585\n",
      "epoch 375 | step 5 | loss: 0.0017730439812380047\n",
      "epoch 375 | step 6 | loss: 0.0020485947041350458\n",
      "epoch 375 | step 7 | loss: 0.0023364067617000174\n",
      "epoch 375 | step 8 | loss: 0.0026316362839738545\n",
      "epoch 375 | step 9 | loss: 0.0029248277990892423\n",
      "epoch 375 | step 10 | loss: 0.003241577703285506\n",
      "epoch 375 | step 11 | loss: 0.0035051618179739647\n",
      "epoch 376 | step 0 | loss: 0.00028727049448687203\n",
      "epoch 376 | step 1 | loss: 0.0005867893128242603\n",
      "epoch 376 | step 2 | loss: 0.0009115979179500601\n",
      "epoch 376 | step 3 | loss: 0.00119394951007817\n",
      "epoch 376 | step 4 | loss: 0.0014666519389311898\n",
      "epoch 376 | step 5 | loss: 0.0018053821761536786\n",
      "epoch 376 | step 6 | loss: 0.0020649522763208747\n",
      "epoch 376 | step 7 | loss: 0.0023733144077104527\n",
      "epoch 376 | step 8 | loss: 0.0026905659840418053\n",
      "epoch 376 | step 9 | loss: 0.0029529095649638093\n",
      "epoch 376 | step 10 | loss: 0.0032368273898927525\n",
      "epoch 376 | step 11 | loss: 0.0035069456759871977\n",
      "epoch 377 | step 0 | loss: 0.0002904665506435662\n",
      "epoch 377 | step 1 | loss: 0.0005685533194427885\n",
      "epoch 377 | step 2 | loss: 0.0008823055098415777\n",
      "epoch 377 | step 3 | loss: 0.0011562894371463673\n",
      "epoch 377 | step 4 | loss: 0.0014451427817616215\n",
      "epoch 377 | step 5 | loss: 0.0017525031212690823\n",
      "epoch 377 | step 6 | loss: 0.0020286186883646555\n",
      "epoch 377 | step 7 | loss: 0.002338731567859247\n",
      "epoch 377 | step 8 | loss: 0.002628804037639215\n",
      "epoch 377 | step 9 | loss: 0.002934974683684511\n",
      "epoch 377 | step 10 | loss: 0.0032112332652961236\n",
      "epoch 377 | step 11 | loss: 0.0035171525953276465\n",
      "epoch 378 | step 0 | loss: 0.000299525667119801\n",
      "epoch 378 | step 1 | loss: 0.0005963490182227327\n",
      "epoch 378 | step 2 | loss: 0.0008813016298643825\n",
      "epoch 378 | step 3 | loss: 0.0011664908553755831\n",
      "epoch 378 | step 4 | loss: 0.001492766612715917\n",
      "epoch 378 | step 5 | loss: 0.0017627828893421811\n",
      "epoch 378 | step 6 | loss: 0.0020588469684909173\n",
      "epoch 378 | step 7 | loss: 0.0023658009506339374\n",
      "epoch 378 | step 8 | loss: 0.002657177829555821\n",
      "epoch 378 | step 9 | loss: 0.002938645637567292\n",
      "epoch 378 | step 10 | loss: 0.0032326290856381826\n",
      "epoch 378 | step 11 | loss: 0.0035087443589075624\n",
      "epoch 379 | step 0 | loss: 0.00026135113840367356\n",
      "epoch 379 | step 1 | loss: 0.0005617830347464001\n",
      "epoch 379 | step 2 | loss: 0.0008344393981112672\n",
      "epoch 379 | step 3 | loss: 0.0011355339797867192\n",
      "epoch 379 | step 4 | loss: 0.0014343828760214317\n",
      "epoch 379 | step 5 | loss: 0.001747722157337532\n",
      "epoch 379 | step 6 | loss: 0.0020439445405277734\n",
      "epoch 379 | step 7 | loss: 0.002335039076007412\n",
      "epoch 379 | step 8 | loss: 0.002618509191410295\n",
      "epoch 379 | step 9 | loss: 0.002912561463945814\n",
      "epoch 379 | step 10 | loss: 0.0032139433973840973\n",
      "epoch 379 | step 11 | loss: 0.0035159081560196524\n",
      "epoch 380 | step 0 | loss: 0.00028843518970961594\n",
      "epoch 380 | step 1 | loss: 0.000563966370603861\n",
      "epoch 380 | step 2 | loss: 0.0008292535019440737\n",
      "epoch 380 | step 3 | loss: 0.0011392761660641456\n",
      "epoch 380 | step 4 | loss: 0.0014443217525014628\n",
      "epoch 380 | step 5 | loss: 0.0017240907726598007\n",
      "epoch 380 | step 6 | loss: 0.002039827702573001\n",
      "epoch 380 | step 7 | loss: 0.0023481755820320394\n",
      "epoch 380 | step 8 | loss: 0.0026266443475232797\n",
      "epoch 380 | step 9 | loss: 0.0029319933114204263\n",
      "epoch 380 | step 10 | loss: 0.0032161977234566146\n",
      "epoch 380 | step 11 | loss: 0.0035150023140474588\n",
      "epoch 381 | step 0 | loss: 0.00031117455987401304\n",
      "epoch 381 | step 1 | loss: 0.0006456776300794262\n",
      "epoch 381 | step 2 | loss: 0.000934406204378677\n",
      "epoch 381 | step 3 | loss: 0.001217861837250235\n",
      "epoch 381 | step 4 | loss: 0.0014745401855393136\n",
      "epoch 381 | step 5 | loss: 0.001772293160678412\n",
      "epoch 381 | step 6 | loss: 0.0020478034737348603\n",
      "epoch 381 | step 7 | loss: 0.0023435231456296252\n",
      "epoch 381 | step 8 | loss: 0.0026359303782008143\n",
      "epoch 381 | step 9 | loss: 0.002924108612565545\n",
      "epoch 381 | step 10 | loss: 0.0032227388301286656\n",
      "epoch 381 | step 11 | loss: 0.0035123332529775333\n",
      "epoch 382 | step 0 | loss: 0.0003185511791591324\n",
      "epoch 382 | step 1 | loss: 0.0006043508154091601\n",
      "epoch 382 | step 2 | loss: 0.0009105473755105389\n",
      "epoch 382 | step 3 | loss: 0.0011800391165055636\n",
      "epoch 382 | step 4 | loss: 0.001482015963869988\n",
      "epoch 382 | step 5 | loss: 0.001751266963807262\n",
      "epoch 382 | step 6 | loss: 0.0020603758100317497\n",
      "epoch 382 | step 7 | loss: 0.0023671434130229504\n",
      "epoch 382 | step 8 | loss: 0.0026621054095322576\n",
      "epoch 382 | step 9 | loss: 0.0029479053802329075\n",
      "epoch 382 | step 10 | loss: 0.003234239783491294\n",
      "epoch 382 | step 11 | loss: 0.0035075467527160486\n",
      "epoch 383 | step 0 | loss: 0.0003165143343967283\n",
      "epoch 383 | step 1 | loss: 0.000586609146809627\n",
      "epoch 383 | step 2 | loss: 0.0008972325548813867\n",
      "epoch 383 | step 3 | loss: 0.001181771346162173\n",
      "epoch 383 | step 4 | loss: 0.0014678218033264486\n",
      "epoch 383 | step 5 | loss: 0.0017545175737049327\n",
      "epoch 383 | step 6 | loss: 0.00205554697972721\n",
      "epoch 383 | step 7 | loss: 0.0023484233618310817\n",
      "epoch 383 | step 8 | loss: 0.0026116361844717127\n",
      "epoch 383 | step 9 | loss: 0.0029123670032518622\n",
      "epoch 383 | step 10 | loss: 0.003203685996682322\n",
      "epoch 383 | step 11 | loss: 0.0035196566129795117\n",
      "epoch 384 | step 0 | loss: 0.00027694373657988756\n",
      "epoch 384 | step 1 | loss: 0.0005555024687928142\n",
      "epoch 384 | step 2 | loss: 0.00085411715262012\n",
      "epoch 384 | step 3 | loss: 0.0011423513314921079\n",
      "epoch 384 | step 4 | loss: 0.0014276223841270543\n",
      "epoch 384 | step 5 | loss: 0.0017276298437607558\n",
      "epoch 384 | step 6 | loss: 0.002050721435337005\n",
      "epoch 384 | step 7 | loss: 0.002354590190665389\n",
      "epoch 384 | step 8 | loss: 0.002632439646654447\n",
      "epoch 384 | step 9 | loss: 0.002917300380445363\n",
      "epoch 384 | step 10 | loss: 0.00320156037244093\n",
      "epoch 384 | step 11 | loss: 0.003520334746967961\n",
      "epoch 385 | step 0 | loss: 0.0002717504547775205\n",
      "epoch 385 | step 1 | loss: 0.0005687717031541433\n",
      "epoch 385 | step 2 | loss: 0.0008814319161405324\n",
      "epoch 385 | step 3 | loss: 0.0011804500537415856\n",
      "epoch 385 | step 4 | loss: 0.0014774390694034206\n",
      "epoch 385 | step 5 | loss: 0.0017498481039644523\n",
      "epoch 385 | step 6 | loss: 0.002044418591918438\n",
      "epoch 385 | step 7 | loss: 0.0023603549371304915\n",
      "epoch 385 | step 8 | loss: 0.0026295846944131176\n",
      "epoch 385 | step 9 | loss: 0.002918373286107938\n",
      "epoch 385 | step 10 | loss: 0.003225179556697573\n",
      "epoch 385 | step 11 | loss: 0.003511031400188562\n",
      "epoch 386 | step 0 | loss: 0.0003064823831051413\n",
      "epoch 386 | step 1 | loss: 0.0006295581041204505\n",
      "epoch 386 | step 2 | loss: 0.0009389187662681226\n",
      "epoch 386 | step 3 | loss: 0.001270838825390155\n",
      "epoch 386 | step 4 | loss: 0.0015237327328425478\n",
      "epoch 386 | step 5 | loss: 0.0018037366296156667\n",
      "epoch 386 | step 6 | loss: 0.002085763172178695\n",
      "epoch 386 | step 7 | loss: 0.0023758048298157925\n",
      "epoch 386 | step 8 | loss: 0.0026647275382222627\n",
      "epoch 386 | step 9 | loss: 0.002947003700508744\n",
      "epoch 386 | step 10 | loss: 0.0032254483143231384\n",
      "epoch 386 | step 11 | loss: 0.0035108232230537375\n",
      "epoch 387 | step 0 | loss: 0.00029765421026666883\n",
      "epoch 387 | step 1 | loss: 0.0005760456213356047\n",
      "epoch 387 | step 2 | loss: 0.0008877469802023631\n",
      "epoch 387 | step 3 | loss: 0.0011602858364280072\n",
      "epoch 387 | step 4 | loss: 0.0014434743025183879\n",
      "epoch 387 | step 5 | loss: 0.0017272981552791003\n",
      "epoch 387 | step 6 | loss: 0.002049469146676074\n",
      "epoch 387 | step 7 | loss: 0.0023276601233800315\n",
      "epoch 387 | step 8 | loss: 0.0026098214495014612\n",
      "epoch 387 | step 9 | loss: 0.0029046564409504058\n",
      "epoch 387 | step 10 | loss: 0.003213358591033032\n",
      "epoch 387 | step 11 | loss: 0.00351565257911704\n",
      "epoch 388 | step 0 | loss: 0.0002849499709843701\n",
      "epoch 388 | step 1 | loss: 0.0005779656232430131\n",
      "epoch 388 | step 2 | loss: 0.0008806257261320477\n",
      "epoch 388 | step 3 | loss: 0.0011609563811476804\n",
      "epoch 388 | step 4 | loss: 0.0014443158117262516\n",
      "epoch 388 | step 5 | loss: 0.0017563501218497066\n",
      "epoch 388 | step 6 | loss: 0.002040147075620982\n",
      "epoch 388 | step 7 | loss: 0.002313916788286936\n",
      "epoch 388 | step 8 | loss: 0.002605285663994498\n",
      "epoch 388 | step 9 | loss: 0.0029073656885049252\n",
      "epoch 388 | step 10 | loss: 0.003210102572581868\n",
      "epoch 388 | step 11 | loss: 0.0035167837007758587\n",
      "epoch 389 | step 0 | loss: 0.00029194940860410845\n",
      "epoch 389 | step 1 | loss: 0.0005553099158661812\n",
      "epoch 389 | step 2 | loss: 0.0008783333634157789\n",
      "epoch 389 | step 3 | loss: 0.0011562806788719556\n",
      "epoch 389 | step 4 | loss: 0.0014601020444278215\n",
      "epoch 389 | step 5 | loss: 0.0017497983099643909\n",
      "epoch 389 | step 6 | loss: 0.002054567300624554\n",
      "epoch 389 | step 7 | loss: 0.002330076819189106\n",
      "epoch 389 | step 8 | loss: 0.00266461996815155\n",
      "epoch 389 | step 9 | loss: 0.002950633958789753\n",
      "epoch 389 | step 10 | loss: 0.003235755165756883\n",
      "epoch 389 | step 11 | loss: 0.0035066053739311715\n",
      "epoch 390 | step 0 | loss: 0.0003137656290610756\n",
      "epoch 390 | step 1 | loss: 0.0005850379751741924\n",
      "epoch 390 | step 2 | loss: 0.0008813735664018659\n",
      "epoch 390 | step 3 | loss: 0.0011578712860714287\n",
      "epoch 390 | step 4 | loss: 0.0014703957950272268\n",
      "epoch 390 | step 5 | loss: 0.0017662553690370078\n",
      "epoch 390 | step 6 | loss: 0.0020375099252441836\n",
      "epoch 390 | step 7 | loss: 0.002332442797242636\n",
      "epoch 390 | step 8 | loss: 0.0026061572922394935\n",
      "epoch 390 | step 9 | loss: 0.0028846485589637386\n",
      "epoch 390 | step 10 | loss: 0.003214129791301341\n",
      "epoch 390 | step 11 | loss: 0.003515116426984457\n",
      "epoch 391 | step 0 | loss: 0.0003117382914432691\n",
      "epoch 391 | step 1 | loss: 0.0006028762743092664\n",
      "epoch 391 | step 2 | loss: 0.0008819878645233736\n",
      "epoch 391 | step 3 | loss: 0.0011531958572191959\n",
      "epoch 391 | step 4 | loss: 0.00146784657031256\n",
      "epoch 391 | step 5 | loss: 0.001779000861883187\n",
      "epoch 391 | step 6 | loss: 0.00206098186960872\n",
      "epoch 391 | step 7 | loss: 0.0023496284049230516\n",
      "epoch 391 | step 8 | loss: 0.0026235459307006196\n",
      "epoch 391 | step 9 | loss: 0.0029088695508020144\n",
      "epoch 391 | step 10 | loss: 0.003190494578500629\n",
      "epoch 391 | step 11 | loss: 0.0035241563526349725\n",
      "epoch 392 | step 0 | loss: 0.0002781373646516546\n",
      "epoch 392 | step 1 | loss: 0.0005493104481629109\n",
      "epoch 392 | step 2 | loss: 0.0008478014010005282\n",
      "epoch 392 | step 3 | loss: 0.0011396033922310897\n",
      "epoch 392 | step 4 | loss: 0.0014262850382826034\n",
      "epoch 392 | step 5 | loss: 0.001737702116232112\n",
      "epoch 392 | step 6 | loss: 0.0020278499846572632\n",
      "epoch 392 | step 7 | loss: 0.002322252580101272\n",
      "epoch 392 | step 8 | loss: 0.0026189373825895217\n",
      "epoch 392 | step 9 | loss: 0.0029080565387887883\n",
      "epoch 392 | step 10 | loss: 0.0032172210223083892\n",
      "epoch 392 | step 11 | loss: 0.003513591761435151\n",
      "epoch 393 | step 0 | loss: 0.00029679966126382\n",
      "epoch 393 | step 1 | loss: 0.0005868407462893785\n",
      "epoch 393 | step 2 | loss: 0.0008791689083017653\n",
      "epoch 393 | step 3 | loss: 0.0011684256492810905\n",
      "epoch 393 | step 4 | loss: 0.001482384815392941\n",
      "epoch 393 | step 5 | loss: 0.001802793521655426\n",
      "epoch 393 | step 6 | loss: 0.002070291705508786\n",
      "epoch 393 | step 7 | loss: 0.0023465776631931726\n",
      "epoch 393 | step 8 | loss: 0.002625591318052802\n",
      "epoch 393 | step 9 | loss: 0.002927954047129299\n",
      "epoch 393 | step 10 | loss: 0.0032232150179200025\n",
      "epoch 393 | step 11 | loss: 0.0035112562925485487\n",
      "epoch 394 | step 0 | loss: 0.0002835158968550552\n",
      "epoch 394 | step 1 | loss: 0.0005927120388293357\n",
      "epoch 394 | step 2 | loss: 0.0008934926631901701\n",
      "epoch 394 | step 3 | loss: 0.0011542207997479346\n",
      "epoch 394 | step 4 | loss: 0.0014791142763856283\n",
      "epoch 394 | step 5 | loss: 0.0017863448411851908\n",
      "epoch 394 | step 6 | loss: 0.002059915367200769\n",
      "epoch 394 | step 7 | loss: 0.0023190095498760934\n",
      "epoch 394 | step 8 | loss: 0.00260545568739611\n",
      "epoch 394 | step 9 | loss: 0.002893258729915919\n",
      "epoch 394 | step 10 | loss: 0.003204204693262185\n",
      "epoch 394 | step 11 | loss: 0.003518463272147708\n",
      "epoch 395 | step 0 | loss: 0.00027041168903855703\n",
      "epoch 395 | step 1 | loss: 0.0005967428556611457\n",
      "epoch 395 | step 2 | loss: 0.0009172762977521003\n",
      "epoch 395 | step 3 | loss: 0.0011893059381053958\n",
      "epoch 395 | step 4 | loss: 0.0014827023647157665\n",
      "epoch 395 | step 5 | loss: 0.001775156097271982\n",
      "epoch 395 | step 6 | loss: 0.002071659165036445\n",
      "epoch 395 | step 7 | loss: 0.0023803598240935736\n",
      "epoch 395 | step 8 | loss: 0.0026583858319296815\n",
      "epoch 395 | step 9 | loss: 0.002922452339467601\n",
      "epoch 395 | step 10 | loss: 0.0032099643976766347\n",
      "epoch 395 | step 11 | loss: 0.0035163845841848047\n",
      "epoch 396 | step 0 | loss: 0.0002696470992885125\n",
      "epoch 396 | step 1 | loss: 0.0005693928894582462\n",
      "epoch 396 | step 2 | loss: 0.0008804834354541925\n",
      "epoch 396 | step 3 | loss: 0.00118482835504314\n",
      "epoch 396 | step 4 | loss: 0.0014496763168074856\n",
      "epoch 396 | step 5 | loss: 0.0017293148347315157\n",
      "epoch 396 | step 6 | loss: 0.00200173564274043\n",
      "epoch 396 | step 7 | loss: 0.0022882814380712573\n",
      "epoch 396 | step 8 | loss: 0.0025971698607253346\n",
      "epoch 396 | step 9 | loss: 0.002920122150511962\n",
      "epoch 396 | step 10 | loss: 0.0032014667363342493\n",
      "epoch 396 | step 11 | loss: 0.0035197101209579014\n",
      "epoch 397 | step 0 | loss: 0.0002929201822695509\n",
      "epoch 397 | step 1 | loss: 0.0005685581509210842\n",
      "epoch 397 | step 2 | loss: 0.0008785378260265587\n",
      "epoch 397 | step 3 | loss: 0.0011651517941493688\n",
      "epoch 397 | step 4 | loss: 0.0014564171768469583\n",
      "epoch 397 | step 5 | loss: 0.0017293247021575764\n",
      "epoch 397 | step 6 | loss: 0.0020471285205239274\n",
      "epoch 397 | step 7 | loss: 0.0023246914442002766\n",
      "epoch 397 | step 8 | loss: 0.0026153018760703283\n",
      "epoch 397 | step 9 | loss: 0.002889688127389695\n",
      "epoch 397 | step 10 | loss: 0.003227219843461251\n",
      "epoch 397 | step 11 | loss: 0.0035093931804993086\n",
      "epoch 398 | step 0 | loss: 0.0003293147086521488\n",
      "epoch 398 | step 1 | loss: 0.0006256603395645045\n",
      "epoch 398 | step 2 | loss: 0.0009250658868667574\n",
      "epoch 398 | step 3 | loss: 0.0012112836648763149\n",
      "epoch 398 | step 4 | loss: 0.0015087754918918454\n",
      "epoch 398 | step 5 | loss: 0.0017814195421299007\n",
      "epoch 398 | step 6 | loss: 0.0020579565147155424\n",
      "epoch 398 | step 7 | loss: 0.002370834266290831\n",
      "epoch 398 | step 8 | loss: 0.002684363872126402\n",
      "epoch 398 | step 9 | loss: 0.0029445606250639673\n",
      "epoch 398 | step 10 | loss: 0.0032250323932193035\n",
      "epoch 398 | step 11 | loss: 0.003510314061479307\n",
      "epoch 399 | step 0 | loss: 0.0003124761791207745\n",
      "epoch 399 | step 1 | loss: 0.0006408234889992935\n",
      "epoch 399 | step 2 | loss: 0.0009140759874775997\n",
      "epoch 399 | step 3 | loss: 0.001198479918915582\n",
      "epoch 399 | step 4 | loss: 0.001493746479830603\n",
      "epoch 399 | step 5 | loss: 0.0017827333789215312\n",
      "epoch 399 | step 6 | loss: 0.0020668797996111843\n",
      "epoch 399 | step 7 | loss: 0.0023425288428783964\n",
      "epoch 399 | step 8 | loss: 0.0026128967184796564\n",
      "epoch 399 | step 9 | loss: 0.0029042172394073682\n",
      "epoch 399 | step 10 | loss: 0.0032113212203697212\n",
      "epoch 399 | step 11 | loss: 0.0035153947684619913\n",
      "epoch 400 | step 0 | loss: 0.0003076920725467372\n",
      "epoch 400 | step 1 | loss: 0.000588899275221876\n",
      "epoch 400 | step 2 | loss: 0.0009033010471760087\n",
      "epoch 400 | step 3 | loss: 0.001198450883096954\n",
      "epoch 400 | step 4 | loss: 0.0014870346133834895\n",
      "epoch 400 | step 5 | loss: 0.0017903085068206328\n",
      "epoch 400 | step 6 | loss: 0.0020777880076389026\n",
      "epoch 400 | step 7 | loss: 0.002364087204130839\n",
      "epoch 400 | step 8 | loss: 0.0026515002947808852\n",
      "epoch 400 | step 9 | loss: 0.0029279124688068674\n",
      "epoch 400 | step 10 | loss: 0.0032080071597849072\n",
      "epoch 400 | step 11 | loss: 0.0035167984716422337\n",
      "epoch 401 | step 0 | loss: 0.0002943993969771565\n",
      "epoch 401 | step 1 | loss: 0.0005955924671731509\n",
      "epoch 401 | step 2 | loss: 0.0008926324814177769\n",
      "epoch 401 | step 3 | loss: 0.0011759506337543782\n",
      "epoch 401 | step 4 | loss: 0.0014792602403650048\n",
      "epoch 401 | step 5 | loss: 0.0017610795149257826\n",
      "epoch 401 | step 6 | loss: 0.0020495725932325785\n",
      "epoch 401 | step 7 | loss: 0.002335277989777185\n",
      "epoch 401 | step 8 | loss: 0.0026311923776513825\n",
      "epoch 401 | step 9 | loss: 0.002929630823619217\n",
      "epoch 401 | step 10 | loss: 0.003228616806076694\n",
      "epoch 401 | step 11 | loss: 0.003508546234525723\n",
      "epoch 402 | step 0 | loss: 0.00030028804053269597\n",
      "epoch 402 | step 1 | loss: 0.0005997099585226508\n",
      "epoch 402 | step 2 | loss: 0.0008779868784871084\n",
      "epoch 402 | step 3 | loss: 0.001178351713616735\n",
      "epoch 402 | step 4 | loss: 0.0014692905531764682\n",
      "epoch 402 | step 5 | loss: 0.0017554787858178663\n",
      "epoch 402 | step 6 | loss: 0.0020492556835024767\n",
      "epoch 402 | step 7 | loss: 0.0023470232147497746\n",
      "epoch 402 | step 8 | loss: 0.002641106491365018\n",
      "epoch 402 | step 9 | loss: 0.002916435667111556\n",
      "epoch 402 | step 10 | loss: 0.003218851305108869\n",
      "epoch 402 | step 11 | loss: 0.0035123101076575763\n",
      "epoch 403 | step 0 | loss: 0.00028850946963620715\n",
      "epoch 403 | step 1 | loss: 0.0005643693825808023\n",
      "epoch 403 | step 2 | loss: 0.0008526794410451812\n",
      "epoch 403 | step 3 | loss: 0.001126890815545419\n",
      "epoch 403 | step 4 | loss: 0.0014109675010899785\n",
      "epoch 403 | step 5 | loss: 0.0017051594632714663\n",
      "epoch 403 | step 6 | loss: 0.0020138496084029533\n",
      "epoch 403 | step 7 | loss: 0.0023124786550030602\n",
      "epoch 403 | step 8 | loss: 0.002635312593692587\n",
      "epoch 403 | step 9 | loss: 0.0029020391865470195\n",
      "epoch 403 | step 10 | loss: 0.0032185125112145067\n",
      "epoch 403 | step 11 | loss: 0.0035123256591463635\n",
      "epoch 404 | step 0 | loss: 0.00031848050070262997\n",
      "epoch 404 | step 1 | loss: 0.000623007282962815\n",
      "epoch 404 | step 2 | loss: 0.0009331972005270589\n",
      "epoch 404 | step 3 | loss: 0.0012487783372245904\n",
      "epoch 404 | step 4 | loss: 0.0015617029263280725\n",
      "epoch 404 | step 5 | loss: 0.0018214246259572406\n",
      "epoch 404 | step 6 | loss: 0.002095166470835983\n",
      "epoch 404 | step 7 | loss: 0.0023750960564328833\n",
      "epoch 404 | step 8 | loss: 0.0026668564876074614\n",
      "epoch 404 | step 9 | loss: 0.0029478838359268066\n",
      "epoch 404 | step 10 | loss: 0.0032077896048015274\n",
      "epoch 404 | step 11 | loss: 0.003516585303940438\n",
      "epoch 405 | step 0 | loss: 0.0003165699062717713\n",
      "epoch 405 | step 1 | loss: 0.0006174496615546848\n",
      "epoch 405 | step 2 | loss: 0.0009079278148853252\n",
      "epoch 405 | step 3 | loss: 0.0012079859174382943\n",
      "epoch 405 | step 4 | loss: 0.0015049118652560578\n",
      "epoch 405 | step 5 | loss: 0.0017890086111938264\n",
      "epoch 405 | step 6 | loss: 0.0020911521570625598\n",
      "epoch 405 | step 7 | loss: 0.0023647250511294625\n",
      "epoch 405 | step 8 | loss: 0.0026763686340751437\n",
      "epoch 405 | step 9 | loss: 0.0029417294732448375\n",
      "epoch 405 | step 10 | loss: 0.0032245626151572323\n",
      "epoch 405 | step 11 | loss: 0.003509828586400128\n",
      "epoch 406 | step 0 | loss: 0.0002722702991257887\n",
      "epoch 406 | step 1 | loss: 0.0005742789577859413\n",
      "epoch 406 | step 2 | loss: 0.0008525176982827054\n",
      "epoch 406 | step 3 | loss: 0.0011274408060331861\n",
      "epoch 406 | step 4 | loss: 0.0014226328829570819\n",
      "epoch 406 | step 5 | loss: 0.0017179603522605037\n",
      "epoch 406 | step 6 | loss: 0.00201786687009864\n",
      "epoch 406 | step 7 | loss: 0.00231317467370772\n",
      "epoch 406 | step 8 | loss: 0.002625400768613869\n",
      "epoch 406 | step 9 | loss: 0.00291642418196733\n",
      "epoch 406 | step 10 | loss: 0.0032141501044192383\n",
      "epoch 406 | step 11 | loss: 0.003513914048395018\n",
      "epoch 407 | step 0 | loss: 0.00032419348011735556\n",
      "epoch 407 | step 1 | loss: 0.0006385292679539638\n",
      "epoch 407 | step 2 | loss: 0.0009295129266897852\n",
      "epoch 407 | step 3 | loss: 0.0012152425769418904\n",
      "epoch 407 | step 4 | loss: 0.0015310913793830576\n",
      "epoch 407 | step 5 | loss: 0.0018103275316722987\n",
      "epoch 407 | step 6 | loss: 0.0021019011655327565\n",
      "epoch 407 | step 7 | loss: 0.0023830401468260725\n",
      "epoch 407 | step 8 | loss: 0.0026581294396462772\n",
      "epoch 407 | step 9 | loss: 0.002924275639191772\n",
      "epoch 407 | step 10 | loss: 0.003230538990605667\n",
      "epoch 407 | step 11 | loss: 0.0035074717569916238\n",
      "epoch 408 | step 0 | loss: 0.00028042247997067655\n",
      "epoch 408 | step 1 | loss: 0.0005814277055530804\n",
      "epoch 408 | step 2 | loss: 0.0008518269664945079\n",
      "epoch 408 | step 3 | loss: 0.0011560043455798243\n",
      "epoch 408 | step 4 | loss: 0.001433271222665926\n",
      "epoch 408 | step 5 | loss: 0.0017107585365351298\n",
      "epoch 408 | step 6 | loss: 0.002025670098655165\n",
      "epoch 408 | step 7 | loss: 0.0023281416406456053\n",
      "epoch 408 | step 8 | loss: 0.0026166645726898945\n",
      "epoch 408 | step 9 | loss: 0.0029084440541726135\n",
      "epoch 408 | step 10 | loss: 0.003223722825181246\n",
      "epoch 408 | step 11 | loss: 0.0035101226894308974\n",
      "epoch 409 | step 0 | loss: 0.0002949916832371201\n",
      "epoch 409 | step 1 | loss: 0.0006016045737388811\n",
      "epoch 409 | step 2 | loss: 0.0009216100977262855\n",
      "epoch 409 | step 3 | loss: 0.0011875969475841255\n",
      "epoch 409 | step 4 | loss: 0.0014248888161618439\n",
      "epoch 409 | step 5 | loss: 0.0017383422021433833\n",
      "epoch 409 | step 6 | loss: 0.002029800429458806\n",
      "epoch 409 | step 7 | loss: 0.0023021445164894934\n",
      "epoch 409 | step 8 | loss: 0.002576074688760417\n",
      "epoch 409 | step 9 | loss: 0.0028942439188010833\n",
      "epoch 409 | step 10 | loss: 0.003190620510835125\n",
      "epoch 409 | step 11 | loss: 0.0035229567679730144\n",
      "epoch 410 | step 0 | loss: 0.0002827130970685649\n",
      "epoch 410 | step 1 | loss: 0.0005708839179758101\n",
      "epoch 410 | step 2 | loss: 0.0008859590207858106\n",
      "epoch 410 | step 3 | loss: 0.0011459936533421723\n",
      "epoch 410 | step 4 | loss: 0.001449047252672565\n",
      "epoch 410 | step 5 | loss: 0.0017382098033146935\n",
      "epoch 410 | step 6 | loss: 0.0020256577562495787\n",
      "epoch 410 | step 7 | loss: 0.0022846328939861707\n",
      "epoch 410 | step 8 | loss: 0.0025990269083890328\n",
      "epoch 410 | step 9 | loss: 0.0029063636593126274\n",
      "epoch 410 | step 10 | loss: 0.0031896438333991915\n",
      "epoch 410 | step 11 | loss: 0.0035233517221501406\n",
      "epoch 411 | step 0 | loss: 0.00029178727932310306\n",
      "epoch 411 | step 1 | loss: 0.0005877688168292588\n",
      "epoch 411 | step 2 | loss: 0.0008666226824679678\n",
      "epoch 411 | step 3 | loss: 0.0011571193902918235\n",
      "epoch 411 | step 4 | loss: 0.0014304068160991246\n",
      "epoch 411 | step 5 | loss: 0.0017203586630001023\n",
      "epoch 411 | step 6 | loss: 0.0020225154224054135\n",
      "epoch 411 | step 7 | loss: 0.0023011978075750056\n",
      "epoch 411 | step 8 | loss: 0.0026228145502356303\n",
      "epoch 411 | step 9 | loss: 0.002919205065559565\n",
      "epoch 411 | step 10 | loss: 0.003211326426862724\n",
      "epoch 411 | step 11 | loss: 0.0035145219278973606\n",
      "epoch 412 | step 0 | loss: 0.00027754609711867024\n",
      "epoch 412 | step 1 | loss: 0.000582728304487499\n",
      "epoch 412 | step 2 | loss: 0.0008501017780217394\n",
      "epoch 412 | step 3 | loss: 0.0011501396260385338\n",
      "epoch 412 | step 4 | loss: 0.0014545144897938637\n",
      "epoch 412 | step 5 | loss: 0.001747025841604496\n",
      "epoch 412 | step 6 | loss: 0.00205007247764295\n",
      "epoch 412 | step 7 | loss: 0.002335007401359463\n",
      "epoch 412 | step 8 | loss: 0.0025984244043286203\n",
      "epoch 412 | step 9 | loss: 0.0028927933808886442\n",
      "epoch 412 | step 10 | loss: 0.0031959414346578938\n",
      "epoch 412 | step 11 | loss: 0.0035206713547349666\n",
      "epoch 413 | step 0 | loss: 0.0002968291292911122\n",
      "epoch 413 | step 1 | loss: 0.0006126343480015759\n",
      "epoch 413 | step 2 | loss: 0.000884842702529863\n",
      "epoch 413 | step 3 | loss: 0.0011613100117041704\n",
      "epoch 413 | step 4 | loss: 0.001442693477919301\n",
      "epoch 413 | step 5 | loss: 0.0017415569177408087\n",
      "epoch 413 | step 6 | loss: 0.0020354612766186834\n",
      "epoch 413 | step 7 | loss: 0.002341202130944085\n",
      "epoch 413 | step 8 | loss: 0.002630908401014985\n",
      "epoch 413 | step 9 | loss: 0.0029030910613474462\n",
      "epoch 413 | step 10 | loss: 0.0032058179200553107\n",
      "epoch 413 | step 11 | loss: 0.0035165608270591535\n",
      "epoch 414 | step 0 | loss: 0.0002787581891793971\n",
      "epoch 414 | step 1 | loss: 0.0005758096319185421\n",
      "epoch 414 | step 2 | loss: 0.0008550853629626873\n",
      "epoch 414 | step 3 | loss: 0.0011462918638814761\n",
      "epoch 414 | step 4 | loss: 0.0014163630586073496\n",
      "epoch 414 | step 5 | loss: 0.0017031897098383393\n",
      "epoch 414 | step 6 | loss: 0.0020033667097229175\n",
      "epoch 414 | step 7 | loss: 0.0023300475802327486\n",
      "epoch 414 | step 8 | loss: 0.0026334319365020325\n",
      "epoch 414 | step 9 | loss: 0.002910676698380467\n",
      "epoch 414 | step 10 | loss: 0.0032304755046316707\n",
      "epoch 414 | step 11 | loss: 0.0035070838423088904\n",
      "epoch 415 | step 0 | loss: 0.00031753608146728614\n",
      "epoch 415 | step 1 | loss: 0.0006366832294692641\n",
      "epoch 415 | step 2 | loss: 0.0009109356618572708\n",
      "epoch 415 | step 3 | loss: 0.0011778014044556138\n",
      "epoch 415 | step 4 | loss: 0.0014711699636119381\n",
      "epoch 415 | step 5 | loss: 0.0017780046506305721\n",
      "epoch 415 | step 6 | loss: 0.002071721472745726\n",
      "epoch 415 | step 7 | loss: 0.0023421351422413745\n",
      "epoch 415 | step 8 | loss: 0.0026550066045383\n",
      "epoch 415 | step 9 | loss: 0.0029414464360843313\n",
      "epoch 415 | step 10 | loss: 0.003218911644338563\n",
      "epoch 415 | step 11 | loss: 0.003511341011116405\n",
      "epoch 416 | step 0 | loss: 0.0003002699699836\n",
      "epoch 416 | step 1 | loss: 0.0005763041853705617\n",
      "epoch 416 | step 2 | loss: 0.0008763646985533085\n",
      "epoch 416 | step 3 | loss: 0.0011917528866659285\n",
      "epoch 416 | step 4 | loss: 0.0014778989580196418\n",
      "epoch 416 | step 5 | loss: 0.001761148307607631\n",
      "epoch 416 | step 6 | loss: 0.0020570506564866267\n",
      "epoch 416 | step 7 | loss: 0.0023592531864699328\n",
      "epoch 416 | step 8 | loss: 0.0026431661609977613\n",
      "epoch 416 | step 9 | loss: 0.0029212617311158338\n",
      "epoch 416 | step 10 | loss: 0.0032139382836884195\n",
      "epoch 416 | step 11 | loss: 0.0035133434434703966\n",
      "epoch 417 | step 0 | loss: 0.0003012490124498743\n",
      "epoch 417 | step 1 | loss: 0.0005755778022771142\n",
      "epoch 417 | step 2 | loss: 0.0008720842359414921\n",
      "epoch 417 | step 3 | loss: 0.001152032059740231\n",
      "epoch 417 | step 4 | loss: 0.0014308244886850473\n",
      "epoch 417 | step 5 | loss: 0.0017411068796198946\n",
      "epoch 417 | step 6 | loss: 0.0020008317356996095\n",
      "epoch 417 | step 7 | loss: 0.0022959947847507917\n",
      "epoch 417 | step 8 | loss: 0.002590107987868998\n",
      "epoch 417 | step 9 | loss: 0.0029243379717932665\n",
      "epoch 417 | step 10 | loss: 0.0032052663304533344\n",
      "epoch 417 | step 11 | loss: 0.003516618480996255\n",
      "epoch 418 | step 0 | loss: 0.0002802841286056603\n",
      "epoch 418 | step 1 | loss: 0.0005475812709246417\n",
      "epoch 418 | step 2 | loss: 0.0008484732650359022\n",
      "epoch 418 | step 3 | loss: 0.0011430685181573726\n",
      "epoch 418 | step 4 | loss: 0.0014171210374126627\n",
      "epoch 418 | step 5 | loss: 0.0017314345930819074\n",
      "epoch 418 | step 6 | loss: 0.0020273566930113112\n",
      "epoch 418 | step 7 | loss: 0.0023382316535880713\n",
      "epoch 418 | step 8 | loss: 0.0026370159807733644\n",
      "epoch 418 | step 9 | loss: 0.002939056576002445\n",
      "epoch 418 | step 10 | loss: 0.003236696109229989\n",
      "epoch 418 | step 11 | loss: 0.003504245169789158\n",
      "epoch 419 | step 0 | loss: 0.00030680734602857887\n",
      "epoch 419 | step 1 | loss: 0.0005978359653338479\n",
      "epoch 419 | step 2 | loss: 0.0008906883682679348\n",
      "epoch 419 | step 3 | loss: 0.0011829462616870807\n",
      "epoch 419 | step 4 | loss: 0.00146449611626692\n",
      "epoch 419 | step 5 | loss: 0.001751717212486515\n",
      "epoch 419 | step 6 | loss: 0.002058295771712743\n",
      "epoch 419 | step 7 | loss: 0.002355680884042018\n",
      "epoch 419 | step 8 | loss: 0.002631303671489507\n",
      "epoch 419 | step 9 | loss: 0.0029611392764178944\n",
      "epoch 419 | step 10 | loss: 0.0032385912115245573\n",
      "epoch 419 | step 11 | loss: 0.003503690590287733\n",
      "epoch 420 | step 0 | loss: 0.00029384364389853037\n",
      "epoch 420 | step 1 | loss: 0.0005851238841443788\n",
      "epoch 420 | step 2 | loss: 0.0008907729701324235\n",
      "epoch 420 | step 3 | loss: 0.0011657515238569526\n",
      "epoch 420 | step 4 | loss: 0.0014516225225519974\n",
      "epoch 420 | step 5 | loss: 0.0017292031203646069\n",
      "epoch 420 | step 6 | loss: 0.0020295960350500885\n",
      "epoch 420 | step 7 | loss: 0.002316196191810119\n",
      "epoch 420 | step 8 | loss: 0.0026165805868485098\n",
      "epoch 420 | step 9 | loss: 0.002910532129883176\n",
      "epoch 420 | step 10 | loss: 0.0032081396430873447\n",
      "epoch 420 | step 11 | loss: 0.0035151990170264693\n",
      "epoch 421 | step 0 | loss: 0.0002822108615802528\n",
      "epoch 421 | step 1 | loss: 0.0005490933432166624\n",
      "epoch 421 | step 2 | loss: 0.0008508571235480291\n",
      "epoch 421 | step 3 | loss: 0.0011437547595907318\n",
      "epoch 421 | step 4 | loss: 0.0014286529285911557\n",
      "epoch 421 | step 5 | loss: 0.0017490365994661961\n",
      "epoch 421 | step 6 | loss: 0.0020269723302629395\n",
      "epoch 421 | step 7 | loss: 0.002317754854625255\n",
      "epoch 421 | step 8 | loss: 0.002625029409434563\n",
      "epoch 421 | step 9 | loss: 0.002902311410280514\n",
      "epoch 421 | step 10 | loss: 0.003211292843688156\n",
      "epoch 421 | step 11 | loss: 0.003514207178562776\n",
      "epoch 422 | step 0 | loss: 0.00030422038667966226\n",
      "epoch 422 | step 1 | loss: 0.0005929108720861342\n",
      "epoch 422 | step 2 | loss: 0.0008748031434143164\n",
      "epoch 422 | step 3 | loss: 0.0011641672622813757\n",
      "epoch 422 | step 4 | loss: 0.0014771007058880907\n",
      "epoch 422 | step 5 | loss: 0.0017587189612451707\n",
      "epoch 422 | step 6 | loss: 0.002061416003357452\n",
      "epoch 422 | step 7 | loss: 0.0023620289699356643\n",
      "epoch 422 | step 8 | loss: 0.002646736848980612\n",
      "epoch 422 | step 9 | loss: 0.002948508334124825\n",
      "epoch 422 | step 10 | loss: 0.003229671528774853\n",
      "epoch 422 | step 11 | loss: 0.0035066524337939694\n",
      "epoch 423 | step 0 | loss: 0.0003131124658037374\n",
      "epoch 423 | step 1 | loss: 0.000593423767801164\n",
      "epoch 423 | step 2 | loss: 0.0008858179148940532\n",
      "epoch 423 | step 3 | loss: 0.0011855140429203493\n",
      "epoch 423 | step 4 | loss: 0.0014586927921499053\n",
      "epoch 423 | step 5 | loss: 0.0017482112285830786\n",
      "epoch 423 | step 6 | loss: 0.0020478268880775724\n",
      "epoch 423 | step 7 | loss: 0.0023455702109420338\n",
      "epoch 423 | step 8 | loss: 0.002632531183822325\n",
      "epoch 423 | step 9 | loss: 0.0029185666615264743\n",
      "epoch 423 | step 10 | loss: 0.003195527955136014\n",
      "epoch 423 | step 11 | loss: 0.003520267570527584\n",
      "epoch 424 | step 0 | loss: 0.0002875510043968561\n",
      "epoch 424 | step 1 | loss: 0.0005741866683209652\n",
      "epoch 424 | step 2 | loss: 0.0008825350533736533\n",
      "epoch 424 | step 3 | loss: 0.001163348829080956\n",
      "epoch 424 | step 4 | loss: 0.0014630584161470976\n",
      "epoch 424 | step 5 | loss: 0.0017397810553918566\n",
      "epoch 424 | step 6 | loss: 0.0020293475891277626\n",
      "epoch 424 | step 7 | loss: 0.0023472176114267924\n",
      "epoch 424 | step 8 | loss: 0.002628002940125955\n",
      "epoch 424 | step 9 | loss: 0.0029077501702235407\n",
      "epoch 424 | step 10 | loss: 0.00321270519220892\n",
      "epoch 424 | step 11 | loss: 0.003513215208420629\n",
      "epoch 425 | step 0 | loss: 0.00029508000466540367\n",
      "epoch 425 | step 1 | loss: 0.0006031883987305664\n",
      "epoch 425 | step 2 | loss: 0.0008928671537193893\n",
      "epoch 425 | step 3 | loss: 0.0012009661926242904\n",
      "epoch 425 | step 4 | loss: 0.0015347317661374145\n",
      "epoch 425 | step 5 | loss: 0.0018204309494010927\n",
      "epoch 425 | step 6 | loss: 0.00212621411120155\n",
      "epoch 425 | step 7 | loss: 0.0023867859302300494\n",
      "epoch 425 | step 8 | loss: 0.002665989618526943\n",
      "epoch 425 | step 9 | loss: 0.0029497187314269\n",
      "epoch 425 | step 10 | loss: 0.0032309624946785624\n",
      "epoch 425 | step 11 | loss: 0.0035059896408666394\n",
      "epoch 426 | step 0 | loss: 0.00028985359154519344\n",
      "epoch 426 | step 1 | loss: 0.0005790748630683651\n",
      "epoch 426 | step 2 | loss: 0.000860692729835176\n",
      "epoch 426 | step 3 | loss: 0.0011619938379531683\n",
      "epoch 426 | step 4 | loss: 0.001482753574126728\n",
      "epoch 426 | step 5 | loss: 0.0017820271506172134\n",
      "epoch 426 | step 6 | loss: 0.002058247173255252\n",
      "epoch 426 | step 7 | loss: 0.002348781631445739\n",
      "epoch 426 | step 8 | loss: 0.002621226159518856\n",
      "epoch 426 | step 9 | loss: 0.002907319473947913\n",
      "epoch 426 | step 10 | loss: 0.003225796100779709\n",
      "epoch 426 | step 11 | loss: 0.0035078923250391803\n",
      "epoch 427 | step 0 | loss: 0.00027620674549344283\n",
      "epoch 427 | step 1 | loss: 0.0005639841937881736\n",
      "epoch 427 | step 2 | loss: 0.0008594203318887884\n",
      "epoch 427 | step 3 | loss: 0.0011468546031176256\n",
      "epoch 427 | step 4 | loss: 0.0014337658142968342\n",
      "epoch 427 | step 5 | loss: 0.0017171938836381038\n",
      "epoch 427 | step 6 | loss: 0.002031998118303721\n",
      "epoch 427 | step 7 | loss: 0.0023360092348680846\n",
      "epoch 427 | step 8 | loss: 0.0026280477355996655\n",
      "epoch 427 | step 9 | loss: 0.0029350033702715806\n",
      "epoch 427 | step 10 | loss: 0.0032251911471726895\n",
      "epoch 427 | step 11 | loss: 0.003508033073596731\n",
      "epoch 428 | step 0 | loss: 0.0002697701579755488\n",
      "epoch 428 | step 1 | loss: 0.0005951204106281384\n",
      "epoch 428 | step 2 | loss: 0.0008847631703103262\n",
      "epoch 428 | step 3 | loss: 0.0011511517424953697\n",
      "epoch 428 | step 4 | loss: 0.0014594892351364928\n",
      "epoch 428 | step 5 | loss: 0.0017503252151692642\n",
      "epoch 428 | step 6 | loss: 0.002047421142924651\n",
      "epoch 428 | step 7 | loss: 0.0023518572845933885\n",
      "epoch 428 | step 8 | loss: 0.0026436132668378387\n",
      "epoch 428 | step 9 | loss: 0.0029179190848395977\n",
      "epoch 428 | step 10 | loss: 0.003209757676384019\n",
      "epoch 428 | step 11 | loss: 0.003514071394218936\n",
      "epoch 429 | step 0 | loss: 0.00025548919659088503\n",
      "epoch 429 | step 1 | loss: 0.0005523539602650322\n",
      "epoch 429 | step 2 | loss: 0.0008493872462653406\n",
      "epoch 429 | step 3 | loss: 0.001121069399658447\n",
      "epoch 429 | step 4 | loss: 0.001415014020457067\n",
      "epoch 429 | step 5 | loss: 0.0017045136770221657\n",
      "epoch 429 | step 6 | loss: 0.002028070712473318\n",
      "epoch 429 | step 7 | loss: 0.0023384492314442538\n",
      "epoch 429 | step 8 | loss: 0.0026525846947790853\n",
      "epoch 429 | step 9 | loss: 0.0029357848766984433\n",
      "epoch 429 | step 10 | loss: 0.003234214021524181\n",
      "epoch 429 | step 11 | loss: 0.003504557446610229\n",
      "epoch 430 | step 0 | loss: 0.0003268129522062694\n",
      "epoch 430 | step 1 | loss: 0.0006282099684460638\n",
      "epoch 430 | step 2 | loss: 0.0009016778932558403\n",
      "epoch 430 | step 3 | loss: 0.0012137014254309793\n",
      "epoch 430 | step 4 | loss: 0.0015429602055193905\n",
      "epoch 430 | step 5 | loss: 0.0018207727315474001\n",
      "epoch 430 | step 6 | loss: 0.002098464141109921\n",
      "epoch 430 | step 7 | loss: 0.002364230926412393\n",
      "epoch 430 | step 8 | loss: 0.0026529390666353674\n",
      "epoch 430 | step 9 | loss: 0.002941264716144004\n",
      "epoch 430 | step 10 | loss: 0.0032160214111996563\n",
      "epoch 430 | step 11 | loss: 0.003511599918643943\n",
      "epoch 431 | step 0 | loss: 0.00030046968245815004\n",
      "epoch 431 | step 1 | loss: 0.0005840992945291753\n",
      "epoch 431 | step 2 | loss: 0.0008932069096141647\n",
      "epoch 431 | step 3 | loss: 0.0011833499205558765\n",
      "epoch 431 | step 4 | loss: 0.0014733541549473747\n",
      "epoch 431 | step 5 | loss: 0.0017549219901848855\n",
      "epoch 431 | step 6 | loss: 0.0020656399947265895\n",
      "epoch 431 | step 7 | loss: 0.002355497873092074\n",
      "epoch 431 | step 8 | loss: 0.0026383955386871075\n",
      "epoch 431 | step 9 | loss: 0.0029012972086186334\n",
      "epoch 431 | step 10 | loss: 0.003205759852071264\n",
      "epoch 431 | step 11 | loss: 0.0035154583143984556\n",
      "epoch 432 | step 0 | loss: 0.00027062364257739266\n",
      "epoch 432 | step 1 | loss: 0.0005742813525113934\n",
      "epoch 432 | step 2 | loss: 0.0008803745035617678\n",
      "epoch 432 | step 3 | loss: 0.0011540679637753453\n",
      "epoch 432 | step 4 | loss: 0.001445113514562426\n",
      "epoch 432 | step 5 | loss: 0.0017296045551608262\n",
      "epoch 432 | step 6 | loss: 0.002042523794385393\n",
      "epoch 432 | step 7 | loss: 0.002346393658410044\n",
      "epoch 432 | step 8 | loss: 0.0026316313377541924\n",
      "epoch 432 | step 9 | loss: 0.0029300491432762885\n",
      "epoch 432 | step 10 | loss: 0.003222196800802786\n",
      "epoch 432 | step 11 | loss: 0.0035087497933147123\n",
      "epoch 433 | step 0 | loss: 0.0002671518008372857\n",
      "epoch 433 | step 1 | loss: 0.0005594004667306281\n",
      "epoch 433 | step 2 | loss: 0.0008300347533619103\n",
      "epoch 433 | step 3 | loss: 0.001127746115782749\n",
      "epoch 433 | step 4 | loss: 0.0014276176463292691\n",
      "epoch 433 | step 5 | loss: 0.0017161645274377166\n",
      "epoch 433 | step 6 | loss: 0.0020078363547787045\n",
      "epoch 433 | step 7 | loss: 0.00230802131630721\n",
      "epoch 433 | step 8 | loss: 0.00259241687414996\n",
      "epoch 433 | step 9 | loss: 0.0028790012450361016\n",
      "epoch 433 | step 10 | loss: 0.0032111919093667624\n",
      "epoch 433 | step 11 | loss: 0.0035130494619350717\n",
      "epoch 434 | step 0 | loss: 0.0002817244375655733\n",
      "epoch 434 | step 1 | loss: 0.0005739844228931517\n",
      "epoch 434 | step 2 | loss: 0.0008683035925362833\n",
      "epoch 434 | step 3 | loss: 0.0011480284362085425\n",
      "epoch 434 | step 4 | loss: 0.001458111127412621\n",
      "epoch 434 | step 5 | loss: 0.0017314502040117117\n",
      "epoch 434 | step 6 | loss: 0.0020136486395040546\n",
      "epoch 434 | step 7 | loss: 0.002309709072284841\n",
      "epoch 434 | step 8 | loss: 0.0026010307978589905\n",
      "epoch 434 | step 9 | loss: 0.002918458976208613\n",
      "epoch 434 | step 10 | loss: 0.003213950598011421\n",
      "epoch 434 | step 11 | loss: 0.0035118580978461115\n",
      "epoch 435 | step 0 | loss: 0.00028451664245604644\n",
      "epoch 435 | step 1 | loss: 0.0005680509182969366\n",
      "epoch 435 | step 2 | loss: 0.0008499188186665034\n",
      "epoch 435 | step 3 | loss: 0.001132744096398399\n",
      "epoch 435 | step 4 | loss: 0.001438057911964803\n",
      "epoch 435 | step 5 | loss: 0.0017428744542402857\n",
      "epoch 435 | step 6 | loss: 0.0020386913717511473\n",
      "epoch 435 | step 7 | loss: 0.002321967114973715\n",
      "epoch 435 | step 8 | loss: 0.0026549853826704196\n",
      "epoch 435 | step 9 | loss: 0.0029478282481078533\n",
      "epoch 435 | step 10 | loss: 0.003234494305109397\n",
      "epoch 435 | step 11 | loss: 0.003503972757349088\n",
      "epoch 436 | step 0 | loss: 0.00027098560584736355\n",
      "epoch 436 | step 1 | loss: 0.0005537983821881746\n",
      "epoch 436 | step 2 | loss: 0.0008703560773950639\n",
      "epoch 436 | step 3 | loss: 0.0011696171782368437\n",
      "epoch 436 | step 4 | loss: 0.0014576441904232938\n",
      "epoch 436 | step 5 | loss: 0.001759798491460085\n",
      "epoch 436 | step 6 | loss: 0.0020597673958281625\n",
      "epoch 436 | step 7 | loss: 0.0023317063266080932\n",
      "epoch 436 | step 8 | loss: 0.002624416357514478\n",
      "epoch 436 | step 9 | loss: 0.0029438847564000692\n",
      "epoch 436 | step 10 | loss: 0.0032145288633651107\n",
      "epoch 436 | step 11 | loss: 0.003511639875793994\n",
      "epoch 437 | step 0 | loss: 0.0003070753029111086\n",
      "epoch 437 | step 1 | loss: 0.0006033844071964437\n",
      "epoch 437 | step 2 | loss: 0.0009223086107706027\n",
      "epoch 437 | step 3 | loss: 0.0012472800513716\n",
      "epoch 437 | step 4 | loss: 0.0015134998743525467\n",
      "epoch 437 | step 5 | loss: 0.0018240912282305292\n",
      "epoch 437 | step 6 | loss: 0.0020893992525435165\n",
      "epoch 437 | step 7 | loss: 0.002364427832188271\n",
      "epoch 437 | step 8 | loss: 0.002649918020155924\n",
      "epoch 437 | step 9 | loss: 0.002944366364907935\n",
      "epoch 437 | step 10 | loss: 0.0032332295793545338\n",
      "epoch 437 | step 11 | loss: 0.00350460410089565\n",
      "epoch 438 | step 0 | loss: 0.00030978479158625614\n",
      "epoch 438 | step 1 | loss: 0.0005850399322177078\n",
      "epoch 438 | step 2 | loss: 0.0008863667117874248\n",
      "epoch 438 | step 3 | loss: 0.001222589564867735\n",
      "epoch 438 | step 4 | loss: 0.0014960132908660856\n",
      "epoch 438 | step 5 | loss: 0.0017966089972112742\n",
      "epoch 438 | step 6 | loss: 0.0020762495503789677\n",
      "epoch 438 | step 7 | loss: 0.0023541773995368765\n",
      "epoch 438 | step 8 | loss: 0.0026263746416320807\n",
      "epoch 438 | step 9 | loss: 0.0029333535405569266\n",
      "epoch 438 | step 10 | loss: 0.0032343433273772032\n",
      "epoch 438 | step 11 | loss: 0.003503681711328722\n",
      "epoch 439 | step 0 | loss: 0.0003025068146880122\n",
      "epoch 439 | step 1 | loss: 0.0006058658425496122\n",
      "epoch 439 | step 2 | loss: 0.0009248812364230589\n",
      "epoch 439 | step 3 | loss: 0.0012038754618423817\n",
      "epoch 439 | step 4 | loss: 0.0014923460864376147\n",
      "epoch 439 | step 5 | loss: 0.0017635965421729921\n",
      "epoch 439 | step 6 | loss: 0.0020341555547677263\n",
      "epoch 439 | step 7 | loss: 0.0023223284409453274\n",
      "epoch 439 | step 8 | loss: 0.002605036123403894\n",
      "epoch 439 | step 9 | loss: 0.0028942791248332167\n",
      "epoch 439 | step 10 | loss: 0.003208873008043682\n",
      "epoch 439 | step 11 | loss: 0.003513551903355834\n",
      "epoch 440 | step 0 | loss: 0.00026496061008132325\n",
      "epoch 440 | step 1 | loss: 0.0005363382763843624\n",
      "epoch 440 | step 2 | loss: 0.0008558104482079352\n",
      "epoch 440 | step 3 | loss: 0.001145194039764238\n",
      "epoch 440 | step 4 | loss: 0.0014308634695736754\n",
      "epoch 440 | step 5 | loss: 0.0017123785321250938\n",
      "epoch 440 | step 6 | loss: 0.0019743094604623077\n",
      "epoch 440 | step 7 | loss: 0.0022892805015899587\n",
      "epoch 440 | step 8 | loss: 0.00259519668458919\n",
      "epoch 440 | step 9 | loss: 0.0029153435376947255\n",
      "epoch 440 | step 10 | loss: 0.0031990959825372325\n",
      "epoch 440 | step 11 | loss: 0.0035172812470194463\n",
      "epoch 441 | step 0 | loss: 0.0002734597694643813\n",
      "epoch 441 | step 1 | loss: 0.000540299716929108\n",
      "epoch 441 | step 2 | loss: 0.0008488289457319791\n",
      "epoch 441 | step 3 | loss: 0.0011313373815638815\n",
      "epoch 441 | step 4 | loss: 0.0014236487556418258\n",
      "epoch 441 | step 5 | loss: 0.0017289493031505492\n",
      "epoch 441 | step 6 | loss: 0.002034252242183109\n",
      "epoch 441 | step 7 | loss: 0.0023090759406036976\n",
      "epoch 441 | step 8 | loss: 0.0026287064293514188\n",
      "epoch 441 | step 9 | loss: 0.0029070650360007947\n",
      "epoch 441 | step 10 | loss: 0.0031876877781016463\n",
      "epoch 441 | step 11 | loss: 0.0035217183629123816\n",
      "epoch 442 | step 0 | loss: 0.0002968876306895734\n",
      "epoch 442 | step 1 | loss: 0.0005895250303911659\n",
      "epoch 442 | step 2 | loss: 0.000877956137240563\n",
      "epoch 442 | step 3 | loss: 0.0011892736017455872\n",
      "epoch 442 | step 4 | loss: 0.0014749733472093603\n",
      "epoch 442 | step 5 | loss: 0.0017574129556305565\n",
      "epoch 442 | step 6 | loss: 0.0020425006782087834\n",
      "epoch 442 | step 7 | loss: 0.002340744537233017\n",
      "epoch 442 | step 8 | loss: 0.002611605436365811\n",
      "epoch 442 | step 9 | loss: 0.002934090209947155\n",
      "epoch 442 | step 10 | loss: 0.0032420087498623877\n",
      "epoch 442 | step 11 | loss: 0.0035003722221089257\n",
      "epoch 443 | step 0 | loss: 0.0002714365794747785\n",
      "epoch 443 | step 1 | loss: 0.0005623604642933237\n",
      "epoch 443 | step 2 | loss: 0.0008218419555211899\n",
      "epoch 443 | step 3 | loss: 0.0011161257827523037\n",
      "epoch 443 | step 4 | loss: 0.0014046284679180847\n",
      "epoch 443 | step 5 | loss: 0.00169604296317448\n",
      "epoch 443 | step 6 | loss: 0.0019964137418910857\n",
      "epoch 443 | step 7 | loss: 0.0022977390557090043\n",
      "epoch 443 | step 8 | loss: 0.002611119922556706\n",
      "epoch 443 | step 9 | loss: 0.002934577349837799\n",
      "epoch 443 | step 10 | loss: 0.003201092138370773\n",
      "epoch 443 | step 11 | loss: 0.0035163415470429926\n",
      "epoch 444 | step 0 | loss: 0.00031250427933877435\n",
      "epoch 444 | step 1 | loss: 0.0006302261990812621\n",
      "epoch 444 | step 2 | loss: 0.0009362093078718865\n",
      "epoch 444 | step 3 | loss: 0.0012090855599762498\n",
      "epoch 444 | step 4 | loss: 0.0015048296534064447\n",
      "epoch 444 | step 5 | loss: 0.0017752698959734585\n",
      "epoch 444 | step 6 | loss: 0.0020820778257465695\n",
      "epoch 444 | step 7 | loss: 0.002383366584519802\n",
      "epoch 444 | step 8 | loss: 0.0026675630492166745\n",
      "epoch 444 | step 9 | loss: 0.0029375223464332715\n",
      "epoch 444 | step 10 | loss: 0.0032275244129978974\n",
      "epoch 444 | step 11 | loss: 0.0035059931432394435\n",
      "epoch 445 | step 0 | loss: 0.00029958921961917436\n",
      "epoch 445 | step 1 | loss: 0.0005894957395090143\n",
      "epoch 445 | step 2 | loss: 0.0009167339128672164\n",
      "epoch 445 | step 3 | loss: 0.001193240952583027\n",
      "epoch 445 | step 4 | loss: 0.0014785230969537125\n",
      "epoch 445 | step 5 | loss: 0.0017551090276973409\n",
      "epoch 445 | step 6 | loss: 0.0020578448601770182\n",
      "epoch 445 | step 7 | loss: 0.0023523307923270244\n",
      "epoch 445 | step 8 | loss: 0.0026421645195074867\n",
      "epoch 445 | step 9 | loss: 0.002945807784766095\n",
      "epoch 445 | step 10 | loss: 0.003224468391428952\n",
      "epoch 445 | step 11 | loss: 0.0035068802553696403\n",
      "epoch 446 | step 0 | loss: 0.00027149044278533714\n",
      "epoch 446 | step 1 | loss: 0.0005959775058280324\n",
      "epoch 446 | step 2 | loss: 0.0008752583791998127\n",
      "epoch 446 | step 3 | loss: 0.001165056152739111\n",
      "epoch 446 | step 4 | loss: 0.0014756103489154632\n",
      "epoch 446 | step 5 | loss: 0.0017767630212628001\n",
      "epoch 446 | step 6 | loss: 0.0020839003640322753\n",
      "epoch 446 | step 7 | loss: 0.0023611446454258073\n",
      "epoch 446 | step 8 | loss: 0.002646309404890944\n",
      "epoch 446 | step 9 | loss: 0.0029159823045948365\n",
      "epoch 446 | step 10 | loss: 0.0031861778317258892\n",
      "epoch 446 | step 11 | loss: 0.003521842238290656\n",
      "epoch 447 | step 0 | loss: 0.000312197880995483\n",
      "epoch 447 | step 1 | loss: 0.0005837938301671445\n",
      "epoch 447 | step 2 | loss: 0.0008745889816992793\n",
      "epoch 447 | step 3 | loss: 0.0011703183770159311\n",
      "epoch 447 | step 4 | loss: 0.0014653296023974423\n",
      "epoch 447 | step 5 | loss: 0.0017540883520803271\n",
      "epoch 447 | step 6 | loss: 0.0020306279170834956\n",
      "epoch 447 | step 7 | loss: 0.002324834031954722\n",
      "epoch 447 | step 8 | loss: 0.0026007365724483866\n",
      "epoch 447 | step 9 | loss: 0.0028993145047475797\n",
      "epoch 447 | step 10 | loss: 0.0032119421457996607\n",
      "epoch 447 | step 11 | loss: 0.003511545450042846\n",
      "epoch 448 | step 0 | loss: 0.0003193007810771491\n",
      "epoch 448 | step 1 | loss: 0.0006229819529170344\n",
      "epoch 448 | step 2 | loss: 0.0009258780214636026\n",
      "epoch 448 | step 3 | loss: 0.0012175600168349815\n",
      "epoch 448 | step 4 | loss: 0.001503759426084698\n",
      "epoch 448 | step 5 | loss: 0.001788855153958814\n",
      "epoch 448 | step 6 | loss: 0.0020727471196947175\n",
      "epoch 448 | step 7 | loss: 0.00236159808161293\n",
      "epoch 448 | step 8 | loss: 0.002646573389224694\n",
      "epoch 448 | step 9 | loss: 0.0029335788930936425\n",
      "epoch 448 | step 10 | loss: 0.003231030280240928\n",
      "epoch 448 | step 11 | loss: 0.0035042641787171947\n",
      "epoch 449 | step 0 | loss: 0.00027239546785354143\n",
      "epoch 449 | step 1 | loss: 0.0005862986854625267\n",
      "epoch 449 | step 2 | loss: 0.0008471012296727802\n",
      "epoch 449 | step 3 | loss: 0.0011249881050671997\n",
      "epoch 449 | step 4 | loss: 0.0014469884842591304\n",
      "epoch 449 | step 5 | loss: 0.0017279885913056487\n",
      "epoch 449 | step 6 | loss: 0.0020336421716642974\n",
      "epoch 449 | step 7 | loss: 0.0023296654006145314\n",
      "epoch 449 | step 8 | loss: 0.002594406454917859\n",
      "epoch 449 | step 9 | loss: 0.0028856722248123993\n",
      "epoch 449 | step 10 | loss: 0.003191170817601159\n",
      "epoch 449 | step 11 | loss: 0.003519626755657214\n",
      "epoch 450 | step 0 | loss: 0.00031633883782882763\n",
      "epoch 450 | step 1 | loss: 0.0006144272439487968\n",
      "epoch 450 | step 2 | loss: 0.0009110087822653366\n",
      "epoch 450 | step 3 | loss: 0.0012004154283947461\n",
      "epoch 450 | step 4 | loss: 0.0015000067597662804\n",
      "epoch 450 | step 5 | loss: 0.0018017684027715572\n",
      "epoch 450 | step 6 | loss: 0.0020853341252587165\n",
      "epoch 450 | step 7 | loss: 0.0023722707496761294\n",
      "epoch 450 | step 8 | loss: 0.0026410954511122665\n",
      "epoch 450 | step 9 | loss: 0.002908402196751372\n",
      "epoch 450 | step 10 | loss: 0.003212546762981736\n",
      "epoch 450 | step 11 | loss: 0.003511251699228565\n",
      "epoch 451 | step 0 | loss: 0.00030274491888458325\n",
      "epoch 451 | step 1 | loss: 0.0005968993524768422\n",
      "epoch 451 | step 2 | loss: 0.0008789743432532414\n",
      "epoch 451 | step 3 | loss: 0.0011620082918278633\n",
      "epoch 451 | step 4 | loss: 0.001455802686671496\n",
      "epoch 451 | step 5 | loss: 0.001741339843850085\n",
      "epoch 451 | step 6 | loss: 0.0020315766567836195\n",
      "epoch 451 | step 7 | loss: 0.002313393818092878\n",
      "epoch 451 | step 8 | loss: 0.002603724814001546\n",
      "epoch 451 | step 9 | loss: 0.0028907871558903507\n",
      "epoch 451 | step 10 | loss: 0.003198708762729941\n",
      "epoch 451 | step 11 | loss: 0.003516446396754357\n",
      "epoch 452 | step 0 | loss: 0.00029471970382886155\n",
      "epoch 452 | step 1 | loss: 0.0005991140625669293\n",
      "epoch 452 | step 2 | loss: 0.0008773649090100732\n",
      "epoch 452 | step 3 | loss: 0.001159654738608233\n",
      "epoch 452 | step 4 | loss: 0.0014658679583088457\n",
      "epoch 452 | step 5 | loss: 0.0017842901959087218\n",
      "epoch 452 | step 6 | loss: 0.0020727955407052303\n",
      "epoch 452 | step 7 | loss: 0.002367074344149516\n",
      "epoch 452 | step 8 | loss: 0.0026512127456324616\n",
      "epoch 452 | step 9 | loss: 0.0029379193359808744\n",
      "epoch 452 | step 10 | loss: 0.0032305142349674507\n",
      "epoch 452 | step 11 | loss: 0.0035040251364994142\n",
      "epoch 453 | step 0 | loss: 0.00030982646522530004\n",
      "epoch 453 | step 1 | loss: 0.0006290067254612876\n",
      "epoch 453 | step 2 | loss: 0.0009062327905712691\n",
      "epoch 453 | step 3 | loss: 0.0012263875952709768\n",
      "epoch 453 | step 4 | loss: 0.0015192310526588398\n",
      "epoch 453 | step 5 | loss: 0.0017899057499322264\n",
      "epoch 453 | step 6 | loss: 0.002052564668070085\n",
      "epoch 453 | step 7 | loss: 0.002347481388679728\n",
      "epoch 453 | step 8 | loss: 0.0026440059412741325\n",
      "epoch 453 | step 9 | loss: 0.0029214990344145373\n",
      "epoch 453 | step 10 | loss: 0.003235249326352636\n",
      "epoch 453 | step 11 | loss: 0.003502161862456706\n",
      "epoch 454 | step 0 | loss: 0.00029910673681005464\n",
      "epoch 454 | step 1 | loss: 0.0005824951745195537\n",
      "epoch 454 | step 2 | loss: 0.0008728504156685412\n",
      "epoch 454 | step 3 | loss: 0.0011761898960667735\n",
      "epoch 454 | step 4 | loss: 0.0014605335471095042\n",
      "epoch 454 | step 5 | loss: 0.0017562166249126265\n",
      "epoch 454 | step 6 | loss: 0.002087800359082071\n",
      "epoch 454 | step 7 | loss: 0.0023887988317829226\n",
      "epoch 454 | step 8 | loss: 0.002651812000506896\n",
      "epoch 454 | step 9 | loss: 0.0029283513802001323\n",
      "epoch 454 | step 10 | loss: 0.0032094787197455787\n",
      "epoch 454 | step 11 | loss: 0.0035123545864060067\n",
      "epoch 455 | step 0 | loss: 0.0003174237324038718\n",
      "epoch 455 | step 1 | loss: 0.0005968016543091871\n",
      "epoch 455 | step 2 | loss: 0.0008919212693986537\n",
      "epoch 455 | step 3 | loss: 0.0011945538045362472\n",
      "epoch 455 | step 4 | loss: 0.0014823147646057552\n",
      "epoch 455 | step 5 | loss: 0.0017551118887867254\n",
      "epoch 455 | step 6 | loss: 0.002038650637669329\n",
      "epoch 455 | step 7 | loss: 0.0023515392374624197\n",
      "epoch 455 | step 8 | loss: 0.002633603697452652\n",
      "epoch 455 | step 9 | loss: 0.002931678664646234\n",
      "epoch 455 | step 10 | loss: 0.0032091227295534793\n",
      "epoch 455 | step 11 | loss: 0.003512036119916638\n",
      "epoch 456 | step 0 | loss: 0.0003183205601779013\n",
      "epoch 456 | step 1 | loss: 0.0006186896594521038\n",
      "epoch 456 | step 2 | loss: 0.000914606906364258\n",
      "epoch 456 | step 3 | loss: 0.001213672216134776\n",
      "epoch 456 | step 4 | loss: 0.0014816041682387177\n",
      "epoch 456 | step 5 | loss: 0.001764514269557926\n",
      "epoch 456 | step 6 | loss: 0.002075452146986941\n",
      "epoch 456 | step 7 | loss: 0.002353878655685488\n",
      "epoch 456 | step 8 | loss: 0.0026419167590428145\n",
      "epoch 456 | step 9 | loss: 0.002949123778115357\n",
      "epoch 456 | step 10 | loss: 0.0032188361081544\n",
      "epoch 456 | step 11 | loss: 0.003508557250533033\n",
      "epoch 457 | step 0 | loss: 0.00029085741171259403\n",
      "epoch 457 | step 1 | loss: 0.0006008325490741589\n",
      "epoch 457 | step 2 | loss: 0.0008912761281184718\n",
      "epoch 457 | step 3 | loss: 0.0012019969265274371\n",
      "epoch 457 | step 4 | loss: 0.0015146196079363467\n",
      "epoch 457 | step 5 | loss: 0.0017825769817883014\n",
      "epoch 457 | step 6 | loss: 0.002074806722822911\n",
      "epoch 457 | step 7 | loss: 0.002355877917237396\n",
      "epoch 457 | step 8 | loss: 0.002658301546362549\n",
      "epoch 457 | step 9 | loss: 0.0029424410437713655\n",
      "epoch 457 | step 10 | loss: 0.0032301080036756303\n",
      "epoch 457 | step 11 | loss: 0.0035038877441170213\n",
      "epoch 458 | step 0 | loss: 0.00030779804598425934\n",
      "epoch 458 | step 1 | loss: 0.0006069334961780762\n",
      "epoch 458 | step 2 | loss: 0.0009099675486365812\n",
      "epoch 458 | step 3 | loss: 0.0011987455795244503\n",
      "epoch 458 | step 4 | loss: 0.0014891518346753225\n",
      "epoch 458 | step 5 | loss: 0.001794756881874839\n",
      "epoch 458 | step 6 | loss: 0.002096600191012952\n",
      "epoch 458 | step 7 | loss: 0.0023954671665280805\n",
      "epoch 458 | step 8 | loss: 0.0026575609751661585\n",
      "epoch 458 | step 9 | loss: 0.0029458019332704913\n",
      "epoch 458 | step 10 | loss: 0.003222155753261312\n",
      "epoch 458 | step 11 | loss: 0.0035068682613419696\n",
      "epoch 459 | step 0 | loss: 0.0002908713859504489\n",
      "epoch 459 | step 1 | loss: 0.0005951980162753739\n",
      "epoch 459 | step 2 | loss: 0.0009163840285998873\n",
      "epoch 459 | step 3 | loss: 0.0011974294920722324\n",
      "epoch 459 | step 4 | loss: 0.0014843331904749366\n",
      "epoch 459 | step 5 | loss: 0.0017497320507567763\n",
      "epoch 459 | step 6 | loss: 0.00205839561678662\n",
      "epoch 459 | step 7 | loss: 0.0023340335170717744\n",
      "epoch 459 | step 8 | loss: 0.002601697083065338\n",
      "epoch 459 | step 9 | loss: 0.002890617316310546\n",
      "epoch 459 | step 10 | loss: 0.0031925716474203126\n",
      "epoch 459 | step 11 | loss: 0.003518356850789387\n",
      "epoch 460 | step 0 | loss: 0.0003361453149334963\n",
      "epoch 460 | step 1 | loss: 0.0006544865407663546\n",
      "epoch 460 | step 2 | loss: 0.000959579077373687\n",
      "epoch 460 | step 3 | loss: 0.0012073043534977196\n",
      "epoch 460 | step 4 | loss: 0.001514986694338759\n",
      "epoch 460 | step 5 | loss: 0.0018274130030345183\n",
      "epoch 460 | step 6 | loss: 0.0020945134773406628\n",
      "epoch 460 | step 7 | loss: 0.0023681774193967534\n",
      "epoch 460 | step 8 | loss: 0.002642275943299463\n",
      "epoch 460 | step 9 | loss: 0.002922981091475752\n",
      "epoch 460 | step 10 | loss: 0.003219805043027886\n",
      "epoch 460 | step 11 | loss: 0.0035076971236295525\n",
      "epoch 461 | step 0 | loss: 0.000291317124187994\n",
      "epoch 461 | step 1 | loss: 0.0005865244675361137\n",
      "epoch 461 | step 2 | loss: 0.0008762839835266394\n",
      "epoch 461 | step 3 | loss: 0.0011711961868719853\n",
      "epoch 461 | step 4 | loss: 0.0014493519439617575\n",
      "epoch 461 | step 5 | loss: 0.0017368480694024378\n",
      "epoch 461 | step 6 | loss: 0.0020093470447623105\n",
      "epoch 461 | step 7 | loss: 0.002312395678552434\n",
      "epoch 461 | step 8 | loss: 0.002606895645672937\n",
      "epoch 461 | step 9 | loss: 0.0028899531649131675\n",
      "epoch 461 | step 10 | loss: 0.003208882524108337\n",
      "epoch 461 | step 11 | loss: 0.0035118034812620597\n",
      "epoch 462 | step 0 | loss: 0.00031055264022242983\n",
      "epoch 462 | step 1 | loss: 0.0006198348834859683\n",
      "epoch 462 | step 2 | loss: 0.0009348533489777662\n",
      "epoch 462 | step 3 | loss: 0.0012158719842511705\n",
      "epoch 462 | step 4 | loss: 0.001497049857635164\n",
      "epoch 462 | step 5 | loss: 0.0018119239604510238\n",
      "epoch 462 | step 6 | loss: 0.002128212908837506\n",
      "epoch 462 | step 7 | loss: 0.0024020582610494454\n",
      "epoch 462 | step 8 | loss: 0.0026940622547206895\n",
      "epoch 462 | step 9 | loss: 0.002971389817938745\n",
      "epoch 462 | step 10 | loss: 0.0032326263572370305\n",
      "epoch 462 | step 11 | loss: 0.003502667123040885\n",
      "epoch 463 | step 0 | loss: 0.0002793618657148791\n",
      "epoch 463 | step 1 | loss: 0.00058496676316713\n",
      "epoch 463 | step 2 | loss: 0.0008818060524177782\n",
      "epoch 463 | step 3 | loss: 0.0011351710576740404\n",
      "epoch 463 | step 4 | loss: 0.001451134452753387\n",
      "epoch 463 | step 5 | loss: 0.0017442587211102638\n",
      "epoch 463 | step 6 | loss: 0.002035367817221933\n",
      "epoch 463 | step 7 | loss: 0.0023192266617739167\n",
      "epoch 463 | step 8 | loss: 0.002605240111616198\n",
      "epoch 463 | step 9 | loss: 0.0029116350845252736\n",
      "epoch 463 | step 10 | loss: 0.003213177518372736\n",
      "epoch 463 | step 11 | loss: 0.003509943985464974\n",
      "epoch 464 | step 0 | loss: 0.0002959694586745131\n",
      "epoch 464 | step 1 | loss: 0.0005991810850521755\n",
      "epoch 464 | step 2 | loss: 0.0008611282766443033\n",
      "epoch 464 | step 3 | loss: 0.001145764547878232\n",
      "epoch 464 | step 4 | loss: 0.0014563018557977273\n",
      "epoch 464 | step 5 | loss: 0.001733900490051197\n",
      "epoch 464 | step 6 | loss: 0.0020410612372813762\n",
      "epoch 464 | step 7 | loss: 0.002317840238515785\n",
      "epoch 464 | step 8 | loss: 0.002617266629600468\n",
      "epoch 464 | step 9 | loss: 0.0029215809229917846\n",
      "epoch 464 | step 10 | loss: 0.0032133055863744543\n",
      "epoch 464 | step 11 | loss: 0.003509680052518476\n",
      "epoch 465 | step 0 | loss: 0.0002707362386372435\n",
      "epoch 465 | step 1 | loss: 0.0005672458129452275\n",
      "epoch 465 | step 2 | loss: 0.0008212024328457347\n",
      "epoch 465 | step 3 | loss: 0.0011138661189237203\n",
      "epoch 465 | step 4 | loss: 0.0014215869024620982\n",
      "epoch 465 | step 5 | loss: 0.0017430181419044685\n",
      "epoch 465 | step 6 | loss: 0.0020334970093994495\n",
      "epoch 465 | step 7 | loss: 0.0023404104021440618\n",
      "epoch 465 | step 8 | loss: 0.002640893578025207\n",
      "epoch 465 | step 9 | loss: 0.002931948290042724\n",
      "epoch 465 | step 10 | loss: 0.0032213223558464088\n",
      "epoch 465 | step 11 | loss: 0.0035065422788689267\n",
      "epoch 466 | step 0 | loss: 0.00027346920675793904\n",
      "epoch 466 | step 1 | loss: 0.0005530920572645599\n",
      "epoch 466 | step 2 | loss: 0.0008704886930447041\n",
      "epoch 466 | step 3 | loss: 0.0011704329950826982\n",
      "epoch 466 | step 4 | loss: 0.001452378242166564\n",
      "epoch 466 | step 5 | loss: 0.0017650448195699552\n",
      "epoch 466 | step 6 | loss: 0.0020582987871303102\n",
      "epoch 466 | step 7 | loss: 0.0023261818094994493\n",
      "epoch 466 | step 8 | loss: 0.0026129855443453665\n",
      "epoch 466 | step 9 | loss: 0.0029497036909328428\n",
      "epoch 466 | step 10 | loss: 0.003236535172667658\n",
      "epoch 466 | step 11 | loss: 0.0035006809451025135\n",
      "epoch 467 | step 0 | loss: 0.000284180504737196\n",
      "epoch 467 | step 1 | loss: 0.0005759514495185913\n",
      "epoch 467 | step 2 | loss: 0.0008576764980680843\n",
      "epoch 467 | step 3 | loss: 0.0011415692364035988\n",
      "epoch 467 | step 4 | loss: 0.0014367231807599183\n",
      "epoch 467 | step 5 | loss: 0.0017533952465830754\n",
      "epoch 467 | step 6 | loss: 0.002026381561932544\n",
      "epoch 467 | step 7 | loss: 0.002316143347969778\n",
      "epoch 467 | step 8 | loss: 0.0026087275699496746\n",
      "epoch 467 | step 9 | loss: 0.0029297807836351085\n",
      "epoch 467 | step 10 | loss: 0.0032198081117625135\n",
      "epoch 467 | step 11 | loss: 0.003507224853331387\n",
      "epoch 468 | step 0 | loss: 0.0002870727463374631\n",
      "epoch 468 | step 1 | loss: 0.0005886862032168164\n",
      "epoch 468 | step 2 | loss: 0.000884417895581117\n",
      "epoch 468 | step 3 | loss: 0.0011553359931604644\n",
      "epoch 468 | step 4 | loss: 0.001468340629983664\n",
      "epoch 468 | step 5 | loss: 0.001766709332686523\n",
      "epoch 468 | step 6 | loss: 0.0020780310392099492\n",
      "epoch 468 | step 7 | loss: 0.002376305396983399\n",
      "epoch 468 | step 8 | loss: 0.002680501337203829\n",
      "epoch 468 | step 9 | loss: 0.0029668313638768493\n",
      "epoch 468 | step 10 | loss: 0.0032303087492926625\n",
      "epoch 468 | step 11 | loss: 0.003502690887062565\n",
      "epoch 469 | step 0 | loss: 0.00025787191150628415\n",
      "epoch 469 | step 1 | loss: 0.0005526079714734153\n",
      "epoch 469 | step 2 | loss: 0.0008452541100552761\n",
      "epoch 469 | step 3 | loss: 0.0011659431376276892\n",
      "epoch 469 | step 4 | loss: 0.0014626455780066942\n",
      "epoch 469 | step 5 | loss: 0.001761950950884636\n",
      "epoch 469 | step 6 | loss: 0.0020331702323074974\n",
      "epoch 469 | step 7 | loss: 0.002339717335170687\n",
      "epoch 469 | step 8 | loss: 0.002609411905973593\n",
      "epoch 469 | step 9 | loss: 0.002903406866367114\n",
      "epoch 469 | step 10 | loss: 0.003203984550468873\n",
      "epoch 469 | step 11 | loss: 0.0035131563375138002\n",
      "epoch 470 | step 0 | loss: 0.000297867390493142\n",
      "epoch 470 | step 1 | loss: 0.0005870621603266248\n",
      "epoch 470 | step 2 | loss: 0.0009025016554872096\n",
      "epoch 470 | step 3 | loss: 0.0011808815480607545\n",
      "epoch 470 | step 4 | loss: 0.0014680450584288827\n",
      "epoch 470 | step 5 | loss: 0.001748777684752932\n",
      "epoch 470 | step 6 | loss: 0.00203627787506266\n",
      "epoch 470 | step 7 | loss: 0.0023324413751215226\n",
      "epoch 470 | step 8 | loss: 0.002602671258405405\n",
      "epoch 470 | step 9 | loss: 0.0029065301705277864\n",
      "epoch 470 | step 10 | loss: 0.0032159767359806997\n",
      "epoch 470 | step 11 | loss: 0.003508301699427012\n",
      "epoch 471 | step 0 | loss: 0.00027896932623812175\n",
      "epoch 471 | step 1 | loss: 0.0005470928026061747\n",
      "epoch 471 | step 2 | loss: 0.0008466535136391182\n",
      "epoch 471 | step 3 | loss: 0.0011622191560628837\n",
      "epoch 471 | step 4 | loss: 0.0014418611867717686\n",
      "epoch 471 | step 5 | loss: 0.0017569825308191198\n",
      "epoch 471 | step 6 | loss: 0.002046735782279057\n",
      "epoch 471 | step 7 | loss: 0.00234206912639939\n",
      "epoch 471 | step 8 | loss: 0.0026161217343333904\n",
      "epoch 471 | step 9 | loss: 0.0029182748490967033\n",
      "epoch 471 | step 10 | loss: 0.0032062234606533404\n",
      "epoch 471 | step 11 | loss: 0.0035121653864983067\n",
      "epoch 472 | step 0 | loss: 0.00030389482360229116\n",
      "epoch 472 | step 1 | loss: 0.0005969017616763094\n",
      "epoch 472 | step 2 | loss: 0.0009037050929630415\n",
      "epoch 472 | step 3 | loss: 0.001227598014970039\n",
      "epoch 472 | step 4 | loss: 0.0015342544334470073\n",
      "epoch 472 | step 5 | loss: 0.0018122931403315554\n",
      "epoch 472 | step 6 | loss: 0.0020802565528398815\n",
      "epoch 472 | step 7 | loss: 0.002373527120022678\n",
      "epoch 472 | step 8 | loss: 0.0026596298469204207\n",
      "epoch 472 | step 9 | loss: 0.002941721045111029\n",
      "epoch 472 | step 10 | loss: 0.0032281550446257882\n",
      "epoch 472 | step 11 | loss: 0.0035036197195274245\n",
      "epoch 473 | step 0 | loss: 0.00029534295706540965\n",
      "epoch 473 | step 1 | loss: 0.0005736954874805665\n",
      "epoch 473 | step 2 | loss: 0.0008534945804806633\n",
      "epoch 473 | step 3 | loss: 0.0011758091963744066\n",
      "epoch 473 | step 4 | loss: 0.0014366165365393283\n",
      "epoch 473 | step 5 | loss: 0.001733324628945633\n",
      "epoch 473 | step 6 | loss: 0.0020164482457000242\n",
      "epoch 473 | step 7 | loss: 0.0023118115438786496\n",
      "epoch 473 | step 8 | loss: 0.002629373533071404\n",
      "epoch 473 | step 9 | loss: 0.0029158053850151704\n",
      "epoch 473 | step 10 | loss: 0.0032003909705998943\n",
      "epoch 473 | step 11 | loss: 0.0035142972156672362\n",
      "epoch 474 | step 0 | loss: 0.00032460801182920644\n",
      "epoch 474 | step 1 | loss: 0.0006098939287175328\n",
      "epoch 474 | step 2 | loss: 0.0009289621729820913\n",
      "epoch 474 | step 3 | loss: 0.0011927097665153227\n",
      "epoch 474 | step 4 | loss: 0.0015044450107354838\n",
      "epoch 474 | step 5 | loss: 0.0018002751792941733\n",
      "epoch 474 | step 6 | loss: 0.0020662581720940778\n",
      "epoch 474 | step 7 | loss: 0.0023296792998740113\n",
      "epoch 474 | step 8 | loss: 0.0026450159283391674\n",
      "epoch 474 | step 9 | loss: 0.0029278387990604574\n",
      "epoch 474 | step 10 | loss: 0.0032243947092882573\n",
      "epoch 474 | step 11 | loss: 0.0035048492552563508\n",
      "epoch 475 | step 0 | loss: 0.0003026041280250203\n",
      "epoch 475 | step 1 | loss: 0.0006350327695139734\n",
      "epoch 475 | step 2 | loss: 0.0009006919037580941\n",
      "epoch 475 | step 3 | loss: 0.0011911806790256259\n",
      "epoch 475 | step 4 | loss: 0.0014543332247926406\n",
      "epoch 475 | step 5 | loss: 0.001758884555938\n",
      "epoch 475 | step 6 | loss: 0.002048066660980755\n",
      "epoch 475 | step 7 | loss: 0.0023431915253116845\n",
      "epoch 475 | step 8 | loss: 0.0026326269868555017\n",
      "epoch 475 | step 9 | loss: 0.0029353071364714824\n",
      "epoch 475 | step 10 | loss: 0.0032284032281420594\n",
      "epoch 475 | step 11 | loss: 0.00350304177263887\n",
      "epoch 476 | step 0 | loss: 0.0002974062661426386\n",
      "epoch 476 | step 1 | loss: 0.0005816192172695191\n",
      "epoch 476 | step 2 | loss: 0.0008704962520241354\n",
      "epoch 476 | step 3 | loss: 0.0011429088789803837\n",
      "epoch 476 | step 4 | loss: 0.0014512027493820733\n",
      "epoch 476 | step 5 | loss: 0.0017170669540465685\n",
      "epoch 476 | step 6 | loss: 0.0020085675990973565\n",
      "epoch 476 | step 7 | loss: 0.0023008856100113413\n",
      "epoch 476 | step 8 | loss: 0.002584311468594394\n",
      "epoch 476 | step 9 | loss: 0.002905634598827562\n",
      "epoch 476 | step 10 | loss: 0.003207166729860313\n",
      "epoch 476 | step 11 | loss: 0.0035112316838382777\n",
      "epoch 477 | step 0 | loss: 0.00030271434411148463\n",
      "epoch 477 | step 1 | loss: 0.0005954314727431998\n",
      "epoch 477 | step 2 | loss: 0.0008765460492024072\n",
      "epoch 477 | step 3 | loss: 0.00116640932675476\n",
      "epoch 477 | step 4 | loss: 0.0014470882699488081\n",
      "epoch 477 | step 5 | loss: 0.001734297326359276\n",
      "epoch 477 | step 6 | loss: 0.0020275551480034093\n",
      "epoch 477 | step 7 | loss: 0.002359162247420819\n",
      "epoch 477 | step 8 | loss: 0.0026554467444379988\n",
      "epoch 477 | step 9 | loss: 0.002955978591743823\n",
      "epoch 477 | step 10 | loss: 0.003243887037782945\n",
      "epoch 477 | step 11 | loss: 0.0034969885224958677\n",
      "epoch 478 | step 0 | loss: 0.0003024230399881026\n",
      "epoch 478 | step 1 | loss: 0.0005720149785814913\n",
      "epoch 478 | step 2 | loss: 0.000850711352993906\n",
      "epoch 478 | step 3 | loss: 0.0011811824753307104\n",
      "epoch 478 | step 4 | loss: 0.001447938282780003\n",
      "epoch 478 | step 5 | loss: 0.0017340168480868493\n",
      "epoch 478 | step 6 | loss: 0.002018729706038219\n",
      "epoch 478 | step 7 | loss: 0.0023362116655191666\n",
      "epoch 478 | step 8 | loss: 0.002625655584893858\n",
      "epoch 478 | step 9 | loss: 0.002930668986743215\n",
      "epoch 478 | step 10 | loss: 0.0032092323186397654\n",
      "epoch 478 | step 11 | loss: 0.0035102346972013636\n",
      "epoch 479 | step 0 | loss: 0.00030649518396647633\n",
      "epoch 479 | step 1 | loss: 0.0006032110798645929\n",
      "epoch 479 | step 2 | loss: 0.000900155263621645\n",
      "epoch 479 | step 3 | loss: 0.0011802448650276025\n",
      "epoch 479 | step 4 | loss: 0.0014777954209325911\n",
      "epoch 479 | step 5 | loss: 0.0017881220229612371\n",
      "epoch 479 | step 6 | loss: 0.0020810464959894444\n",
      "epoch 479 | step 7 | loss: 0.0023487648052265375\n",
      "epoch 479 | step 8 | loss: 0.002650625988815828\n",
      "epoch 479 | step 9 | loss: 0.0029528306030217827\n",
      "epoch 479 | step 10 | loss: 0.003224586904577828\n",
      "epoch 479 | step 11 | loss: 0.0035043280609250737\n",
      "epoch 480 | step 0 | loss: 0.000289924652404771\n",
      "epoch 480 | step 1 | loss: 0.000557956336004547\n",
      "epoch 480 | step 2 | loss: 0.0008493731349251806\n",
      "epoch 480 | step 3 | loss: 0.0011356587743522717\n",
      "epoch 480 | step 4 | loss: 0.0014458746173431458\n",
      "epoch 480 | step 5 | loss: 0.001728660004049427\n",
      "epoch 480 | step 6 | loss: 0.0020148616729227914\n",
      "epoch 480 | step 7 | loss: 0.002301620534331808\n",
      "epoch 480 | step 8 | loss: 0.0025912942581559806\n",
      "epoch 480 | step 9 | loss: 0.002918437852306191\n",
      "epoch 480 | step 10 | loss: 0.003185413526983042\n",
      "epoch 480 | step 11 | loss: 0.0035195965637571367\n",
      "epoch 481 | step 0 | loss: 0.0002743921236349125\n",
      "epoch 481 | step 1 | loss: 0.0005567264727586023\n",
      "epoch 481 | step 2 | loss: 0.0008607092294718573\n",
      "epoch 481 | step 3 | loss: 0.0011503001147853335\n",
      "epoch 481 | step 4 | loss: 0.0014386600870084324\n",
      "epoch 481 | step 5 | loss: 0.0017241175038278586\n",
      "epoch 481 | step 6 | loss: 0.002030726128073994\n",
      "epoch 481 | step 7 | loss: 0.002347857196434652\n",
      "epoch 481 | step 8 | loss: 0.002647079337786867\n",
      "epoch 481 | step 9 | loss: 0.0029363033618358407\n",
      "epoch 481 | step 10 | loss: 0.0032370034144117445\n",
      "epoch 481 | step 11 | loss: 0.0034993544097565313\n",
      "epoch 482 | step 0 | loss: 0.0002968127410252775\n",
      "epoch 482 | step 1 | loss: 0.0005827758999017057\n",
      "epoch 482 | step 2 | loss: 0.000896747110866399\n",
      "epoch 482 | step 3 | loss: 0.001179514275365436\n",
      "epoch 482 | step 4 | loss: 0.0014550228829352569\n",
      "epoch 482 | step 5 | loss: 0.0017406863754311597\n",
      "epoch 482 | step 6 | loss: 0.0020191639343238458\n",
      "epoch 482 | step 7 | loss: 0.002301099914082104\n",
      "epoch 482 | step 8 | loss: 0.002610544799293707\n",
      "epoch 482 | step 9 | loss: 0.0029023966408041352\n",
      "epoch 482 | step 10 | loss: 0.003191644173766933\n",
      "epoch 482 | step 11 | loss: 0.0035171127318889745\n",
      "epoch 483 | step 0 | loss: 0.0002949359800187424\n",
      "epoch 483 | step 1 | loss: 0.0006071927439344039\n",
      "epoch 483 | step 2 | loss: 0.0009044486808723783\n",
      "epoch 483 | step 3 | loss: 0.0011782749880538533\n",
      "epoch 483 | step 4 | loss: 0.00147345882669911\n",
      "epoch 483 | step 5 | loss: 0.0017638254675526835\n",
      "epoch 483 | step 6 | loss: 0.0020481005598681117\n",
      "epoch 483 | step 7 | loss: 0.0023416813718754987\n",
      "epoch 483 | step 8 | loss: 0.0026374184999062417\n",
      "epoch 483 | step 9 | loss: 0.0029440216571989864\n",
      "epoch 483 | step 10 | loss: 0.0032110628519655634\n",
      "epoch 483 | step 11 | loss: 0.0035094295287900084\n",
      "epoch 484 | step 0 | loss: 0.00031208994858658156\n",
      "epoch 484 | step 1 | loss: 0.0006178643895472242\n",
      "epoch 484 | step 2 | loss: 0.0009289046987653914\n",
      "epoch 484 | step 3 | loss: 0.0012315729635604805\n",
      "epoch 484 | step 4 | loss: 0.0015124582800170251\n",
      "epoch 484 | step 5 | loss: 0.0017862849298339004\n",
      "epoch 484 | step 6 | loss: 0.0020936406801914543\n",
      "epoch 484 | step 7 | loss: 0.002373261109632905\n",
      "epoch 484 | step 8 | loss: 0.0026565002825206727\n",
      "epoch 484 | step 9 | loss: 0.0029165337211437977\n",
      "epoch 484 | step 10 | loss: 0.0032178857705927053\n",
      "epoch 484 | step 11 | loss: 0.003506298423092611\n",
      "epoch 485 | step 0 | loss: 0.0003001936764315678\n",
      "epoch 485 | step 1 | loss: 0.000570722969251346\n",
      "epoch 485 | step 2 | loss: 0.0008608804177011754\n",
      "epoch 485 | step 3 | loss: 0.0011422071251725565\n",
      "epoch 485 | step 4 | loss: 0.001424370902109023\n",
      "epoch 485 | step 5 | loss: 0.0017332550146169066\n",
      "epoch 485 | step 6 | loss: 0.002022241637705538\n",
      "epoch 485 | step 7 | loss: 0.002322416252448148\n",
      "epoch 485 | step 8 | loss: 0.002633410470177735\n",
      "epoch 485 | step 9 | loss: 0.0029117096862714036\n",
      "epoch 485 | step 10 | loss: 0.003208945416132316\n",
      "epoch 485 | step 11 | loss: 0.0035099728609379577\n",
      "epoch 486 | step 0 | loss: 0.00029082803022175227\n",
      "epoch 486 | step 1 | loss: 0.0005767656198850954\n",
      "epoch 486 | step 2 | loss: 0.0008681862740060487\n",
      "epoch 486 | step 3 | loss: 0.0011597683821977026\n",
      "epoch 486 | step 4 | loss: 0.0014462178213205325\n",
      "epoch 486 | step 5 | loss: 0.0017512181065030713\n",
      "epoch 486 | step 6 | loss: 0.0020404015470373285\n",
      "epoch 486 | step 7 | loss: 0.002328601323912326\n",
      "epoch 486 | step 8 | loss: 0.002619053482178168\n",
      "epoch 486 | step 9 | loss: 0.002915588011250158\n",
      "epoch 486 | step 10 | loss: 0.0032159930503693006\n",
      "epoch 486 | step 11 | loss: 0.003507120056967359\n",
      "epoch 487 | step 0 | loss: 0.00029588464954816325\n",
      "epoch 487 | step 1 | loss: 0.0005536166457229235\n",
      "epoch 487 | step 2 | loss: 0.0008350383935909169\n",
      "epoch 487 | step 3 | loss: 0.0011460330755289623\n",
      "epoch 487 | step 4 | loss: 0.001455413051207202\n",
      "epoch 487 | step 5 | loss: 0.0017487331448049303\n",
      "epoch 487 | step 6 | loss: 0.0020027901055577795\n",
      "epoch 487 | step 7 | loss: 0.0022737480946309853\n",
      "epoch 487 | step 8 | loss: 0.0025891101825167634\n",
      "epoch 487 | step 9 | loss: 0.0028942721915743786\n",
      "epoch 487 | step 10 | loss: 0.0031795846533323154\n",
      "epoch 487 | step 11 | loss: 0.0035212766434700336\n",
      "epoch 488 | step 0 | loss: 0.00031146192997897866\n",
      "epoch 488 | step 1 | loss: 0.0005857198561846158\n",
      "epoch 488 | step 2 | loss: 0.0008837196607940264\n",
      "epoch 488 | step 3 | loss: 0.0012073224504125437\n",
      "epoch 488 | step 4 | loss: 0.0014922278390848028\n",
      "epoch 488 | step 5 | loss: 0.001776087642654843\n",
      "epoch 488 | step 6 | loss: 0.002056586557036641\n",
      "epoch 488 | step 7 | loss: 0.0023457783617604503\n",
      "epoch 488 | step 8 | loss: 0.002621683500351117\n",
      "epoch 488 | step 9 | loss: 0.0029178232337763875\n",
      "epoch 488 | step 10 | loss: 0.0032159273808973687\n",
      "epoch 488 | step 11 | loss: 0.0035071024185391536\n",
      "epoch 489 | step 0 | loss: 0.0002832041513006192\n",
      "epoch 489 | step 1 | loss: 0.0005723631487900988\n",
      "epoch 489 | step 2 | loss: 0.0008841234326195418\n",
      "epoch 489 | step 3 | loss: 0.001159329726960525\n",
      "epoch 489 | step 4 | loss: 0.001454851259479992\n",
      "epoch 489 | step 5 | loss: 0.0017512580795679233\n",
      "epoch 489 | step 6 | loss: 0.00203587894145239\n",
      "epoch 489 | step 7 | loss: 0.002319981348454848\n",
      "epoch 489 | step 8 | loss: 0.0026031133284150935\n",
      "epoch 489 | step 9 | loss: 0.00291335186846259\n",
      "epoch 489 | step 10 | loss: 0.0032074771150181125\n",
      "epoch 489 | step 11 | loss: 0.003510097423795177\n",
      "epoch 490 | step 0 | loss: 0.00028842555910122303\n",
      "epoch 490 | step 1 | loss: 0.0006206193679243941\n",
      "epoch 490 | step 2 | loss: 0.0009250281721270305\n",
      "epoch 490 | step 3 | loss: 0.001202613732857624\n",
      "epoch 490 | step 4 | loss: 0.001510193353679826\n",
      "epoch 490 | step 5 | loss: 0.0018099294923596021\n",
      "epoch 490 | step 6 | loss: 0.0020719837147717916\n",
      "epoch 490 | step 7 | loss: 0.002342760490092002\n",
      "epoch 490 | step 8 | loss: 0.0026452622822487697\n",
      "epoch 490 | step 9 | loss: 0.0029418700973143767\n",
      "epoch 490 | step 10 | loss: 0.003221517688465307\n",
      "epoch 490 | step 11 | loss: 0.0035044458666106163\n",
      "epoch 491 | step 0 | loss: 0.00031646419563896897\n",
      "epoch 491 | step 1 | loss: 0.0006267047524966843\n",
      "epoch 491 | step 2 | loss: 0.000916249706561687\n",
      "epoch 491 | step 3 | loss: 0.0012298860460099234\n",
      "epoch 491 | step 4 | loss: 0.0015151087211321623\n",
      "epoch 491 | step 5 | loss: 0.0018303179831638914\n",
      "epoch 491 | step 6 | loss: 0.0021133080613174758\n",
      "epoch 491 | step 7 | loss: 0.0024072117555836137\n",
      "epoch 491 | step 8 | loss: 0.0026933765505870134\n",
      "epoch 491 | step 9 | loss: 0.0029598982252714958\n",
      "epoch 491 | step 10 | loss: 0.0032281755501103503\n",
      "epoch 491 | step 11 | loss: 0.003501788873456269\n",
      "epoch 492 | step 0 | loss: 0.00030422305594642377\n",
      "epoch 492 | step 1 | loss: 0.0006176541367023747\n",
      "epoch 492 | step 2 | loss: 0.000904964213654938\n",
      "epoch 492 | step 3 | loss: 0.0012046000864956727\n",
      "epoch 492 | step 4 | loss: 0.0015099416519314777\n",
      "epoch 492 | step 5 | loss: 0.001803356582612977\n",
      "epoch 492 | step 6 | loss: 0.0021115439644850747\n",
      "epoch 492 | step 7 | loss: 0.002390105245695432\n",
      "epoch 492 | step 8 | loss: 0.0026833662510657484\n",
      "epoch 492 | step 9 | loss: 0.0029640509818809335\n",
      "epoch 492 | step 10 | loss: 0.003228144667816457\n",
      "epoch 492 | step 11 | loss: 0.003501905664342163\n",
      "epoch 493 | step 0 | loss: 0.00028744611747364763\n",
      "epoch 493 | step 1 | loss: 0.0005681955624556943\n",
      "epoch 493 | step 2 | loss: 0.0008768222457340427\n",
      "epoch 493 | step 3 | loss: 0.001163548309936128\n",
      "epoch 493 | step 4 | loss: 0.0014698825150955166\n",
      "epoch 493 | step 5 | loss: 0.0017804168882533486\n",
      "epoch 493 | step 6 | loss: 0.0020659869620928495\n",
      "epoch 493 | step 7 | loss: 0.002329062300327498\n",
      "epoch 493 | step 8 | loss: 0.0026138211238069834\n",
      "epoch 493 | step 9 | loss: 0.0029199401172319067\n",
      "epoch 493 | step 10 | loss: 0.0032117996167880082\n",
      "epoch 493 | step 11 | loss: 0.003507939548415706\n",
      "epoch 494 | step 0 | loss: 0.0003159488101933704\n",
      "epoch 494 | step 1 | loss: 0.0006176094516565806\n",
      "epoch 494 | step 2 | loss: 0.0009062678778219407\n",
      "epoch 494 | step 3 | loss: 0.0012039577937985296\n",
      "epoch 494 | step 4 | loss: 0.001510427224446219\n",
      "epoch 494 | step 5 | loss: 0.001806288117141269\n",
      "epoch 494 | step 6 | loss: 0.002090430573463486\n",
      "epoch 494 | step 7 | loss: 0.0023706125047461154\n",
      "epoch 494 | step 8 | loss: 0.0026737429123453857\n",
      "epoch 494 | step 9 | loss: 0.0029564570537443068\n",
      "epoch 494 | step 10 | loss: 0.0032373887688187655\n",
      "epoch 494 | step 11 | loss: 0.00349817586371817\n",
      "epoch 495 | step 0 | loss: 0.00029181190749715387\n",
      "epoch 495 | step 1 | loss: 0.0005796299540757159\n",
      "epoch 495 | step 2 | loss: 0.0008734740056777323\n",
      "epoch 495 | step 3 | loss: 0.0011722610190771428\n",
      "epoch 495 | step 4 | loss: 0.001441807566692974\n",
      "epoch 495 | step 5 | loss: 0.001722935021221103\n",
      "epoch 495 | step 6 | loss: 0.00201650210527255\n",
      "epoch 495 | step 7 | loss: 0.0022965047581260394\n",
      "epoch 495 | step 8 | loss: 0.002604372229247612\n",
      "epoch 495 | step 9 | loss: 0.002911119990688138\n",
      "epoch 495 | step 10 | loss: 0.0032131511755526796\n",
      "epoch 495 | step 11 | loss: 0.0035075621262061945\n",
      "epoch 496 | step 0 | loss: 0.0003020266249063792\n",
      "epoch 496 | step 1 | loss: 0.0005972574501557734\n",
      "epoch 496 | step 2 | loss: 0.0008989420318393445\n",
      "epoch 496 | step 3 | loss: 0.0011917752824126898\n",
      "epoch 496 | step 4 | loss: 0.0015075760393311675\n",
      "epoch 496 | step 5 | loss: 0.0017866438216284943\n",
      "epoch 496 | step 6 | loss: 0.002047899114737638\n",
      "epoch 496 | step 7 | loss: 0.00234174973291456\n",
      "epoch 496 | step 8 | loss: 0.0026626349546897176\n",
      "epoch 496 | step 9 | loss: 0.002933776047153153\n",
      "epoch 496 | step 10 | loss: 0.0032297417528088443\n",
      "epoch 496 | step 11 | loss: 0.0035007559894887396\n",
      "epoch 497 | step 0 | loss: 0.0002949369502668259\n",
      "epoch 497 | step 1 | loss: 0.0005947698339578946\n",
      "epoch 497 | step 2 | loss: 0.0008689921223064683\n",
      "epoch 497 | step 3 | loss: 0.0011678251510754071\n",
      "epoch 497 | step 4 | loss: 0.0014625881302613272\n",
      "epoch 497 | step 5 | loss: 0.0017812259764257733\n",
      "epoch 497 | step 6 | loss: 0.0020912091907642847\n",
      "epoch 497 | step 7 | loss: 0.002371559840092566\n",
      "epoch 497 | step 8 | loss: 0.0026711891772762454\n",
      "epoch 497 | step 9 | loss: 0.0029606269089927015\n",
      "epoch 497 | step 10 | loss: 0.003225861139296711\n",
      "epoch 497 | step 11 | loss: 0.003502318535963657\n",
      "epoch 498 | step 0 | loss: 0.00028583172493475357\n",
      "epoch 498 | step 1 | loss: 0.000594757836816929\n",
      "epoch 498 | step 2 | loss: 0.0008943945426026837\n",
      "epoch 498 | step 3 | loss: 0.0012041022060623615\n",
      "epoch 498 | step 4 | loss: 0.0014897488301097639\n",
      "epoch 498 | step 5 | loss: 0.0017814668144798458\n",
      "epoch 498 | step 6 | loss: 0.00208694393870044\n",
      "epoch 498 | step 7 | loss: 0.0023499620888323834\n",
      "epoch 498 | step 8 | loss: 0.0026372438081695444\n",
      "epoch 498 | step 9 | loss: 0.0029410631999138926\n",
      "epoch 498 | step 10 | loss: 0.003221124380712864\n",
      "epoch 498 | step 11 | loss: 0.0035042674885765604\n",
      "epoch 499 | step 0 | loss: 0.0002655978682154309\n",
      "epoch 499 | step 1 | loss: 0.0005537320150543439\n",
      "epoch 499 | step 2 | loss: 0.000818473881466313\n",
      "epoch 499 | step 3 | loss: 0.0011312876365109205\n",
      "epoch 499 | step 4 | loss: 0.0014140699110294988\n",
      "epoch 499 | step 5 | loss: 0.0017144083949964268\n",
      "epoch 499 | step 6 | loss: 0.002001439003259112\n",
      "epoch 499 | step 7 | loss: 0.0023210002425880323\n",
      "epoch 499 | step 8 | loss: 0.0026359121239515945\n",
      "epoch 499 | step 9 | loss: 0.0029645690350142044\n",
      "epoch 499 | step 10 | loss: 0.0032345892229014425\n",
      "epoch 499 | step 11 | loss: 0.0034986091650407508\n",
      "epoch 500 | step 0 | loss: 0.00030886510976927566\n",
      "epoch 500 | step 1 | loss: 0.0006054692425778661\n",
      "epoch 500 | step 2 | loss: 0.000924096674613915\n",
      "epoch 500 | step 3 | loss: 0.0011922064328409251\n",
      "epoch 500 | step 4 | loss: 0.0014737034527655605\n",
      "epoch 500 | step 5 | loss: 0.00177254160923324\n",
      "epoch 500 | step 6 | loss: 0.002057471011048982\n",
      "epoch 500 | step 7 | loss: 0.0023718471296703394\n",
      "epoch 500 | step 8 | loss: 0.0026611104419029885\n",
      "epoch 500 | step 9 | loss: 0.002933355879805387\n",
      "epoch 500 | step 10 | loss: 0.00321733798325744\n",
      "epoch 500 | step 11 | loss: 0.003505309043902158\n",
      "epoch 501 | step 0 | loss: 0.00029780291729201714\n",
      "epoch 501 | step 1 | loss: 0.0005936357334624916\n",
      "epoch 501 | step 2 | loss: 0.0008919063566421274\n",
      "epoch 501 | step 3 | loss: 0.001176968877188247\n",
      "epoch 501 | step 4 | loss: 0.0014487981585618289\n",
      "epoch 501 | step 5 | loss: 0.0017529047473091755\n",
      "epoch 501 | step 6 | loss: 0.0020308038248714514\n",
      "epoch 501 | step 7 | loss: 0.002351481802060659\n",
      "epoch 501 | step 8 | loss: 0.0026233194061375787\n",
      "epoch 501 | step 9 | loss: 0.002912509274759989\n",
      "epoch 501 | step 10 | loss: 0.0032229703937884852\n",
      "epoch 501 | step 11 | loss: 0.003503001137801922\n",
      "epoch 502 | step 0 | loss: 0.00027228218486464713\n",
      "epoch 502 | step 1 | loss: 0.0005480594946524956\n",
      "epoch 502 | step 2 | loss: 0.0008497650281098525\n",
      "epoch 502 | step 3 | loss: 0.0011554388174534393\n",
      "epoch 502 | step 4 | loss: 0.0014218174636420194\n",
      "epoch 502 | step 5 | loss: 0.001720224208194144\n",
      "epoch 502 | step 6 | loss: 0.0020253967490288386\n",
      "epoch 502 | step 7 | loss: 0.0023440215871576345\n",
      "epoch 502 | step 8 | loss: 0.002617232797062848\n",
      "epoch 502 | step 9 | loss: 0.0029113555552783637\n",
      "epoch 502 | step 10 | loss: 0.0032022696083203787\n",
      "epoch 502 | step 11 | loss: 0.003510957402326829\n",
      "epoch 503 | step 0 | loss: 0.0002884988138041101\n",
      "epoch 503 | step 1 | loss: 0.0005830937243326202\n",
      "epoch 503 | step 2 | loss: 0.000863588820066339\n",
      "epoch 503 | step 3 | loss: 0.0011749089908866719\n",
      "epoch 503 | step 4 | loss: 0.001494855971993163\n",
      "epoch 503 | step 5 | loss: 0.0018064541709192112\n",
      "epoch 503 | step 6 | loss: 0.0020748330839483495\n",
      "epoch 503 | step 7 | loss: 0.0023543469854365087\n",
      "epoch 503 | step 8 | loss: 0.002652334969701724\n",
      "epoch 503 | step 9 | loss: 0.0029264895838083953\n",
      "epoch 503 | step 10 | loss: 0.003218311323623313\n",
      "epoch 503 | step 11 | loss: 0.003504843847105442\n",
      "epoch 504 | step 0 | loss: 0.00028961318806135764\n",
      "epoch 504 | step 1 | loss: 0.000593724133877254\n",
      "epoch 504 | step 2 | loss: 0.0008765696899611569\n",
      "epoch 504 | step 3 | loss: 0.001151940056852574\n",
      "epoch 504 | step 4 | loss: 0.0014463068900914928\n",
      "epoch 504 | step 5 | loss: 0.001755934043548684\n",
      "epoch 504 | step 6 | loss: 0.00202490861671233\n",
      "epoch 504 | step 7 | loss: 0.0023006421747533224\n",
      "epoch 504 | step 8 | loss: 0.002601742580705121\n",
      "epoch 504 | step 9 | loss: 0.0029066320096737116\n",
      "epoch 504 | step 10 | loss: 0.0032107500548874033\n",
      "epoch 504 | step 11 | loss: 0.003507366058457021\n",
      "epoch 505 | step 0 | loss: 0.0002973242913511271\n",
      "epoch 505 | step 1 | loss: 0.0005991389473203457\n",
      "epoch 505 | step 2 | loss: 0.0008883172011060773\n",
      "epoch 505 | step 3 | loss: 0.001191132768168604\n",
      "epoch 505 | step 4 | loss: 0.001468059084440371\n",
      "epoch 505 | step 5 | loss: 0.0017681535195655366\n",
      "epoch 505 | step 6 | loss: 0.002052320908789965\n",
      "epoch 505 | step 7 | loss: 0.0023257780509926464\n",
      "epoch 505 | step 8 | loss: 0.0026188624205345846\n",
      "epoch 505 | step 9 | loss: 0.0029138863374012927\n",
      "epoch 505 | step 10 | loss: 0.003215309805661734\n",
      "epoch 505 | step 11 | loss: 0.0035056424632262306\n",
      "epoch 506 | step 0 | loss: 0.00029687005390716906\n",
      "epoch 506 | step 1 | loss: 0.0006096857749918904\n",
      "epoch 506 | step 2 | loss: 0.0009009865094092266\n",
      "epoch 506 | step 3 | loss: 0.0011848723355025593\n",
      "epoch 506 | step 4 | loss: 0.0014574448760644805\n",
      "epoch 506 | step 5 | loss: 0.0017277573430605022\n",
      "epoch 506 | step 6 | loss: 0.0020292674871747645\n",
      "epoch 506 | step 7 | loss: 0.002321216732526205\n",
      "epoch 506 | step 8 | loss: 0.0026351562432217067\n",
      "epoch 506 | step 9 | loss: 0.0029288811195554002\n",
      "epoch 506 | step 10 | loss: 0.0032036209273838693\n",
      "epoch 506 | step 11 | loss: 0.003509968587501729\n",
      "epoch 507 | step 0 | loss: 0.00028655665321549855\n",
      "epoch 507 | step 1 | loss: 0.0005851417339533647\n",
      "epoch 507 | step 2 | loss: 0.0008722410807620472\n",
      "epoch 507 | step 3 | loss: 0.0011488798785773031\n",
      "epoch 507 | step 4 | loss: 0.0014161918784088398\n",
      "epoch 507 | step 5 | loss: 0.0017279873458098608\n",
      "epoch 507 | step 6 | loss: 0.002047707736096153\n",
      "epoch 507 | step 7 | loss: 0.002339194182918619\n",
      "epoch 507 | step 8 | loss: 0.0025965308249142893\n",
      "epoch 507 | step 9 | loss: 0.0029089261426971853\n",
      "epoch 507 | step 10 | loss: 0.0032146430342986053\n",
      "epoch 507 | step 11 | loss: 0.0035059159169779286\n",
      "epoch 508 | step 0 | loss: 0.00031037365661243686\n",
      "epoch 508 | step 1 | loss: 0.0006105791654684855\n",
      "epoch 508 | step 2 | loss: 0.0009099452158411834\n",
      "epoch 508 | step 3 | loss: 0.001173411526442104\n",
      "epoch 508 | step 4 | loss: 0.00148820713682698\n",
      "epoch 508 | step 5 | loss: 0.0017473515714100888\n",
      "epoch 508 | step 6 | loss: 0.002071390466267466\n",
      "epoch 508 | step 7 | loss: 0.0023506493040298276\n",
      "epoch 508 | step 8 | loss: 0.0026332202861884444\n",
      "epoch 508 | step 9 | loss: 0.002937458002897459\n",
      "epoch 508 | step 10 | loss: 0.0032093173545554186\n",
      "epoch 508 | step 11 | loss: 0.0035080956098769408\n",
      "epoch 509 | step 0 | loss: 0.00028798406520771263\n",
      "epoch 509 | step 1 | loss: 0.0005773230838555476\n",
      "epoch 509 | step 2 | loss: 0.0008551403865062375\n",
      "epoch 509 | step 3 | loss: 0.0011352304585768227\n",
      "epoch 509 | step 4 | loss: 0.0014339598095028678\n",
      "epoch 509 | step 5 | loss: 0.0017534196187738373\n",
      "epoch 509 | step 6 | loss: 0.0020470455897404175\n",
      "epoch 509 | step 7 | loss: 0.00230105231399844\n",
      "epoch 509 | step 8 | loss: 0.0026087329892515785\n",
      "epoch 509 | step 9 | loss: 0.002894401301297633\n",
      "epoch 509 | step 10 | loss: 0.0032132592671323635\n",
      "epoch 509 | step 11 | loss: 0.0035065317661223407\n",
      "epoch 510 | step 0 | loss: 0.0002867286256045262\n",
      "epoch 510 | step 1 | loss: 0.0005575673127326657\n",
      "epoch 510 | step 2 | loss: 0.000837882916606748\n",
      "epoch 510 | step 3 | loss: 0.0011395001514286122\n",
      "epoch 510 | step 4 | loss: 0.0014680633953963683\n",
      "epoch 510 | step 5 | loss: 0.0017475763369216614\n",
      "epoch 510 | step 6 | loss: 0.0020481274349473796\n",
      "epoch 510 | step 7 | loss: 0.002341387746909022\n",
      "epoch 510 | step 8 | loss: 0.0026371193714071215\n",
      "epoch 510 | step 9 | loss: 0.0029540555899255903\n",
      "epoch 510 | step 10 | loss: 0.0032322724081778045\n",
      "epoch 510 | step 11 | loss: 0.0034985740912246146\n",
      "epoch 511 | step 0 | loss: 0.000293856972983343\n",
      "epoch 511 | step 1 | loss: 0.0005719402873550688\n",
      "epoch 511 | step 2 | loss: 0.0008646952680156783\n",
      "epoch 511 | step 3 | loss: 0.0011867478319443454\n",
      "epoch 511 | step 4 | loss: 0.0014840208989496575\n",
      "epoch 511 | step 5 | loss: 0.0017744976554807557\n",
      "epoch 511 | step 6 | loss: 0.0020562608071228354\n",
      "epoch 511 | step 7 | loss: 0.0023204645433450443\n",
      "epoch 511 | step 8 | loss: 0.00262743288091634\n",
      "epoch 511 | step 9 | loss: 0.0029160510905947906\n",
      "epoch 511 | step 10 | loss: 0.0031977462296044422\n",
      "epoch 511 | step 11 | loss: 0.003512085128579637\n",
      "epoch 512 | step 0 | loss: 0.0002849109131237465\n",
      "epoch 512 | step 1 | loss: 0.0005668302341938101\n",
      "epoch 512 | step 2 | loss: 0.0008400820826672912\n",
      "epoch 512 | step 3 | loss: 0.001118093733820814\n",
      "epoch 512 | step 4 | loss: 0.0014302493122977313\n",
      "epoch 512 | step 5 | loss: 0.001727699271730563\n",
      "epoch 512 | step 6 | loss: 0.002024158349897056\n",
      "epoch 512 | step 7 | loss: 0.0023370001516545237\n",
      "epoch 512 | step 8 | loss: 0.0026332379223535833\n",
      "epoch 512 | step 9 | loss: 0.0029024639169711545\n",
      "epoch 512 | step 10 | loss: 0.0031966554997158213\n",
      "epoch 512 | step 11 | loss: 0.0035123212182113566\n",
      "epoch 513 | step 0 | loss: 0.00029234246902601466\n",
      "epoch 513 | step 1 | loss: 0.0005481210398489972\n",
      "epoch 513 | step 2 | loss: 0.0008102544983116511\n",
      "epoch 513 | step 3 | loss: 0.0010934385408744788\n",
      "epoch 513 | step 4 | loss: 0.0013842948632095107\n",
      "epoch 513 | step 5 | loss: 0.0017013103846460595\n",
      "epoch 513 | step 6 | loss: 0.002007437509539152\n",
      "epoch 513 | step 7 | loss: 0.002311325595790803\n",
      "epoch 513 | step 8 | loss: 0.002591703916549621\n",
      "epoch 513 | step 9 | loss: 0.002898511463868773\n",
      "epoch 513 | step 10 | loss: 0.003216864725558932\n",
      "epoch 513 | step 11 | loss: 0.0035041761254421184\n",
      "epoch 514 | step 0 | loss: 0.0003022975815432943\n",
      "epoch 514 | step 1 | loss: 0.0006037823277028921\n",
      "epoch 514 | step 2 | loss: 0.0009105106930672822\n",
      "epoch 514 | step 3 | loss: 0.0012273765802557465\n",
      "epoch 514 | step 4 | loss: 0.0015127836337919223\n",
      "epoch 514 | step 5 | loss: 0.0017839010020666679\n",
      "epoch 514 | step 6 | loss: 0.0020664018392833705\n",
      "epoch 514 | step 7 | loss: 0.0023360256414261247\n",
      "epoch 514 | step 8 | loss: 0.0026184107653780993\n",
      "epoch 514 | step 9 | loss: 0.0029103335846553413\n",
      "epoch 514 | step 10 | loss: 0.0032190169337148996\n",
      "epoch 514 | step 11 | loss: 0.0035033798288858485\n",
      "epoch 515 | step 0 | loss: 0.0003032925398405171\n",
      "epoch 515 | step 1 | loss: 0.0005788808646658151\n",
      "epoch 515 | step 2 | loss: 0.0008452197349154365\n",
      "epoch 515 | step 3 | loss: 0.0011399503387780553\n",
      "epoch 515 | step 4 | loss: 0.0014313468868578524\n",
      "epoch 515 | step 5 | loss: 0.001755488419914792\n",
      "epoch 515 | step 6 | loss: 0.0020588474838696045\n",
      "epoch 515 | step 7 | loss: 0.0023319310977584712\n",
      "epoch 515 | step 8 | loss: 0.0026313521703507587\n",
      "epoch 515 | step 9 | loss: 0.0029289482497173993\n",
      "epoch 515 | step 10 | loss: 0.0032151041980709033\n",
      "epoch 515 | step 11 | loss: 0.0035047210352764173\n",
      "epoch 516 | step 0 | loss: 0.0002878705187511277\n",
      "epoch 516 | step 1 | loss: 0.000593480633321088\n",
      "epoch 516 | step 2 | loss: 0.0008841376084824151\n",
      "epoch 516 | step 3 | loss: 0.0011694788713557257\n",
      "epoch 516 | step 4 | loss: 0.001441855839997162\n",
      "epoch 516 | step 5 | loss: 0.0017484849074837223\n",
      "epoch 516 | step 6 | loss: 0.002045711682491954\n",
      "epoch 516 | step 7 | loss: 0.0023549515172105098\n",
      "epoch 516 | step 8 | loss: 0.0026345706377068514\n",
      "epoch 516 | step 9 | loss: 0.002947928174635384\n",
      "epoch 516 | step 10 | loss: 0.0032313550321360503\n",
      "epoch 516 | step 11 | loss: 0.0034982716541946944\n",
      "epoch 517 | step 0 | loss: 0.00028923328068219945\n",
      "epoch 517 | step 1 | loss: 0.0005813695107741388\n",
      "epoch 517 | step 2 | loss: 0.0008703272041360403\n",
      "epoch 517 | step 3 | loss: 0.0011447723320246767\n",
      "epoch 517 | step 4 | loss: 0.0014458753218726664\n",
      "epoch 517 | step 5 | loss: 0.0017589674272689565\n",
      "epoch 517 | step 6 | loss: 0.002038400874179638\n",
      "epoch 517 | step 7 | loss: 0.0023429107000899575\n",
      "epoch 517 | step 8 | loss: 0.0026439069045607186\n",
      "epoch 517 | step 9 | loss: 0.002925555915128884\n",
      "epoch 517 | step 10 | loss: 0.003207109310821945\n",
      "epoch 517 | step 11 | loss: 0.003507714970394254\n",
      "epoch 518 | step 0 | loss: 0.0002905059538652074\n",
      "epoch 518 | step 1 | loss: 0.0005764101381518631\n",
      "epoch 518 | step 2 | loss: 0.0008735927590101519\n",
      "epoch 518 | step 3 | loss: 0.001171810194138419\n",
      "epoch 518 | step 4 | loss: 0.0014817489962100875\n",
      "epoch 518 | step 5 | loss: 0.001770840505267729\n",
      "epoch 518 | step 6 | loss: 0.0020631896586455717\n",
      "epoch 518 | step 7 | loss: 0.002344583957928939\n",
      "epoch 518 | step 8 | loss: 0.002626486630911272\n",
      "epoch 518 | step 9 | loss: 0.0029280099457457\n",
      "epoch 518 | step 10 | loss: 0.0032144309514654675\n",
      "epoch 518 | step 11 | loss: 0.0035050931260571564\n",
      "epoch 519 | step 0 | loss: 0.0002967105141879483\n",
      "epoch 519 | step 1 | loss: 0.000581632979091707\n",
      "epoch 519 | step 2 | loss: 0.0008619268310411042\n",
      "epoch 519 | step 3 | loss: 0.0011430622164272158\n",
      "epoch 519 | step 4 | loss: 0.0014426961222541903\n",
      "epoch 519 | step 5 | loss: 0.0017684362031474064\n",
      "epoch 519 | step 6 | loss: 0.002086771121762232\n",
      "epoch 519 | step 7 | loss: 0.002364836176318206\n",
      "epoch 519 | step 8 | loss: 0.002654347996812517\n",
      "epoch 519 | step 9 | loss: 0.0029274946475297465\n",
      "epoch 519 | step 10 | loss: 0.003192146285637995\n",
      "epoch 519 | step 11 | loss: 0.0035136204782358522\n",
      "epoch 520 | step 0 | loss: 0.0003085857941106318\n",
      "epoch 520 | step 1 | loss: 0.0006191072858019832\n",
      "epoch 520 | step 2 | loss: 0.0009181857055414524\n",
      "epoch 520 | step 3 | loss: 0.0012023108622537426\n",
      "epoch 520 | step 4 | loss: 0.001497510394926612\n",
      "epoch 520 | step 5 | loss: 0.0018027538333913623\n",
      "epoch 520 | step 6 | loss: 0.002079250497568958\n",
      "epoch 520 | step 7 | loss: 0.002368755679875082\n",
      "epoch 520 | step 8 | loss: 0.002633237586579869\n",
      "epoch 520 | step 9 | loss: 0.0029102536014554885\n",
      "epoch 520 | step 10 | loss: 0.003203498854684119\n",
      "epoch 520 | step 11 | loss: 0.0035089599893124977\n",
      "epoch 521 | step 0 | loss: 0.00030845279632806184\n",
      "epoch 521 | step 1 | loss: 0.0005996516575760196\n",
      "epoch 521 | step 2 | loss: 0.0009064460665121369\n",
      "epoch 521 | step 3 | loss: 0.0012118840524458058\n",
      "epoch 521 | step 4 | loss: 0.001504728790138141\n",
      "epoch 521 | step 5 | loss: 0.0017850127107155976\n",
      "epoch 521 | step 6 | loss: 0.002063033724972772\n",
      "epoch 521 | step 7 | loss: 0.002373515100472445\n",
      "epoch 521 | step 8 | loss: 0.0026580947100291368\n",
      "epoch 521 | step 9 | loss: 0.002942559755371895\n",
      "epoch 521 | step 10 | loss: 0.0032164225001620624\n",
      "epoch 521 | step 11 | loss: 0.0035040815203951576\n",
      "epoch 522 | step 0 | loss: 0.00027193960506995357\n",
      "epoch 522 | step 1 | loss: 0.0005904647258720844\n",
      "epoch 522 | step 2 | loss: 0.0008825977606009963\n",
      "epoch 522 | step 3 | loss: 0.0011631369180843565\n",
      "epoch 522 | step 4 | loss: 0.0014508944486040735\n",
      "epoch 522 | step 5 | loss: 0.0017303195830591634\n",
      "epoch 522 | step 6 | loss: 0.0020122630685299446\n",
      "epoch 522 | step 7 | loss: 0.002295584033216372\n",
      "epoch 522 | step 8 | loss: 0.002587233152929268\n",
      "epoch 522 | step 9 | loss: 0.0029003646850515735\n",
      "epoch 522 | step 10 | loss: 0.003206367361286551\n",
      "epoch 522 | step 11 | loss: 0.0035077927162205915\n",
      "epoch 523 | step 0 | loss: 0.00029760479989242886\n",
      "epoch 523 | step 1 | loss: 0.0006032647728918966\n",
      "epoch 523 | step 2 | loss: 0.0008759910867607211\n",
      "epoch 523 | step 3 | loss: 0.0011444040160378127\n",
      "epoch 523 | step 4 | loss: 0.0014290129067167508\n",
      "epoch 523 | step 5 | loss: 0.0017360629286763424\n",
      "epoch 523 | step 6 | loss: 0.0020255854325998625\n",
      "epoch 523 | step 7 | loss: 0.0023314143876276665\n",
      "epoch 523 | step 8 | loss: 0.0026225156784746217\n",
      "epoch 523 | step 9 | loss: 0.0028892528097714918\n",
      "epoch 523 | step 10 | loss: 0.003206883289014488\n",
      "epoch 523 | step 11 | loss: 0.003507519291140338\n",
      "epoch 524 | step 0 | loss: 0.00028452126362620527\n",
      "epoch 524 | step 1 | loss: 0.0006162075677546635\n",
      "epoch 524 | step 2 | loss: 0.0009168066031608549\n",
      "epoch 524 | step 3 | loss: 0.0012107988527654402\n",
      "epoch 524 | step 4 | loss: 0.0015289148227939568\n",
      "epoch 524 | step 5 | loss: 0.0017913322737328804\n",
      "epoch 524 | step 6 | loss: 0.0020624751272041754\n",
      "epoch 524 | step 7 | loss: 0.0023434378421426753\n",
      "epoch 524 | step 8 | loss: 0.0026311778337136577\n",
      "epoch 524 | step 9 | loss: 0.0029409395364201505\n",
      "epoch 524 | step 10 | loss: 0.0032176514388242315\n",
      "epoch 524 | step 11 | loss: 0.0035030078646165197\n",
      "epoch 525 | step 0 | loss: 0.00033794750594076824\n",
      "epoch 525 | step 1 | loss: 0.0006056716732077244\n",
      "epoch 525 | step 2 | loss: 0.0008736421762829882\n",
      "epoch 525 | step 3 | loss: 0.0011520493401625865\n",
      "epoch 525 | step 4 | loss: 0.0014473857861785997\n",
      "epoch 525 | step 5 | loss: 0.0017566770185865916\n",
      "epoch 525 | step 6 | loss: 0.002051122886414011\n",
      "epoch 525 | step 7 | loss: 0.0023417056512769923\n",
      "epoch 525 | step 8 | loss: 0.002620200537517183\n",
      "epoch 525 | step 9 | loss: 0.0029396608096942437\n",
      "epoch 525 | step 10 | loss: 0.003234439360160074\n",
      "epoch 525 | step 11 | loss: 0.003496325169962284\n",
      "epoch 526 | step 0 | loss: 0.0003118706712224786\n",
      "epoch 526 | step 1 | loss: 0.0006078353603691047\n",
      "epoch 526 | step 2 | loss: 0.0008752869325911433\n",
      "epoch 526 | step 3 | loss: 0.0011851501155926212\n",
      "epoch 526 | step 4 | loss: 0.0014869537623121935\n",
      "epoch 526 | step 5 | loss: 0.0017681388331150384\n",
      "epoch 526 | step 6 | loss: 0.0020570895592004384\n",
      "epoch 526 | step 7 | loss: 0.002371603668406626\n",
      "epoch 526 | step 8 | loss: 0.002642957257451659\n",
      "epoch 526 | step 9 | loss: 0.0029436847490278075\n",
      "epoch 526 | step 10 | loss: 0.003211707280419622\n",
      "epoch 526 | step 11 | loss: 0.003505071302075792\n",
      "epoch 527 | step 0 | loss: 0.00027762058157100567\n",
      "epoch 527 | step 1 | loss: 0.00057726858138834\n",
      "epoch 527 | step 2 | loss: 0.0008845428100869957\n",
      "epoch 527 | step 3 | loss: 0.001152146908513002\n",
      "epoch 527 | step 4 | loss: 0.0014715332302451607\n",
      "epoch 527 | step 5 | loss: 0.0017506569751240847\n",
      "epoch 527 | step 6 | loss: 0.0020294647304407647\n",
      "epoch 527 | step 7 | loss: 0.0023309576605948226\n",
      "epoch 527 | step 8 | loss: 0.002623572682693303\n",
      "epoch 527 | step 9 | loss: 0.0029226117223742415\n",
      "epoch 527 | step 10 | loss: 0.0032179341340468222\n",
      "epoch 527 | step 11 | loss: 0.0035026601817994135\n",
      "epoch 528 | step 0 | loss: 0.0002861740927379266\n",
      "epoch 528 | step 1 | loss: 0.000603396947142003\n",
      "epoch 528 | step 2 | loss: 0.0008976136518160932\n",
      "epoch 528 | step 3 | loss: 0.0011676953344514375\n",
      "epoch 528 | step 4 | loss: 0.0014565587316271562\n",
      "epoch 528 | step 5 | loss: 0.0017408602822280178\n",
      "epoch 528 | step 6 | loss: 0.0020017653372379905\n",
      "epoch 528 | step 7 | loss: 0.002295221533446916\n",
      "epoch 528 | step 8 | loss: 0.0026027616433607517\n",
      "epoch 528 | step 9 | loss: 0.0029088824129717807\n",
      "epoch 528 | step 10 | loss: 0.003219083663738238\n",
      "epoch 528 | step 11 | loss: 0.0035021774983914023\n",
      "epoch 529 | step 0 | loss: 0.00027756529747704535\n",
      "epoch 529 | step 1 | loss: 0.0005949241053701707\n",
      "epoch 529 | step 2 | loss: 0.0009011822601055197\n",
      "epoch 529 | step 3 | loss: 0.0012021489944975198\n",
      "epoch 529 | step 4 | loss: 0.0014864276653590062\n",
      "epoch 529 | step 5 | loss: 0.001790092558809454\n",
      "epoch 529 | step 6 | loss: 0.0020878235464663413\n",
      "epoch 529 | step 7 | loss: 0.0023549953402073566\n",
      "epoch 529 | step 8 | loss: 0.0026310834288157474\n",
      "epoch 529 | step 9 | loss: 0.0029300685361224633\n",
      "epoch 529 | step 10 | loss: 0.0032208680062708536\n",
      "epoch 529 | step 11 | loss: 0.0035013644330370046\n",
      "epoch 530 | step 0 | loss: 0.00030522085108300223\n",
      "epoch 530 | step 1 | loss: 0.0005901812230226407\n",
      "epoch 530 | step 2 | loss: 0.0008972533741648107\n",
      "epoch 530 | step 3 | loss: 0.001197747690533687\n",
      "epoch 530 | step 4 | loss: 0.001464172883567256\n",
      "epoch 530 | step 5 | loss: 0.0017517110276948078\n",
      "epoch 530 | step 6 | loss: 0.0020462710753972177\n",
      "epoch 530 | step 7 | loss: 0.0023312231755630267\n",
      "epoch 530 | step 8 | loss: 0.0026100255503477914\n",
      "epoch 530 | step 9 | loss: 0.0029081613144625804\n",
      "epoch 530 | step 10 | loss: 0.0032232966904898854\n",
      "epoch 530 | step 11 | loss: 0.0035001869686122435\n",
      "epoch 531 | step 0 | loss: 0.00029358334466511696\n",
      "epoch 531 | step 1 | loss: 0.000597557393614\n",
      "epoch 531 | step 2 | loss: 0.0008782111648293175\n",
      "epoch 531 | step 3 | loss: 0.0011777536970184988\n",
      "epoch 531 | step 4 | loss: 0.001466429619225875\n",
      "epoch 531 | step 5 | loss: 0.0017333725334098527\n",
      "epoch 531 | step 6 | loss: 0.0020348386013759358\n",
      "epoch 531 | step 7 | loss: 0.00231497569323634\n",
      "epoch 531 | step 8 | loss: 0.0025921896370100527\n",
      "epoch 531 | step 9 | loss: 0.0029035444886783787\n",
      "epoch 531 | step 10 | loss: 0.0031970976843527046\n",
      "epoch 531 | step 11 | loss: 0.0035104435117667724\n",
      "epoch 532 | step 0 | loss: 0.000284250367864312\n",
      "epoch 532 | step 1 | loss: 0.0005680266544350217\n",
      "epoch 532 | step 2 | loss: 0.0008483751718009781\n",
      "epoch 532 | step 3 | loss: 0.0011373993562335367\n",
      "epoch 532 | step 4 | loss: 0.0013786332439178143\n",
      "epoch 532 | step 5 | loss: 0.0017076375291370527\n",
      "epoch 532 | step 6 | loss: 0.002014825235987354\n",
      "epoch 532 | step 7 | loss: 0.0023012623800454336\n",
      "epoch 532 | step 8 | loss: 0.002619367857394116\n",
      "epoch 532 | step 9 | loss: 0.0029335138859273913\n",
      "epoch 532 | step 10 | loss: 0.003232347652572294\n",
      "epoch 532 | step 11 | loss: 0.0034967594739332606\n",
      "epoch 533 | step 0 | loss: 0.0002999662666528562\n",
      "epoch 533 | step 1 | loss: 0.0005851167004215882\n",
      "epoch 533 | step 2 | loss: 0.0008817616598861186\n",
      "epoch 533 | step 3 | loss: 0.0011704933401702955\n",
      "epoch 533 | step 4 | loss: 0.0014576018841309922\n",
      "epoch 533 | step 5 | loss: 0.0017681282798198959\n",
      "epoch 533 | step 6 | loss: 0.0020533009062371757\n",
      "epoch 533 | step 7 | loss: 0.0023283312386317796\n",
      "epoch 533 | step 8 | loss: 0.0026204429161928475\n",
      "epoch 533 | step 9 | loss: 0.0029309489423660785\n",
      "epoch 533 | step 10 | loss: 0.003216876969469577\n",
      "epoch 533 | step 11 | loss: 0.003502509677488135\n",
      "epoch 534 | step 0 | loss: 0.0003111624013811729\n",
      "epoch 534 | step 1 | loss: 0.0005987132415892626\n",
      "epoch 534 | step 2 | loss: 0.0008734771650587024\n",
      "epoch 534 | step 3 | loss: 0.001186296241099823\n",
      "epoch 534 | step 4 | loss: 0.0014673253118954708\n",
      "epoch 534 | step 5 | loss: 0.0017419854217067931\n",
      "epoch 534 | step 6 | loss: 0.002074801480442511\n",
      "epoch 534 | step 7 | loss: 0.002359537493992316\n",
      "epoch 534 | step 8 | loss: 0.0026460591427435037\n",
      "epoch 534 | step 9 | loss: 0.002906516847296366\n",
      "epoch 534 | step 10 | loss: 0.003206923990310075\n",
      "epoch 534 | step 11 | loss: 0.003506273233997565\n",
      "epoch 535 | step 0 | loss: 0.00030033919479097155\n",
      "epoch 535 | step 1 | loss: 0.0005906283472913274\n",
      "epoch 535 | step 2 | loss: 0.0008724217010335067\n",
      "epoch 535 | step 3 | loss: 0.0011460754843099163\n",
      "epoch 535 | step 4 | loss: 0.0014535400122606828\n",
      "epoch 535 | step 5 | loss: 0.0017609479880664543\n",
      "epoch 535 | step 6 | loss: 0.0020434372016849755\n",
      "epoch 535 | step 7 | loss: 0.002313856103292996\n",
      "epoch 535 | step 8 | loss: 0.002595070962002214\n",
      "epoch 535 | step 9 | loss: 0.0028898465554498174\n",
      "epoch 535 | step 10 | loss: 0.0031941089632953673\n",
      "epoch 535 | step 11 | loss: 0.0035112420858368085\n",
      "epoch 536 | step 0 | loss: 0.0003185598086929748\n",
      "epoch 536 | step 1 | loss: 0.0006103243091785147\n",
      "epoch 536 | step 2 | loss: 0.0009031774320708106\n",
      "epoch 536 | step 3 | loss: 0.0011963292943352674\n",
      "epoch 536 | step 4 | loss: 0.0014888160667966728\n",
      "epoch 536 | step 5 | loss: 0.0017735506365120783\n",
      "epoch 536 | step 6 | loss: 0.0020496298540155747\n",
      "epoch 536 | step 7 | loss: 0.002334546801270876\n",
      "epoch 536 | step 8 | loss: 0.002629713491265589\n",
      "epoch 536 | step 9 | loss: 0.0029054589041876966\n",
      "epoch 536 | step 10 | loss: 0.003194730437305238\n",
      "epoch 536 | step 11 | loss: 0.0035113189517235223\n",
      "epoch 537 | step 0 | loss: 0.0003042485025735585\n",
      "epoch 537 | step 1 | loss: 0.0006129315352226749\n",
      "epoch 537 | step 2 | loss: 0.0009078233566958058\n",
      "epoch 537 | step 3 | loss: 0.0012070351817113681\n",
      "epoch 537 | step 4 | loss: 0.0014958911891019729\n",
      "epoch 537 | step 5 | loss: 0.0017818801172613664\n",
      "epoch 537 | step 6 | loss: 0.0020782898401224843\n",
      "epoch 537 | step 7 | loss: 0.0023457828999072943\n",
      "epoch 537 | step 8 | loss: 0.0026311292701222356\n",
      "epoch 537 | step 9 | loss: 0.002937107988372607\n",
      "epoch 537 | step 10 | loss: 0.003211763269788735\n",
      "epoch 537 | step 11 | loss: 0.0035042967190261377\n",
      "epoch 538 | step 0 | loss: 0.0002856403364380364\n",
      "epoch 538 | step 1 | loss: 0.0006130973149272116\n",
      "epoch 538 | step 2 | loss: 0.0008850942812637458\n",
      "epoch 538 | step 3 | loss: 0.0011687224563064612\n",
      "epoch 538 | step 4 | loss: 0.0014315597790315142\n",
      "epoch 538 | step 5 | loss: 0.001750962284377093\n",
      "epoch 538 | step 6 | loss: 0.0020441893845200856\n",
      "epoch 538 | step 7 | loss: 0.0023556405747047096\n",
      "epoch 538 | step 8 | loss: 0.002647694282486876\n",
      "epoch 538 | step 9 | loss: 0.0029442967462372546\n",
      "epoch 538 | step 10 | loss: 0.003214379199797463\n",
      "epoch 538 | step 11 | loss: 0.0035033006590194015\n",
      "epoch 539 | step 0 | loss: 0.00027712166649548057\n",
      "epoch 539 | step 1 | loss: 0.0005921309852293999\n",
      "epoch 539 | step 2 | loss: 0.0008745591308760096\n",
      "epoch 539 | step 3 | loss: 0.0011410697874218501\n",
      "epoch 539 | step 4 | loss: 0.0014428233547339592\n",
      "epoch 539 | step 5 | loss: 0.001746268502714044\n",
      "epoch 539 | step 6 | loss: 0.002017568906777756\n",
      "epoch 539 | step 7 | loss: 0.002302492077290489\n",
      "epoch 539 | step 8 | loss: 0.0026158427960802713\n",
      "epoch 539 | step 9 | loss: 0.0029167416310542085\n",
      "epoch 539 | step 10 | loss: 0.003213926860875157\n",
      "epoch 539 | step 11 | loss: 0.0035035937129969065\n",
      "epoch 540 | step 0 | loss: 0.0003061213848948581\n",
      "epoch 540 | step 1 | loss: 0.0005831376594189057\n",
      "epoch 540 | step 2 | loss: 0.0008674210755970406\n",
      "epoch 540 | step 3 | loss: 0.0011962723173326148\n",
      "epoch 540 | step 4 | loss: 0.0014760303890112086\n",
      "epoch 540 | step 5 | loss: 0.0017728250520490985\n",
      "epoch 540 | step 6 | loss: 0.0020904981610302313\n",
      "epoch 540 | step 7 | loss: 0.0023957326524683955\n",
      "epoch 540 | step 8 | loss: 0.0026737153403768653\n",
      "epoch 540 | step 9 | loss: 0.0029489210118560137\n",
      "epoch 540 | step 10 | loss: 0.003224468197499339\n",
      "epoch 540 | step 11 | loss: 0.0034993996058911396\n",
      "epoch 541 | step 0 | loss: 0.0002764919599957772\n",
      "epoch 541 | step 1 | loss: 0.0005428823233534332\n",
      "epoch 541 | step 2 | loss: 0.0008566886669951923\n",
      "epoch 541 | step 3 | loss: 0.0011824507006800503\n",
      "epoch 541 | step 4 | loss: 0.001472238474064948\n",
      "epoch 541 | step 5 | loss: 0.0017705650761795744\n",
      "epoch 541 | step 6 | loss: 0.002038130508787472\n",
      "epoch 541 | step 7 | loss: 0.002330685682748708\n",
      "epoch 541 | step 8 | loss: 0.0026427946101278852\n",
      "epoch 541 | step 9 | loss: 0.0029363239482447013\n",
      "epoch 541 | step 10 | loss: 0.0032449254937646373\n",
      "epoch 541 | step 11 | loss: 0.003490943212793173\n",
      "epoch 542 | step 0 | loss: 0.0002910801295211731\n",
      "epoch 542 | step 1 | loss: 0.0006065214152312277\n",
      "epoch 542 | step 2 | loss: 0.0008729678099960641\n",
      "epoch 542 | step 3 | loss: 0.0011597383440423569\n",
      "epoch 542 | step 4 | loss: 0.001453009390651758\n",
      "epoch 542 | step 5 | loss: 0.001751491250799843\n",
      "epoch 542 | step 6 | loss: 0.002069579417403293\n",
      "epoch 542 | step 7 | loss: 0.0023655165222347986\n",
      "epoch 542 | step 8 | loss: 0.0026444298725176968\n",
      "epoch 542 | step 9 | loss: 0.002915545178300087\n",
      "epoch 542 | step 10 | loss: 0.003218579866739072\n",
      "epoch 542 | step 11 | loss: 0.0035012630743836656\n",
      "epoch 543 | step 0 | loss: 0.0002896067884712432\n",
      "epoch 543 | step 1 | loss: 0.0005623702422183147\n",
      "epoch 543 | step 2 | loss: 0.0008843155650086976\n",
      "epoch 543 | step 3 | loss: 0.0011580676538390225\n",
      "epoch 543 | step 4 | loss: 0.0014619921450944155\n",
      "epoch 543 | step 5 | loss: 0.0017596884742719326\n",
      "epoch 543 | step 6 | loss: 0.0020354430045414263\n",
      "epoch 543 | step 7 | loss: 0.0023290331646981014\n",
      "epoch 543 | step 8 | loss: 0.0026159911011392656\n",
      "epoch 543 | step 9 | loss: 0.0029246085028487717\n",
      "epoch 543 | step 10 | loss: 0.003226138927928738\n",
      "epoch 543 | step 11 | loss: 0.0034979894563481684\n",
      "epoch 544 | step 0 | loss: 0.00028710851758823963\n",
      "epoch 544 | step 1 | loss: 0.0005654347699066168\n",
      "epoch 544 | step 2 | loss: 0.0008725553432953658\n",
      "epoch 544 | step 3 | loss: 0.001147276633442146\n",
      "epoch 544 | step 4 | loss: 0.0013963696270319751\n",
      "epoch 544 | step 5 | loss: 0.0017002780103265447\n",
      "epoch 544 | step 6 | loss: 0.002013777362452634\n",
      "epoch 544 | step 7 | loss: 0.002326056145220244\n",
      "epoch 544 | step 8 | loss: 0.002597998998908644\n",
      "epoch 544 | step 9 | loss: 0.0029361595390357603\n",
      "epoch 544 | step 10 | loss: 0.00321137810154299\n",
      "epoch 544 | step 11 | loss: 0.0035037919569295833\n",
      "epoch 545 | step 0 | loss: 0.00028222860836455766\n",
      "epoch 545 | step 1 | loss: 0.0005662389221066852\n",
      "epoch 545 | step 2 | loss: 0.0008614972179414709\n",
      "epoch 545 | step 3 | loss: 0.0011598701126839996\n",
      "epoch 545 | step 4 | loss: 0.0014347065518722328\n",
      "epoch 545 | step 5 | loss: 0.0017273026069817808\n",
      "epoch 545 | step 6 | loss: 0.002013003131825309\n",
      "epoch 545 | step 7 | loss: 0.0022971432412160963\n",
      "epoch 545 | step 8 | loss: 0.0025977086005674276\n",
      "epoch 545 | step 9 | loss: 0.002877318525283725\n",
      "epoch 545 | step 10 | loss: 0.003206491663031224\n",
      "epoch 545 | step 11 | loss: 0.0035056261669408977\n",
      "epoch 546 | step 0 | loss: 0.0002829208097743096\n",
      "epoch 546 | step 1 | loss: 0.0006036641602512344\n",
      "epoch 546 | step 2 | loss: 0.0009131029521708828\n",
      "epoch 546 | step 3 | loss: 0.0012028993978006725\n",
      "epoch 546 | step 4 | loss: 0.0014805964502093466\n",
      "epoch 546 | step 5 | loss: 0.001761336596765465\n",
      "epoch 546 | step 6 | loss: 0.002020062088205643\n",
      "epoch 546 | step 7 | loss: 0.002338511834926633\n",
      "epoch 546 | step 8 | loss: 0.0026376420094616623\n",
      "epoch 546 | step 9 | loss: 0.002940559352966659\n",
      "epoch 546 | step 10 | loss: 0.0032351901179941953\n",
      "epoch 546 | step 11 | loss: 0.00349414693340776\n",
      "epoch 547 | step 0 | loss: 0.0002981646171987932\n",
      "epoch 547 | step 1 | loss: 0.0006021631662290938\n",
      "epoch 547 | step 2 | loss: 0.0008903643656457712\n",
      "epoch 547 | step 3 | loss: 0.0011865322274829371\n",
      "epoch 547 | step 4 | loss: 0.0014859970690659914\n",
      "epoch 547 | step 5 | loss: 0.0018035747178583098\n",
      "epoch 547 | step 6 | loss: 0.0021028129759569454\n",
      "epoch 547 | step 7 | loss: 0.0023756067896276143\n",
      "epoch 547 | step 8 | loss: 0.0026453552438254965\n",
      "epoch 547 | step 9 | loss: 0.0029226632951623427\n",
      "epoch 547 | step 10 | loss: 0.003227368852649723\n",
      "epoch 547 | step 11 | loss: 0.0034973942261603856\n",
      "epoch 548 | step 0 | loss: 0.00026671223130660517\n",
      "epoch 548 | step 1 | loss: 0.0005735780381948671\n",
      "epoch 548 | step 2 | loss: 0.0008883987405053266\n",
      "epoch 548 | step 3 | loss: 0.0011821225092545555\n",
      "epoch 548 | step 4 | loss: 0.0014778689393431365\n",
      "epoch 548 | step 5 | loss: 0.001787201948271275\n",
      "epoch 548 | step 6 | loss: 0.002031719035284498\n",
      "epoch 548 | step 7 | loss: 0.002289767724545993\n",
      "epoch 548 | step 8 | loss: 0.002596621651688076\n",
      "epoch 548 | step 9 | loss: 0.0028815139144026634\n",
      "epoch 548 | step 10 | loss: 0.0032172608484467833\n",
      "epoch 548 | step 11 | loss: 0.0035010045511487667\n",
      "epoch 549 | step 0 | loss: 0.0003057965859434717\n",
      "epoch 549 | step 1 | loss: 0.0005738299244390716\n",
      "epoch 549 | step 2 | loss: 0.0008841757802747026\n",
      "epoch 549 | step 3 | loss: 0.0011532649819455895\n",
      "epoch 549 | step 4 | loss: 0.001452146975295099\n",
      "epoch 549 | step 5 | loss: 0.001758272089773969\n",
      "epoch 549 | step 6 | loss: 0.0020570769314123137\n",
      "epoch 549 | step 7 | loss: 0.0023572308751676164\n",
      "epoch 549 | step 8 | loss: 0.002657544765008633\n",
      "epoch 549 | step 9 | loss: 0.0029312934617217535\n",
      "epoch 549 | step 10 | loss: 0.003223575802684979\n",
      "epoch 549 | step 11 | loss: 0.00349839239885162\n",
      "epoch 550 | step 0 | loss: 0.00029665411252898535\n",
      "epoch 550 | step 1 | loss: 0.0005895323880745551\n",
      "epoch 550 | step 2 | loss: 0.0008841557619529197\n",
      "epoch 550 | step 3 | loss: 0.0011568400255279583\n",
      "epoch 550 | step 4 | loss: 0.0014500926852878868\n",
      "epoch 550 | step 5 | loss: 0.001748375118350027\n",
      "epoch 550 | step 6 | loss: 0.0020484855776385764\n",
      "epoch 550 | step 7 | loss: 0.0023492422816428623\n",
      "epoch 550 | step 8 | loss: 0.0026483539889762867\n",
      "epoch 550 | step 9 | loss: 0.002938090923294386\n",
      "epoch 550 | step 10 | loss: 0.003204381497873375\n",
      "epoch 550 | step 11 | loss: 0.003506203901849459\n",
      "epoch 551 | step 0 | loss: 0.000300253274579942\n",
      "epoch 551 | step 1 | loss: 0.0005944105786762253\n",
      "epoch 551 | step 2 | loss: 0.0008717092868854515\n",
      "epoch 551 | step 3 | loss: 0.001160867426343581\n",
      "epoch 551 | step 4 | loss: 0.0014321344214142779\n",
      "epoch 551 | step 5 | loss: 0.0017166427560584654\n",
      "epoch 551 | step 6 | loss: 0.002018655785930849\n",
      "epoch 551 | step 7 | loss: 0.002305958514273161\n",
      "epoch 551 | step 8 | loss: 0.0026286440844215343\n",
      "epoch 551 | step 9 | loss: 0.0029315061564859214\n",
      "epoch 551 | step 10 | loss: 0.0032117448687857494\n",
      "epoch 551 | step 11 | loss: 0.003502811804857682\n",
      "epoch 552 | step 0 | loss: 0.0003130931571786533\n",
      "epoch 552 | step 1 | loss: 0.0006089437887870028\n",
      "epoch 552 | step 2 | loss: 0.0008837564353430534\n",
      "epoch 552 | step 3 | loss: 0.0011862017592262231\n",
      "epoch 552 | step 4 | loss: 0.00145124503682088\n",
      "epoch 552 | step 5 | loss: 0.0017502291787464673\n",
      "epoch 552 | step 6 | loss: 0.002045936702165339\n",
      "epoch 552 | step 7 | loss: 0.002328775533791116\n",
      "epoch 552 | step 8 | loss: 0.002609492051023021\n",
      "epoch 552 | step 9 | loss: 0.002927687666185056\n",
      "epoch 552 | step 10 | loss: 0.003217160557991034\n",
      "epoch 552 | step 11 | loss: 0.0035006215588966583\n",
      "epoch 553 | step 0 | loss: 0.00030415326698198517\n",
      "epoch 553 | step 1 | loss: 0.0005779082165339067\n",
      "epoch 553 | step 2 | loss: 0.0008944123225418059\n",
      "epoch 553 | step 3 | loss: 0.0011718929351611992\n",
      "epoch 553 | step 4 | loss: 0.0014424492422014687\n",
      "epoch 553 | step 5 | loss: 0.0017242863864064643\n",
      "epoch 553 | step 6 | loss: 0.002002797086686036\n",
      "epoch 553 | step 7 | loss: 0.002282171513052015\n",
      "epoch 553 | step 8 | loss: 0.002575137702752831\n",
      "epoch 553 | step 9 | loss: 0.002899908088352531\n",
      "epoch 553 | step 10 | loss: 0.0032160889297157312\n",
      "epoch 553 | step 11 | loss: 0.003501001723214692\n",
      "epoch 554 | step 0 | loss: 0.0002890921462067818\n",
      "epoch 554 | step 1 | loss: 0.000564164246495801\n",
      "epoch 554 | step 2 | loss: 0.0008631794436868628\n",
      "epoch 554 | step 3 | loss: 0.0011603185380846665\n",
      "epoch 554 | step 4 | loss: 0.0014611609545695154\n",
      "epoch 554 | step 5 | loss: 0.0017397363413690213\n",
      "epoch 554 | step 6 | loss: 0.0020439068477042335\n",
      "epoch 554 | step 7 | loss: 0.0023643804760824817\n",
      "epoch 554 | step 8 | loss: 0.0026215021329536793\n",
      "epoch 554 | step 9 | loss: 0.0029159702862806987\n",
      "epoch 554 | step 10 | loss: 0.0032199848935211945\n",
      "epoch 554 | step 11 | loss: 0.003499448956143309\n",
      "epoch 555 | step 0 | loss: 0.00029342591797130985\n",
      "epoch 555 | step 1 | loss: 0.0005922453940095452\n",
      "epoch 555 | step 2 | loss: 0.0008714821385489718\n",
      "epoch 555 | step 3 | loss: 0.001159142933083362\n",
      "epoch 555 | step 4 | loss: 0.0014595739247476364\n",
      "epoch 555 | step 5 | loss: 0.0017378055863394887\n",
      "epoch 555 | step 6 | loss: 0.002046451155132028\n",
      "epoch 555 | step 7 | loss: 0.0023630219189570423\n",
      "epoch 555 | step 8 | loss: 0.002655334370075915\n",
      "epoch 555 | step 9 | loss: 0.00295653448171934\n",
      "epoch 555 | step 10 | loss: 0.0032230705993256\n",
      "epoch 555 | step 11 | loss: 0.0034980972246597707\n",
      "epoch 556 | step 0 | loss: 0.0003072062706123378\n",
      "epoch 556 | step 1 | loss: 0.0005859302777876169\n",
      "epoch 556 | step 2 | loss: 0.0008962468144336443\n",
      "epoch 556 | step 3 | loss: 0.0011934497933453595\n",
      "epoch 556 | step 4 | loss: 0.0014935029346427456\n",
      "epoch 556 | step 5 | loss: 0.0017772038221946858\n",
      "epoch 556 | step 6 | loss: 0.0020622614137269877\n",
      "epoch 556 | step 7 | loss: 0.002356552883700052\n",
      "epoch 556 | step 8 | loss: 0.0026580877375546597\n",
      "epoch 556 | step 9 | loss: 0.0029332164642813926\n",
      "epoch 556 | step 10 | loss: 0.0032019431686600083\n",
      "epoch 556 | step 11 | loss: 0.0035066167416064075\n",
      "epoch 557 | step 0 | loss: 0.00028282545114173996\n",
      "epoch 557 | step 1 | loss: 0.000569118120351046\n",
      "epoch 557 | step 2 | loss: 0.000869301402329987\n",
      "epoch 557 | step 3 | loss: 0.0011673940776854577\n",
      "epoch 557 | step 4 | loss: 0.0014790221995528078\n",
      "epoch 557 | step 5 | loss: 0.001788227035289176\n",
      "epoch 557 | step 6 | loss: 0.0020963297030103093\n",
      "epoch 557 | step 7 | loss: 0.002377304695417919\n",
      "epoch 557 | step 8 | loss: 0.002643369075889271\n",
      "epoch 557 | step 9 | loss: 0.00294286775438397\n",
      "epoch 557 | step 10 | loss: 0.003226940080968817\n",
      "epoch 557 | step 11 | loss: 0.003496611890563943\n",
      "epoch 558 | step 0 | loss: 0.0002874618634748084\n",
      "epoch 558 | step 1 | loss: 0.0005836953147528972\n",
      "epoch 558 | step 2 | loss: 0.0008844985081122436\n",
      "epoch 558 | step 3 | loss: 0.0011546582839371802\n",
      "epoch 558 | step 4 | loss: 0.0014436832199659923\n",
      "epoch 558 | step 5 | loss: 0.0017198469946218306\n",
      "epoch 558 | step 6 | loss: 0.0020075392063206906\n",
      "epoch 558 | step 7 | loss: 0.002280516315954545\n",
      "epoch 558 | step 8 | loss: 0.0025927301269139435\n",
      "epoch 558 | step 9 | loss: 0.002904060206404924\n",
      "epoch 558 | step 10 | loss: 0.0032216239058415496\n",
      "epoch 558 | step 11 | loss: 0.003498633267761217\n",
      "epoch 559 | step 0 | loss: 0.00031623965021554737\n",
      "epoch 559 | step 1 | loss: 0.0006074571719461889\n",
      "epoch 559 | step 2 | loss: 0.0008866118452154064\n",
      "epoch 559 | step 3 | loss: 0.0011547265445739177\n",
      "epoch 559 | step 4 | loss: 0.0014567779995952469\n",
      "epoch 559 | step 5 | loss: 0.0017379315311420013\n",
      "epoch 559 | step 6 | loss: 0.002005184668060548\n",
      "epoch 559 | step 7 | loss: 0.0023018303476613844\n",
      "epoch 559 | step 8 | loss: 0.0025955707655178745\n",
      "epoch 559 | step 9 | loss: 0.002900762614648402\n",
      "epoch 559 | step 10 | loss: 0.0031796737387321168\n",
      "epoch 559 | step 11 | loss: 0.003514693625735932\n",
      "epoch 560 | step 0 | loss: 0.0002634004010073043\n",
      "epoch 560 | step 1 | loss: 0.0005556335455839824\n",
      "epoch 560 | step 2 | loss: 0.0008748401742471993\n",
      "epoch 560 | step 3 | loss: 0.0011529697814475226\n",
      "epoch 560 | step 4 | loss: 0.0014414904373987857\n",
      "epoch 560 | step 5 | loss: 0.001735944810674719\n",
      "epoch 560 | step 6 | loss: 0.002037520080956731\n",
      "epoch 560 | step 7 | loss: 0.0023259996006024576\n",
      "epoch 560 | step 8 | loss: 0.002614049413603891\n",
      "epoch 560 | step 9 | loss: 0.0029258694578623524\n",
      "epoch 560 | step 10 | loss: 0.0032155287788846144\n",
      "epoch 560 | step 11 | loss: 0.003500559038513671\n",
      "epoch 561 | step 0 | loss: 0.000292233031546786\n",
      "epoch 561 | step 1 | loss: 0.0006140016997090347\n",
      "epoch 561 | step 2 | loss: 0.0009080685272300221\n",
      "epoch 561 | step 3 | loss: 0.001170607584894782\n",
      "epoch 561 | step 4 | loss: 0.0014478484611435626\n",
      "epoch 561 | step 5 | loss: 0.001759162013845753\n",
      "epoch 561 | step 6 | loss: 0.0020590222292364625\n",
      "epoch 561 | step 7 | loss: 0.002359520856677614\n",
      "epoch 561 | step 8 | loss: 0.0026659682828017615\n",
      "epoch 561 | step 9 | loss: 0.0029688958905887157\n",
      "epoch 561 | step 10 | loss: 0.00323074339115047\n",
      "epoch 561 | step 11 | loss: 0.0034946943041255806\n",
      "epoch 562 | step 0 | loss: 0.0002772807677027268\n",
      "epoch 562 | step 1 | loss: 0.0005663770514587817\n",
      "epoch 562 | step 2 | loss: 0.0008950095129679293\n",
      "epoch 562 | step 3 | loss: 0.001165064352808975\n",
      "epoch 562 | step 4 | loss: 0.001460358240867612\n",
      "epoch 562 | step 5 | loss: 0.0017409212827497473\n",
      "epoch 562 | step 6 | loss: 0.0020305929563698135\n",
      "epoch 562 | step 7 | loss: 0.0023235793694502595\n",
      "epoch 562 | step 8 | loss: 0.002605420981532775\n",
      "epoch 562 | step 9 | loss: 0.002919273769136378\n",
      "epoch 562 | step 10 | loss: 0.003219871669434161\n",
      "epoch 562 | step 11 | loss: 0.0034987059550274846\n",
      "epoch 563 | step 0 | loss: 0.0002793683057016044\n",
      "epoch 563 | step 1 | loss: 0.0005574618399164325\n",
      "epoch 563 | step 2 | loss: 0.000858802347254011\n",
      "epoch 563 | step 3 | loss: 0.0011432443303605994\n",
      "epoch 563 | step 4 | loss: 0.001453597699103228\n",
      "epoch 563 | step 5 | loss: 0.0017513453629255404\n",
      "epoch 563 | step 6 | loss: 0.0020306370642190824\n",
      "epoch 563 | step 7 | loss: 0.0023486418289236206\n",
      "epoch 563 | step 8 | loss: 0.002632168064806537\n",
      "epoch 563 | step 9 | loss: 0.002900652755882589\n",
      "epoch 563 | step 10 | loss: 0.0032147051264136634\n",
      "epoch 563 | step 11 | loss: 0.0035007462390602533\n",
      "epoch 564 | step 0 | loss: 0.00027381971944241343\n",
      "epoch 564 | step 1 | loss: 0.0005983279823236972\n",
      "epoch 564 | step 2 | loss: 0.0008979305266252037\n",
      "epoch 564 | step 3 | loss: 0.0011705653090035735\n",
      "epoch 564 | step 4 | loss: 0.0014455222157329479\n",
      "epoch 564 | step 5 | loss: 0.001744015722696965\n",
      "epoch 564 | step 6 | loss: 0.0020287956807049615\n",
      "epoch 564 | step 7 | loss: 0.0022903084494223435\n",
      "epoch 564 | step 8 | loss: 0.002579205875946818\n",
      "epoch 564 | step 9 | loss: 0.0028746164683740062\n",
      "epoch 564 | step 10 | loss: 0.0031809289546762868\n",
      "epoch 564 | step 11 | loss: 0.003513941464660568\n",
      "epoch 565 | step 0 | loss: 0.00030658659258649715\n",
      "epoch 565 | step 1 | loss: 0.0005866062902682164\n",
      "epoch 565 | step 2 | loss: 0.0008860564569836509\n",
      "epoch 565 | step 3 | loss: 0.0011797252178690767\n",
      "epoch 565 | step 4 | loss: 0.0014802233231588633\n",
      "epoch 565 | step 5 | loss: 0.001769967249636756\n",
      "epoch 565 | step 6 | loss: 0.0020843144499061174\n",
      "epoch 565 | step 7 | loss: 0.0023434897274700823\n",
      "epoch 565 | step 8 | loss: 0.0026487069692214195\n",
      "epoch 565 | step 9 | loss: 0.002939317148671706\n",
      "epoch 565 | step 10 | loss: 0.0032257156387950946\n",
      "epoch 565 | step 11 | loss: 0.0034963793711887377\n",
      "epoch 566 | step 0 | loss: 0.00028676687531427683\n",
      "epoch 566 | step 1 | loss: 0.0005928574560937041\n",
      "epoch 566 | step 2 | loss: 0.0008853648517303965\n",
      "epoch 566 | step 3 | loss: 0.0011656415277826963\n",
      "epoch 566 | step 4 | loss: 0.0014712245018387768\n",
      "epoch 566 | step 5 | loss: 0.0017632438615352252\n",
      "epoch 566 | step 6 | loss: 0.002047270861549112\n",
      "epoch 566 | step 7 | loss: 0.002339831891856491\n",
      "epoch 566 | step 8 | loss: 0.002621822671962419\n",
      "epoch 566 | step 9 | loss: 0.0029457349488751813\n",
      "epoch 566 | step 10 | loss: 0.0032332505630645104\n",
      "epoch 566 | step 11 | loss: 0.00349307730550085\n",
      "epoch 567 | step 0 | loss: 0.00026159301351723113\n",
      "epoch 567 | step 1 | loss: 0.0005879138596928997\n",
      "epoch 567 | step 2 | loss: 0.0008611333815171082\n",
      "epoch 567 | step 3 | loss: 0.0011745219165939943\n",
      "epoch 567 | step 4 | loss: 0.0014555115392823172\n",
      "epoch 567 | step 5 | loss: 0.001770571843719222\n",
      "epoch 567 | step 6 | loss: 0.0020588124844921904\n",
      "epoch 567 | step 7 | loss: 0.0023149547366424885\n",
      "epoch 567 | step 8 | loss: 0.002616787016470474\n",
      "epoch 567 | step 9 | loss: 0.0029265633151599225\n",
      "epoch 567 | step 10 | loss: 0.003220164102306597\n",
      "epoch 567 | step 11 | loss: 0.0034982162054720102\n",
      "epoch 568 | step 0 | loss: 0.00031222763042807876\n",
      "epoch 568 | step 1 | loss: 0.0005889740825582629\n",
      "epoch 568 | step 2 | loss: 0.0008868440823319619\n",
      "epoch 568 | step 3 | loss: 0.0011926391481196024\n",
      "epoch 568 | step 4 | loss: 0.001487102073127796\n",
      "epoch 568 | step 5 | loss: 0.0017784871956663902\n",
      "epoch 568 | step 6 | loss: 0.00208124279388719\n",
      "epoch 568 | step 7 | loss: 0.0023665632764506405\n",
      "epoch 568 | step 8 | loss: 0.002669357189394156\n",
      "epoch 568 | step 9 | loss: 0.002964533306268664\n",
      "epoch 568 | step 10 | loss: 0.0032329023320487463\n",
      "epoch 568 | step 11 | loss: 0.0034931018207866515\n",
      "epoch 569 | step 0 | loss: 0.00026689662557479783\n",
      "epoch 569 | step 1 | loss: 0.0005476549612476558\n",
      "epoch 569 | step 2 | loss: 0.0008353730867044915\n",
      "epoch 569 | step 3 | loss: 0.0011208623068878602\n",
      "epoch 569 | step 4 | loss: 0.0014350280102590112\n",
      "epoch 569 | step 5 | loss: 0.0017475385084055964\n",
      "epoch 569 | step 6 | loss: 0.002038939126851806\n",
      "epoch 569 | step 7 | loss: 0.002334192202060479\n",
      "epoch 569 | step 8 | loss: 0.0026187241918401707\n",
      "epoch 569 | step 9 | loss: 0.0029000310140936264\n",
      "epoch 569 | step 10 | loss: 0.0031791125608137947\n",
      "epoch 569 | step 11 | loss: 0.0035141936121173218\n",
      "epoch 570 | step 0 | loss: 0.00029828149983895165\n",
      "epoch 570 | step 1 | loss: 0.0005830256476631035\n",
      "epoch 570 | step 2 | loss: 0.000896560048051568\n",
      "epoch 570 | step 3 | loss: 0.001184528489881023\n",
      "epoch 570 | step 4 | loss: 0.0014522002100099455\n",
      "epoch 570 | step 5 | loss: 0.0017458947941266698\n",
      "epoch 570 | step 6 | loss: 0.0020239982151796965\n",
      "epoch 570 | step 7 | loss: 0.0023227089223123707\n",
      "epoch 570 | step 8 | loss: 0.0026117190319702287\n",
      "epoch 570 | step 9 | loss: 0.002922522284075555\n",
      "epoch 570 | step 10 | loss: 0.0032107065209485723\n",
      "epoch 570 | step 11 | loss: 0.003501689239563583\n",
      "epoch 571 | step 0 | loss: 0.00031584935039453803\n",
      "epoch 571 | step 1 | loss: 0.0006064795060543146\n",
      "epoch 571 | step 2 | loss: 0.0008854357695791057\n",
      "epoch 571 | step 3 | loss: 0.001151882422884581\n",
      "epoch 571 | step 4 | loss: 0.0014442068795533828\n",
      "epoch 571 | step 5 | loss: 0.0017091600563937325\n",
      "epoch 571 | step 6 | loss: 0.0019944041363804173\n",
      "epoch 571 | step 7 | loss: 0.0023087258805183732\n",
      "epoch 571 | step 8 | loss: 0.0026194840315779426\n",
      "epoch 571 | step 9 | loss: 0.0029221946336741153\n",
      "epoch 571 | step 10 | loss: 0.003211487188858059\n",
      "epoch 571 | step 11 | loss: 0.003501300783655941\n",
      "epoch 572 | step 0 | loss: 0.00029955462378113997\n",
      "epoch 572 | step 1 | loss: 0.0006088459280189062\n",
      "epoch 572 | step 2 | loss: 0.0008725376404406821\n",
      "epoch 572 | step 3 | loss: 0.0011629902968679148\n",
      "epoch 572 | step 4 | loss: 0.001455670754941948\n",
      "epoch 572 | step 5 | loss: 0.0017647512418586294\n",
      "epoch 572 | step 6 | loss: 0.0020794965131962764\n",
      "epoch 572 | step 7 | loss: 0.002343590845314898\n",
      "epoch 572 | step 8 | loss: 0.002641204278017998\n",
      "epoch 572 | step 9 | loss: 0.002914889305517905\n",
      "epoch 572 | step 10 | loss: 0.0032174707700379076\n",
      "epoch 572 | step 11 | loss: 0.0034988050681217463\n",
      "epoch 573 | step 0 | loss: 0.0003026247227487121\n",
      "epoch 573 | step 1 | loss: 0.0005957393047219095\n",
      "epoch 573 | step 2 | loss: 0.0008915148397124348\n",
      "epoch 573 | step 3 | loss: 0.0011852048797795576\n",
      "epoch 573 | step 4 | loss: 0.0014593443488921036\n",
      "epoch 573 | step 5 | loss: 0.00172769923581574\n",
      "epoch 573 | step 6 | loss: 0.0020213750442048496\n",
      "epoch 573 | step 7 | loss: 0.002309396763951271\n",
      "epoch 573 | step 8 | loss: 0.0026175730647385064\n",
      "epoch 573 | step 9 | loss: 0.00294511742283954\n",
      "epoch 573 | step 10 | loss: 0.0032299981064923323\n",
      "epoch 573 | step 11 | loss: 0.0034939093269625876\n",
      "epoch 574 | step 0 | loss: 0.00030451438518420607\n",
      "epoch 574 | step 1 | loss: 0.0005911891135481545\n",
      "epoch 574 | step 2 | loss: 0.0008913529248124932\n",
      "epoch 574 | step 3 | loss: 0.00119048887571829\n",
      "epoch 574 | step 4 | loss: 0.0014774441832775416\n",
      "epoch 574 | step 5 | loss: 0.0017603778559491968\n",
      "epoch 574 | step 6 | loss: 0.0020701373892449203\n",
      "epoch 574 | step 7 | loss: 0.0023579578674528825\n",
      "epoch 574 | step 8 | loss: 0.0026562155169525966\n",
      "epoch 574 | step 9 | loss: 0.0029273290379612533\n",
      "epoch 574 | step 10 | loss: 0.003227848187997306\n",
      "epoch 574 | step 11 | loss: 0.0034944071129339913\n",
      "epoch 575 | step 0 | loss: 0.00029227466314753946\n",
      "epoch 575 | step 1 | loss: 0.0005795364115900982\n",
      "epoch 575 | step 2 | loss: 0.0009148863717533869\n",
      "epoch 575 | step 3 | loss: 0.0012344099619415335\n",
      "epoch 575 | step 4 | loss: 0.0015366024680763264\n",
      "epoch 575 | step 5 | loss: 0.0018346264704944756\n",
      "epoch 575 | step 6 | loss: 0.0021138749568138156\n",
      "epoch 575 | step 7 | loss: 0.002393467843304763\n",
      "epoch 575 | step 8 | loss: 0.0026663968155394187\n",
      "epoch 575 | step 9 | loss: 0.00291957586171385\n",
      "epoch 575 | step 10 | loss: 0.003205630523352694\n",
      "epoch 575 | step 11 | loss: 0.0035030625342591707\n",
      "epoch 576 | step 0 | loss: 0.00029260255783801253\n",
      "epoch 576 | step 1 | loss: 0.0005676981079234077\n",
      "epoch 576 | step 2 | loss: 0.0008654307978734538\n",
      "epoch 576 | step 3 | loss: 0.001154976237276857\n",
      "epoch 576 | step 4 | loss: 0.001419615550533967\n",
      "epoch 576 | step 5 | loss: 0.0016951659899600506\n",
      "epoch 576 | step 6 | loss: 0.001995704523614225\n",
      "epoch 576 | step 7 | loss: 0.0023072825722327333\n",
      "epoch 576 | step 8 | loss: 0.0026043643492336026\n",
      "epoch 576 | step 9 | loss: 0.0029171328423400313\n",
      "epoch 576 | step 10 | loss: 0.0032221211596914824\n",
      "epoch 576 | step 11 | loss: 0.0034965803774602783\n",
      "epoch 577 | step 0 | loss: 0.00027665887173322184\n",
      "epoch 577 | step 1 | loss: 0.0005793079282611452\n",
      "epoch 577 | step 2 | loss: 0.0008789242170374283\n",
      "epoch 577 | step 3 | loss: 0.0011944559962312228\n",
      "epoch 577 | step 4 | loss: 0.0015004310444948622\n",
      "epoch 577 | step 5 | loss: 0.0018018356796860448\n",
      "epoch 577 | step 6 | loss: 0.0020815730562669835\n",
      "epoch 577 | step 7 | loss: 0.0023496473341359783\n",
      "epoch 577 | step 8 | loss: 0.0026390747910226076\n",
      "epoch 577 | step 9 | loss: 0.002921089046243938\n",
      "epoch 577 | step 10 | loss: 0.003201455285543122\n",
      "epoch 577 | step 11 | loss: 0.0035044696765468624\n",
      "epoch 578 | step 0 | loss: 0.0003069101053402185\n",
      "epoch 578 | step 1 | loss: 0.0005821811917674201\n",
      "epoch 578 | step 2 | loss: 0.0008585622776529514\n",
      "epoch 578 | step 3 | loss: 0.0011775138042160425\n",
      "epoch 578 | step 4 | loss: 0.0014815762829796454\n",
      "epoch 578 | step 5 | loss: 0.0017745070980239876\n",
      "epoch 578 | step 6 | loss: 0.0020584477469732703\n",
      "epoch 578 | step 7 | loss: 0.002329417180853679\n",
      "epoch 578 | step 8 | loss: 0.002624956801368499\n",
      "epoch 578 | step 9 | loss: 0.00291512483325174\n",
      "epoch 578 | step 10 | loss: 0.003209461649098398\n",
      "epoch 578 | step 11 | loss: 0.003501531805786503\n",
      "epoch 579 | step 0 | loss: 0.0002995235671090178\n",
      "epoch 579 | step 1 | loss: 0.000602742974429697\n",
      "epoch 579 | step 2 | loss: 0.000900549196608855\n",
      "epoch 579 | step 3 | loss: 0.0011879576692307072\n",
      "epoch 579 | step 4 | loss: 0.0014826281759261408\n",
      "epoch 579 | step 5 | loss: 0.0017651487487683627\n",
      "epoch 579 | step 6 | loss: 0.0020557202909409883\n",
      "epoch 579 | step 7 | loss: 0.0023459385131415337\n",
      "epoch 579 | step 8 | loss: 0.00261970629672993\n",
      "epoch 579 | step 9 | loss: 0.002900638114513543\n",
      "epoch 579 | step 10 | loss: 0.0032150025615856935\n",
      "epoch 579 | step 11 | loss: 0.0034990967259173078\n",
      "epoch 580 | step 0 | loss: 0.0002923045170763973\n",
      "epoch 580 | step 1 | loss: 0.0005773035730092825\n",
      "epoch 580 | step 2 | loss: 0.0008454353131688127\n",
      "epoch 580 | step 3 | loss: 0.0011627471153868633\n",
      "epoch 580 | step 4 | loss: 0.0014599090507226506\n",
      "epoch 580 | step 5 | loss: 0.0017636438702247404\n",
      "epoch 580 | step 6 | loss: 0.0020470714449245917\n",
      "epoch 580 | step 7 | loss: 0.0023298798556425915\n",
      "epoch 580 | step 8 | loss: 0.002615258973292476\n",
      "epoch 580 | step 9 | loss: 0.0029026251530933967\n",
      "epoch 580 | step 10 | loss: 0.0032126898941885903\n",
      "epoch 580 | step 11 | loss: 0.0034998438715876682\n",
      "epoch 581 | step 0 | loss: 0.00029896337218532733\n",
      "epoch 581 | step 1 | loss: 0.0006107036955960222\n",
      "epoch 581 | step 2 | loss: 0.0009203470254746443\n",
      "epoch 581 | step 3 | loss: 0.0012179138929681158\n",
      "epoch 581 | step 4 | loss: 0.0015150845857561782\n",
      "epoch 581 | step 5 | loss: 0.0017746520283240426\n",
      "epoch 581 | step 6 | loss: 0.002073215357182022\n",
      "epoch 581 | step 7 | loss: 0.0023951084975784892\n",
      "epoch 581 | step 8 | loss: 0.0026584863700578783\n",
      "epoch 581 | step 9 | loss: 0.0029634590345101563\n",
      "epoch 581 | step 10 | loss: 0.0032297039094879277\n",
      "epoch 581 | step 11 | loss: 0.0034931166130852676\n",
      "epoch 582 | step 0 | loss: 0.00026186213468205795\n",
      "epoch 582 | step 1 | loss: 0.0005512787172283019\n",
      "epoch 582 | step 2 | loss: 0.0008551860674753288\n",
      "epoch 582 | step 3 | loss: 0.0011304950600751202\n",
      "epoch 582 | step 4 | loss: 0.0014247743164894373\n",
      "epoch 582 | step 5 | loss: 0.001721891794296135\n",
      "epoch 582 | step 6 | loss: 0.001995851982041615\n",
      "epoch 582 | step 7 | loss: 0.002307815575474038\n",
      "epoch 582 | step 8 | loss: 0.0025911923715093636\n",
      "epoch 582 | step 9 | loss: 0.0028842479319261616\n",
      "epoch 582 | step 10 | loss: 0.003188980588667051\n",
      "epoch 582 | step 11 | loss: 0.003508948415284982\n",
      "epoch 583 | step 0 | loss: 0.0002619042196706245\n",
      "epoch 583 | step 1 | loss: 0.0005736610470607327\n",
      "epoch 583 | step 2 | loss: 0.000851220523464475\n",
      "epoch 583 | step 3 | loss: 0.0011252365074926034\n",
      "epoch 583 | step 4 | loss: 0.001422497873983658\n",
      "epoch 583 | step 5 | loss: 0.00173367311741871\n",
      "epoch 583 | step 6 | loss: 0.0020474687485581744\n",
      "epoch 583 | step 7 | loss: 0.0023119814522408457\n",
      "epoch 583 | step 8 | loss: 0.0026097995214903625\n",
      "epoch 583 | step 9 | loss: 0.0029034980632208583\n",
      "epoch 583 | step 10 | loss: 0.0031946558692751157\n",
      "epoch 583 | step 11 | loss: 0.003506593404899904\n",
      "epoch 584 | step 0 | loss: 0.00028081100461374255\n",
      "epoch 584 | step 1 | loss: 0.0005780039747466124\n",
      "epoch 584 | step 2 | loss: 0.0008595992362569711\n",
      "epoch 584 | step 3 | loss: 0.001138328986814937\n",
      "epoch 584 | step 4 | loss: 0.0014365150800756121\n",
      "epoch 584 | step 5 | loss: 0.0017179772117494629\n",
      "epoch 584 | step 6 | loss: 0.0020102624527716254\n",
      "epoch 584 | step 7 | loss: 0.0023246042921871634\n",
      "epoch 584 | step 8 | loss: 0.002634716321484759\n",
      "epoch 584 | step 9 | loss: 0.0029281758094392217\n",
      "epoch 584 | step 10 | loss: 0.003211110587945016\n",
      "epoch 584 | step 11 | loss: 0.0035001050089770664\n",
      "epoch 585 | step 0 | loss: 0.0002707343767662005\n",
      "epoch 585 | step 1 | loss: 0.0005756981476241537\n",
      "epoch 585 | step 2 | loss: 0.0008888519286268031\n",
      "epoch 585 | step 3 | loss: 0.001137300514079977\n",
      "epoch 585 | step 4 | loss: 0.001442407944328154\n",
      "epoch 585 | step 5 | loss: 0.0017360102518804807\n",
      "epoch 585 | step 6 | loss: 0.002020918615390307\n",
      "epoch 585 | step 7 | loss: 0.002322169836514435\n",
      "epoch 585 | step 8 | loss: 0.002594811590250505\n",
      "epoch 585 | step 9 | loss: 0.0029174301470674914\n",
      "epoch 585 | step 10 | loss: 0.0032196498460649674\n",
      "epoch 585 | step 11 | loss: 0.0034967882543965345\n",
      "epoch 586 | step 0 | loss: 0.00030001634677996776\n",
      "epoch 586 | step 1 | loss: 0.000591114737052334\n",
      "epoch 586 | step 2 | loss: 0.0008811399628184807\n",
      "epoch 586 | step 3 | loss: 0.0011655196541275471\n",
      "epoch 586 | step 4 | loss: 0.0014817773980736033\n",
      "epoch 586 | step 5 | loss: 0.0017648721403980972\n",
      "epoch 586 | step 6 | loss: 0.0020476961668747773\n",
      "epoch 586 | step 7 | loss: 0.002363285431984117\n",
      "epoch 586 | step 8 | loss: 0.0026380678709526733\n",
      "epoch 586 | step 9 | loss: 0.0029428385067660646\n",
      "epoch 586 | step 10 | loss: 0.0032279448137042094\n",
      "epoch 586 | step 11 | loss: 0.003493558396442863\n",
      "epoch 587 | step 0 | loss: 0.0002721113535852593\n",
      "epoch 587 | step 1 | loss: 0.0005651401113198902\n",
      "epoch 587 | step 2 | loss: 0.0008382365327264723\n",
      "epoch 587 | step 3 | loss: 0.0011272590605470492\n",
      "epoch 587 | step 4 | loss: 0.001430244344442166\n",
      "epoch 587 | step 5 | loss: 0.0017369950445807701\n",
      "epoch 587 | step 6 | loss: 0.0020385827649975945\n",
      "epoch 587 | step 7 | loss: 0.0023305962138228087\n",
      "epoch 587 | step 8 | loss: 0.002612597527158847\n",
      "epoch 587 | step 9 | loss: 0.0029132372371970033\n",
      "epoch 587 | step 10 | loss: 0.003198358591528211\n",
      "epoch 587 | step 11 | loss: 0.003504681408977602\n",
      "epoch 588 | step 0 | loss: 0.0002950171541019741\n",
      "epoch 588 | step 1 | loss: 0.000575830985178464\n",
      "epoch 588 | step 2 | loss: 0.0008869305458713903\n",
      "epoch 588 | step 3 | loss: 0.0011740347928245042\n",
      "epoch 588 | step 4 | loss: 0.0014494858654927742\n",
      "epoch 588 | step 5 | loss: 0.0017120494159543285\n",
      "epoch 588 | step 6 | loss: 0.002016449107345475\n",
      "epoch 588 | step 7 | loss: 0.0023089795704990153\n",
      "epoch 588 | step 8 | loss: 0.0026305946265996853\n",
      "epoch 588 | step 9 | loss: 0.002926610931414711\n",
      "epoch 588 | step 10 | loss: 0.003217310854340498\n",
      "epoch 588 | step 11 | loss: 0.0034974795947326958\n",
      "epoch 589 | step 0 | loss: 0.0002882000284702092\n",
      "epoch 589 | step 1 | loss: 0.0005613855160812187\n",
      "epoch 589 | step 2 | loss: 0.0008657766358783267\n",
      "epoch 589 | step 3 | loss: 0.0011489049182260228\n",
      "epoch 589 | step 4 | loss: 0.0014508037893124397\n",
      "epoch 589 | step 5 | loss: 0.001742133854207092\n",
      "epoch 589 | step 6 | loss: 0.002052877041150438\n",
      "epoch 589 | step 7 | loss: 0.002377467035133706\n",
      "epoch 589 | step 8 | loss: 0.0026381735116736095\n",
      "epoch 589 | step 9 | loss: 0.0029018600621675303\n",
      "epoch 589 | step 10 | loss: 0.003205571646073462\n",
      "epoch 589 | step 11 | loss: 0.003502202195567023\n",
      "epoch 590 | step 0 | loss: 0.00032801618579041277\n",
      "epoch 590 | step 1 | loss: 0.0006230336822551079\n",
      "epoch 590 | step 2 | loss: 0.000905488658132525\n",
      "epoch 590 | step 3 | loss: 0.0011892574078201114\n",
      "epoch 590 | step 4 | loss: 0.0014659679355702206\n",
      "epoch 590 | step 5 | loss: 0.0017584039517058191\n",
      "epoch 590 | step 6 | loss: 0.002050741150143071\n",
      "epoch 590 | step 7 | loss: 0.00235501123657527\n",
      "epoch 590 | step 8 | loss: 0.0026438327128999605\n",
      "epoch 590 | step 9 | loss: 0.0029218700319375824\n",
      "epoch 590 | step 10 | loss: 0.0032276931876587195\n",
      "epoch 590 | step 11 | loss: 0.0034931719362176774\n",
      "epoch 591 | step 0 | loss: 0.0002792690261518106\n",
      "epoch 591 | step 1 | loss: 0.0005537667839473144\n",
      "epoch 591 | step 2 | loss: 0.0008600221486262714\n",
      "epoch 591 | step 3 | loss: 0.0011375534096045147\n",
      "epoch 591 | step 4 | loss: 0.001460328131905476\n",
      "epoch 591 | step 5 | loss: 0.0017553221676465682\n",
      "epoch 591 | step 6 | loss: 0.0020513105145967053\n",
      "epoch 591 | step 7 | loss: 0.0023330345846908724\n",
      "epoch 591 | step 8 | loss: 0.0026501167170662355\n",
      "epoch 591 | step 9 | loss: 0.002931917729233014\n",
      "epoch 591 | step 10 | loss: 0.003222273215314099\n",
      "epoch 591 | step 11 | loss: 0.003495158060804494\n",
      "epoch 592 | step 0 | loss: 0.00028106293995668047\n",
      "epoch 592 | step 1 | loss: 0.000566954900756915\n",
      "epoch 592 | step 2 | loss: 0.0008590280623615044\n",
      "epoch 592 | step 3 | loss: 0.0011611773959312901\n",
      "epoch 592 | step 4 | loss: 0.0014327819208115804\n",
      "epoch 592 | step 5 | loss: 0.0017125582427303307\n",
      "epoch 592 | step 6 | loss: 0.0020094452867487926\n",
      "epoch 592 | step 7 | loss: 0.002330802325360077\n",
      "epoch 592 | step 8 | loss: 0.0026232287079389067\n",
      "epoch 592 | step 9 | loss: 0.0029492550377937937\n",
      "epoch 592 | step 10 | loss: 0.0032261994120550814\n",
      "epoch 592 | step 11 | loss: 0.0034936088293814455\n",
      "epoch 593 | step 0 | loss: 0.0003041928454629086\n",
      "epoch 593 | step 1 | loss: 0.0006074188417755562\n",
      "epoch 593 | step 2 | loss: 0.0009099480429079808\n",
      "epoch 593 | step 3 | loss: 0.001208042146305758\n",
      "epoch 593 | step 4 | loss: 0.001475462782097354\n",
      "epoch 593 | step 5 | loss: 0.0017582922325915435\n",
      "epoch 593 | step 6 | loss: 0.0020623648354884555\n",
      "epoch 593 | step 7 | loss: 0.0023514834248463618\n",
      "epoch 593 | step 8 | loss: 0.0026490509654739165\n",
      "epoch 593 | step 9 | loss: 0.0029541920207384192\n",
      "epoch 593 | step 10 | loss: 0.00321744033531234\n",
      "epoch 593 | step 11 | loss: 0.0034967987378790354\n",
      "epoch 594 | step 0 | loss: 0.0002914561484038124\n",
      "epoch 594 | step 1 | loss: 0.0005772723580224333\n",
      "epoch 594 | step 2 | loss: 0.0008552734942584876\n",
      "epoch 594 | step 3 | loss: 0.0011634690268425819\n",
      "epoch 594 | step 4 | loss: 0.0014500811105147564\n",
      "epoch 594 | step 5 | loss: 0.001750728578076437\n",
      "epoch 594 | step 6 | loss: 0.00204710202963619\n",
      "epoch 594 | step 7 | loss: 0.0023466764111828857\n",
      "epoch 594 | step 8 | loss: 0.0026269382399289644\n",
      "epoch 594 | step 9 | loss: 0.0029326536809214836\n",
      "epoch 594 | step 10 | loss: 0.0032157060877295958\n",
      "epoch 594 | step 11 | loss: 0.0034976392453683986\n",
      "epoch 595 | step 0 | loss: 0.0002915712767591613\n",
      "epoch 595 | step 1 | loss: 0.0005801769016313893\n",
      "epoch 595 | step 2 | loss: 0.0008512647503537552\n",
      "epoch 595 | step 3 | loss: 0.0011628310266142982\n",
      "epoch 595 | step 4 | loss: 0.0014518750240831873\n",
      "epoch 595 | step 5 | loss: 0.0017208813193673998\n",
      "epoch 595 | step 6 | loss: 0.0020038028852070035\n",
      "epoch 595 | step 7 | loss: 0.0023011182564734722\n",
      "epoch 595 | step 8 | loss: 0.002596321131776936\n",
      "epoch 595 | step 9 | loss: 0.0028928887297999914\n",
      "epoch 595 | step 10 | loss: 0.0032155009674626955\n",
      "epoch 595 | step 11 | loss: 0.003497254291880607\n",
      "epoch 596 | step 0 | loss: 0.0002580286324941031\n",
      "epoch 596 | step 1 | loss: 0.0005392000968124369\n",
      "epoch 596 | step 2 | loss: 0.0008437740552802327\n",
      "epoch 596 | step 3 | loss: 0.001160826518018245\n",
      "epoch 596 | step 4 | loss: 0.0014370908324872539\n",
      "epoch 596 | step 5 | loss: 0.0017352249643927907\n",
      "epoch 596 | step 6 | loss: 0.0020269156481973706\n",
      "epoch 596 | step 7 | loss: 0.002325113831405004\n",
      "epoch 596 | step 8 | loss: 0.0026072314593632304\n",
      "epoch 596 | step 9 | loss: 0.002892023404807644\n",
      "epoch 596 | step 10 | loss: 0.0031924071266858133\n",
      "epoch 596 | step 11 | loss: 0.003506337542352652\n",
      "epoch 597 | step 0 | loss: 0.0002900356894328704\n",
      "epoch 597 | step 1 | loss: 0.0006386027411005876\n",
      "epoch 597 | step 2 | loss: 0.0009256434273981458\n",
      "epoch 597 | step 3 | loss: 0.0012123203224244017\n",
      "epoch 597 | step 4 | loss: 0.0014901734205710836\n",
      "epoch 597 | step 5 | loss: 0.001762694717188751\n",
      "epoch 597 | step 6 | loss: 0.002044748039434779\n",
      "epoch 597 | step 7 | loss: 0.0023284467056771526\n",
      "epoch 597 | step 8 | loss: 0.0026311577230841822\n",
      "epoch 597 | step 9 | loss: 0.0029297088792489072\n",
      "epoch 597 | step 10 | loss: 0.0032191759910814356\n",
      "epoch 597 | step 11 | loss: 0.0034956982477554457\n",
      "epoch 598 | step 0 | loss: 0.0003040338759603837\n",
      "epoch 598 | step 1 | loss: 0.0005752954757929378\n",
      "epoch 598 | step 2 | loss: 0.0008990549330233312\n",
      "epoch 598 | step 3 | loss: 0.0012160638714165876\n",
      "epoch 598 | step 4 | loss: 0.0014887504802862804\n",
      "epoch 598 | step 5 | loss: 0.001763380609264259\n",
      "epoch 598 | step 6 | loss: 0.002042802424977571\n",
      "epoch 598 | step 7 | loss: 0.0023367741023988795\n",
      "epoch 598 | step 8 | loss: 0.00263386195757142\n",
      "epoch 598 | step 9 | loss: 0.002918384895950574\n",
      "epoch 598 | step 10 | loss: 0.003210548259608744\n",
      "epoch 598 | step 11 | loss: 0.0034989723600503736\n",
      "epoch 599 | step 0 | loss: 0.0003189662150746035\n",
      "epoch 599 | step 1 | loss: 0.0006018227276910604\n",
      "epoch 599 | step 2 | loss: 0.0008907210423512689\n",
      "epoch 599 | step 3 | loss: 0.001180980317765411\n",
      "epoch 599 | step 4 | loss: 0.0014666789611606338\n",
      "epoch 599 | step 5 | loss: 0.0017653517658577735\n",
      "epoch 599 | step 6 | loss: 0.002054372798327852\n",
      "epoch 599 | step 7 | loss: 0.0023515365688985997\n",
      "epoch 599 | step 8 | loss: 0.0026345300859639403\n",
      "epoch 599 | step 9 | loss: 0.002928379521898286\n",
      "epoch 599 | step 10 | loss: 0.0032229986850552048\n",
      "epoch 599 | step 11 | loss: 0.0034939234888084125\n",
      "epoch 600 | step 0 | loss: 0.0002833828273398861\n",
      "epoch 600 | step 1 | loss: 0.000588369755994179\n",
      "epoch 600 | step 2 | loss: 0.000874661014307964\n",
      "epoch 600 | step 3 | loss: 0.001162084555216554\n",
      "epoch 600 | step 4 | loss: 0.0014365565479413252\n",
      "epoch 600 | step 5 | loss: 0.0017339210822017738\n",
      "epoch 600 | step 6 | loss: 0.002008919538491082\n",
      "epoch 600 | step 7 | loss: 0.0023013305380932022\n",
      "epoch 600 | step 8 | loss: 0.002618206394216481\n",
      "epoch 600 | step 9 | loss: 0.0028798493959799115\n",
      "epoch 600 | step 10 | loss: 0.0031956172578183873\n",
      "epoch 600 | step 11 | loss: 0.0035047848142289013\n",
      "epoch 601 | step 0 | loss: 0.0003041695940591311\n",
      "epoch 601 | step 1 | loss: 0.0005749477853005421\n",
      "epoch 601 | step 2 | loss: 0.0008461104705378387\n",
      "epoch 601 | step 3 | loss: 0.001145777003715245\n",
      "epoch 601 | step 4 | loss: 0.0014281260580690414\n",
      "epoch 601 | step 5 | loss: 0.001711911574120243\n",
      "epoch 601 | step 6 | loss: 0.0020165125003480298\n",
      "epoch 601 | step 7 | loss: 0.0023013638283486586\n",
      "epoch 601 | step 8 | loss: 0.0025980034325167357\n",
      "epoch 601 | step 9 | loss: 0.0028955261895708723\n",
      "epoch 601 | step 10 | loss: 0.0032005138146064373\n",
      "epoch 601 | step 11 | loss: 0.0035025811467075314\n",
      "epoch 602 | step 0 | loss: 0.000265079837581881\n",
      "epoch 602 | step 1 | loss: 0.0005669502221064458\n",
      "epoch 602 | step 2 | loss: 0.00084654102252066\n",
      "epoch 602 | step 3 | loss: 0.0011624149476384725\n",
      "epoch 602 | step 4 | loss: 0.001459332324931873\n",
      "epoch 602 | step 5 | loss: 0.0017390749691848853\n",
      "epoch 602 | step 6 | loss: 0.002029613219764618\n",
      "epoch 602 | step 7 | loss: 0.0023305635725820867\n",
      "epoch 602 | step 8 | loss: 0.002623192815128847\n",
      "epoch 602 | step 9 | loss: 0.0029065391891302784\n",
      "epoch 602 | step 10 | loss: 0.00321742308381488\n",
      "epoch 602 | step 11 | loss: 0.0034958578874931803\n",
      "epoch 603 | step 0 | loss: 0.00032912578539691086\n",
      "epoch 603 | step 1 | loss: 0.0006319605293132269\n",
      "epoch 603 | step 2 | loss: 0.0009132466821077656\n",
      "epoch 603 | step 3 | loss: 0.0012559194143488637\n",
      "epoch 603 | step 4 | loss: 0.001504940260429798\n",
      "epoch 603 | step 5 | loss: 0.0017939332863371703\n",
      "epoch 603 | step 6 | loss: 0.002058324906607214\n",
      "epoch 603 | step 7 | loss: 0.002350968323908124\n",
      "epoch 603 | step 8 | loss: 0.002634384520052657\n",
      "epoch 603 | step 9 | loss: 0.0029167976408217954\n",
      "epoch 603 | step 10 | loss: 0.003205200144008409\n",
      "epoch 603 | step 11 | loss: 0.003500651989960177\n",
      "epoch 604 | step 0 | loss: 0.00029940847272782697\n",
      "epoch 604 | step 1 | loss: 0.0005814559511830763\n",
      "epoch 604 | step 2 | loss: 0.0008649623040957874\n",
      "epoch 604 | step 3 | loss: 0.0011279520408423206\n",
      "epoch 604 | step 4 | loss: 0.001416857064737469\n",
      "epoch 604 | step 5 | loss: 0.0016964279417369774\n",
      "epoch 604 | step 6 | loss: 0.0020197085890265613\n",
      "epoch 604 | step 7 | loss: 0.0023286799654689155\n",
      "epoch 604 | step 8 | loss: 0.002633836709374346\n",
      "epoch 604 | step 9 | loss: 0.0029251377409449755\n",
      "epoch 604 | step 10 | loss: 0.0032194073976934217\n",
      "epoch 604 | step 11 | loss: 0.003494945321555778\n",
      "epoch 605 | step 0 | loss: 0.0002657951581348399\n",
      "epoch 605 | step 1 | loss: 0.0005974482272272394\n",
      "epoch 605 | step 2 | loss: 0.0008652770249776727\n",
      "epoch 605 | step 3 | loss: 0.0011430072398450105\n",
      "epoch 605 | step 4 | loss: 0.0014468983272576384\n",
      "epoch 605 | step 5 | loss: 0.0017395465207662246\n",
      "epoch 605 | step 6 | loss: 0.0020553239321771727\n",
      "epoch 605 | step 7 | loss: 0.00231957885780043\n",
      "epoch 605 | step 8 | loss: 0.002625437249024084\n",
      "epoch 605 | step 9 | loss: 0.0029035524769521603\n",
      "epoch 605 | step 10 | loss: 0.0032092735172793174\n",
      "epoch 605 | step 11 | loss: 0.00349874083515688\n",
      "epoch 606 | step 0 | loss: 0.0002841444098963722\n",
      "epoch 606 | step 1 | loss: 0.0005731933948491948\n",
      "epoch 606 | step 2 | loss: 0.0008782419740806074\n",
      "epoch 606 | step 3 | loss: 0.0011631735579296532\n",
      "epoch 606 | step 4 | loss: 0.0014490620485346566\n",
      "epoch 606 | step 5 | loss: 0.0017288401440804248\n",
      "epoch 606 | step 6 | loss: 0.0020362148357013464\n",
      "epoch 606 | step 7 | loss: 0.002311568132745386\n",
      "epoch 606 | step 8 | loss: 0.002603714895314447\n",
      "epoch 606 | step 9 | loss: 0.0029098177477836474\n",
      "epoch 606 | step 10 | loss: 0.0031950562312161963\n",
      "epoch 606 | step 11 | loss: 0.0035042060109937005\n",
      "epoch 607 | step 0 | loss: 0.0002973489370807319\n",
      "epoch 607 | step 1 | loss: 0.0005917200895825009\n",
      "epoch 607 | step 2 | loss: 0.0008861733979819055\n",
      "epoch 607 | step 3 | loss: 0.0011638882583418619\n",
      "epoch 607 | step 4 | loss: 0.0014748483019547908\n",
      "epoch 607 | step 5 | loss: 0.0017456449291387045\n",
      "epoch 607 | step 6 | loss: 0.002042817446564067\n",
      "epoch 607 | step 7 | loss: 0.002310292931543144\n",
      "epoch 607 | step 8 | loss: 0.002578873753185578\n",
      "epoch 607 | step 9 | loss: 0.0028745925715744887\n",
      "epoch 607 | step 10 | loss: 0.0031791827125757397\n",
      "epoch 607 | step 11 | loss: 0.003510680035366546\n",
      "epoch 608 | step 0 | loss: 0.00031594112485203884\n",
      "epoch 608 | step 1 | loss: 0.0006146184363529622\n",
      "epoch 608 | step 2 | loss: 0.0008981780947300788\n",
      "epoch 608 | step 3 | loss: 0.001158802074874774\n",
      "epoch 608 | step 4 | loss: 0.0014251705567085548\n",
      "epoch 608 | step 5 | loss: 0.001726560740662822\n",
      "epoch 608 | step 6 | loss: 0.0020129881692353777\n",
      "epoch 608 | step 7 | loss: 0.0023016261885029513\n",
      "epoch 608 | step 8 | loss: 0.0026028338155738863\n",
      "epoch 608 | step 9 | loss: 0.002933336309913169\n",
      "epoch 608 | step 10 | loss: 0.003218913978919115\n",
      "epoch 608 | step 11 | loss: 0.0034949562293253565\n",
      "epoch 609 | step 0 | loss: 0.00026826020301574954\n",
      "epoch 609 | step 1 | loss: 0.0005694655906925306\n",
      "epoch 609 | step 2 | loss: 0.0008785741253188499\n",
      "epoch 609 | step 3 | loss: 0.001154464919706212\n",
      "epoch 609 | step 4 | loss: 0.0014442151687138326\n",
      "epoch 609 | step 5 | loss: 0.0017772786629967224\n",
      "epoch 609 | step 6 | loss: 0.002059409188592986\n",
      "epoch 609 | step 7 | loss: 0.002364993189014806\n",
      "epoch 609 | step 8 | loss: 0.002630676355836518\n",
      "epoch 609 | step 9 | loss: 0.002940472638732849\n",
      "epoch 609 | step 10 | loss: 0.0032095671003952533\n",
      "epoch 609 | step 11 | loss: 0.0034985558757873398\n",
      "epoch 610 | step 0 | loss: 0.0002770172983763081\n",
      "epoch 610 | step 1 | loss: 0.0005560932783619457\n",
      "epoch 610 | step 2 | loss: 0.0008269707934787186\n",
      "epoch 610 | step 3 | loss: 0.0011035644699537925\n",
      "epoch 610 | step 4 | loss: 0.001405332381051708\n",
      "epoch 610 | step 5 | loss: 0.0016928143463666358\n",
      "epoch 610 | step 6 | loss: 0.00199931155197698\n",
      "epoch 610 | step 7 | loss: 0.002311257203117896\n",
      "epoch 610 | step 8 | loss: 0.002602901979941218\n",
      "epoch 610 | step 9 | loss: 0.002883521629983903\n",
      "epoch 610 | step 10 | loss: 0.003204283217738952\n",
      "epoch 610 | step 11 | loss: 0.003500269712791637\n",
      "epoch 611 | step 0 | loss: 0.00026428410197688537\n",
      "epoch 611 | step 1 | loss: 0.000555747752335021\n",
      "epoch 611 | step 2 | loss: 0.0008564339122245634\n",
      "epoch 611 | step 3 | loss: 0.0011526997350472829\n",
      "epoch 611 | step 4 | loss: 0.0014611261747891837\n",
      "epoch 611 | step 5 | loss: 0.001749181048518696\n",
      "epoch 611 | step 6 | loss: 0.002046128999009927\n",
      "epoch 611 | step 7 | loss: 0.0023299741713427064\n",
      "epoch 611 | step 8 | loss: 0.002636839027505976\n",
      "epoch 611 | step 9 | loss: 0.002912556710756783\n",
      "epoch 611 | step 10 | loss: 0.0031945517756768855\n",
      "epoch 611 | step 11 | loss: 0.003504200231964613\n",
      "epoch 612 | step 0 | loss: 0.0003005429521712831\n",
      "epoch 612 | step 1 | loss: 0.0006124695154851973\n",
      "epoch 612 | step 2 | loss: 0.000909550533803151\n",
      "epoch 612 | step 3 | loss: 0.0011855850238603847\n",
      "epoch 612 | step 4 | loss: 0.0014550210248250712\n",
      "epoch 612 | step 5 | loss: 0.0017385962700178642\n",
      "epoch 612 | step 6 | loss: 0.002038185773412916\n",
      "epoch 612 | step 7 | loss: 0.002336025763981504\n",
      "epoch 612 | step 8 | loss: 0.0026217735180057086\n",
      "epoch 612 | step 9 | loss: 0.0029018883189839847\n",
      "epoch 612 | step 10 | loss: 0.0032021252409789418\n",
      "epoch 612 | step 11 | loss: 0.0035008026679976554\n",
      "epoch 613 | step 0 | loss: 0.0002975329802707402\n",
      "epoch 613 | step 1 | loss: 0.0005947270111436856\n",
      "epoch 613 | step 2 | loss: 0.000858130571227221\n",
      "epoch 613 | step 3 | loss: 0.0011346801202424912\n",
      "epoch 613 | step 4 | loss: 0.0014498412563929998\n",
      "epoch 613 | step 5 | loss: 0.0017426493374503456\n",
      "epoch 613 | step 6 | loss: 0.0020309193274124218\n",
      "epoch 613 | step 7 | loss: 0.0023140952653328416\n",
      "epoch 613 | step 8 | loss: 0.002616081862517779\n",
      "epoch 613 | step 9 | loss: 0.0029238102332072224\n",
      "epoch 613 | step 10 | loss: 0.0032182902969500847\n",
      "epoch 613 | step 11 | loss: 0.0034947559145236716\n",
      "epoch 614 | step 0 | loss: 0.0002596717913161997\n",
      "epoch 614 | step 1 | loss: 0.0005430722615004695\n",
      "epoch 614 | step 2 | loss: 0.0008326083188761168\n",
      "epoch 614 | step 3 | loss: 0.0011149661403109805\n",
      "epoch 614 | step 4 | loss: 0.0014305125513771099\n",
      "epoch 614 | step 5 | loss: 0.0017089897488880245\n",
      "epoch 614 | step 6 | loss: 0.001980095677166008\n",
      "epoch 614 | step 7 | loss: 0.0022827735645284545\n",
      "epoch 614 | step 8 | loss: 0.002600343375110839\n",
      "epoch 614 | step 9 | loss: 0.0029011263435791508\n",
      "epoch 614 | step 10 | loss: 0.0031864744235407033\n",
      "epoch 614 | step 11 | loss: 0.003506616703596916\n",
      "epoch 615 | step 0 | loss: 0.00029588485310906587\n",
      "epoch 615 | step 1 | loss: 0.0006023680674626102\n",
      "epoch 615 | step 2 | loss: 0.000896933790402138\n",
      "epoch 615 | step 3 | loss: 0.001153350878242745\n",
      "epoch 615 | step 4 | loss: 0.0014568469113640532\n",
      "epoch 615 | step 5 | loss: 0.0017649450525212321\n",
      "epoch 615 | step 6 | loss: 0.0020398740838102713\n",
      "epoch 615 | step 7 | loss: 0.0023401415969453256\n",
      "epoch 615 | step 8 | loss: 0.0026483587561893737\n",
      "epoch 615 | step 9 | loss: 0.0029189063820944843\n",
      "epoch 615 | step 10 | loss: 0.003214072497296263\n",
      "epoch 615 | step 11 | loss: 0.0034958181982005544\n",
      "epoch 616 | step 0 | loss: 0.00028378718556733816\n",
      "epoch 616 | step 1 | loss: 0.000562210719425447\n",
      "epoch 616 | step 2 | loss: 0.0008630771040656293\n",
      "epoch 616 | step 3 | loss: 0.0011226115406160078\n",
      "epoch 616 | step 4 | loss: 0.0014222782961251755\n",
      "epoch 616 | step 5 | loss: 0.0017243745833971438\n",
      "epoch 616 | step 6 | loss: 0.0020231766275194654\n",
      "epoch 616 | step 7 | loss: 0.0023206220935876264\n",
      "epoch 616 | step 8 | loss: 0.0026301511079061556\n",
      "epoch 616 | step 9 | loss: 0.002943656438274086\n",
      "epoch 616 | step 10 | loss: 0.0032136215956108443\n",
      "epoch 616 | step 11 | loss: 0.0034958762044893\n",
      "epoch 617 | step 0 | loss: 0.0003045816581998502\n",
      "epoch 617 | step 1 | loss: 0.0006145823682132529\n",
      "epoch 617 | step 2 | loss: 0.0009343903575996274\n",
      "epoch 617 | step 3 | loss: 0.0012314275537853897\n",
      "epoch 617 | step 4 | loss: 0.001526806068750879\n",
      "epoch 617 | step 5 | loss: 0.0018066526883051964\n",
      "epoch 617 | step 6 | loss: 0.0020961877325876995\n",
      "epoch 617 | step 7 | loss: 0.0023669284380837766\n",
      "epoch 617 | step 8 | loss: 0.0026370340317279964\n",
      "epoch 617 | step 9 | loss: 0.002921104546839476\n",
      "epoch 617 | step 10 | loss: 0.0032212250009932457\n",
      "epoch 617 | step 11 | loss: 0.00349308752329417\n",
      "epoch 618 | step 0 | loss: 0.00026985721820625913\n",
      "epoch 618 | step 1 | loss: 0.0005629615834828738\n",
      "epoch 618 | step 2 | loss: 0.0008425582578258214\n",
      "epoch 618 | step 3 | loss: 0.001122375706097714\n",
      "epoch 618 | step 4 | loss: 0.0014058094121577873\n",
      "epoch 618 | step 5 | loss: 0.0017032081646474768\n",
      "epoch 618 | step 6 | loss: 0.0020174564326367497\n",
      "epoch 618 | step 7 | loss: 0.0023094231794844825\n",
      "epoch 618 | step 8 | loss: 0.002618266156472547\n",
      "epoch 618 | step 9 | loss: 0.002941591354173543\n",
      "epoch 618 | step 10 | loss: 0.003205639121241828\n",
      "epoch 618 | step 11 | loss: 0.0034988572302620787\n",
      "epoch 619 | step 0 | loss: 0.00028674532776092304\n",
      "epoch 619 | step 1 | loss: 0.0005654726411873552\n",
      "epoch 619 | step 2 | loss: 0.000882994224988534\n",
      "epoch 619 | step 3 | loss: 0.001181912360192268\n",
      "epoch 619 | step 4 | loss: 0.0014729340867257659\n",
      "epoch 619 | step 5 | loss: 0.0017550744245745657\n",
      "epoch 619 | step 6 | loss: 0.0020126912443912863\n",
      "epoch 619 | step 7 | loss: 0.002305837317024633\n",
      "epoch 619 | step 8 | loss: 0.0026104400249798137\n",
      "epoch 619 | step 9 | loss: 0.00287955020501307\n",
      "epoch 619 | step 10 | loss: 0.0031981104974929306\n",
      "epoch 619 | step 11 | loss: 0.0035018327898835707\n",
      "epoch 620 | step 0 | loss: 0.0002899635096096662\n",
      "epoch 620 | step 1 | loss: 0.0005934240582204659\n",
      "epoch 620 | step 2 | loss: 0.0008767039017387245\n",
      "epoch 620 | step 3 | loss: 0.0011621676722009852\n",
      "epoch 620 | step 4 | loss: 0.0014678095798064535\n",
      "epoch 620 | step 5 | loss: 0.0017686256446293454\n",
      "epoch 620 | step 6 | loss: 0.002060129413543868\n",
      "epoch 620 | step 7 | loss: 0.002364109979317921\n",
      "epoch 620 | step 8 | loss: 0.0026333871524612884\n",
      "epoch 620 | step 9 | loss: 0.0029162564266117866\n",
      "epoch 620 | step 10 | loss: 0.003209756484395526\n",
      "epoch 620 | step 11 | loss: 0.003497920449898156\n",
      "epoch 621 | step 0 | loss: 0.0002920351518322857\n",
      "epoch 621 | step 1 | loss: 0.0005818593383776609\n",
      "epoch 621 | step 2 | loss: 0.0008597955454570673\n",
      "epoch 621 | step 3 | loss: 0.0011675836443070838\n",
      "epoch 621 | step 4 | loss: 0.0014568951664905992\n",
      "epoch 621 | step 5 | loss: 0.0017391759335455615\n",
      "epoch 621 | step 6 | loss: 0.0020568566618729862\n",
      "epoch 621 | step 7 | loss: 0.0023408730174999557\n",
      "epoch 621 | step 8 | loss: 0.002625324894999698\n",
      "epoch 621 | step 9 | loss: 0.0029118574621077153\n",
      "epoch 621 | step 10 | loss: 0.003221502286212758\n",
      "epoch 621 | step 11 | loss: 0.003492797869587381\n",
      "epoch 622 | step 0 | loss: 0.0002732118297792682\n",
      "epoch 622 | step 1 | loss: 0.0005822485819187783\n",
      "epoch 622 | step 2 | loss: 0.0008764150831032137\n",
      "epoch 622 | step 3 | loss: 0.0011753951974774767\n",
      "epoch 622 | step 4 | loss: 0.0014813997124131831\n",
      "epoch 622 | step 5 | loss: 0.0017634615417348384\n",
      "epoch 622 | step 6 | loss: 0.0020428848018930095\n",
      "epoch 622 | step 7 | loss: 0.002330764639874431\n",
      "epoch 622 | step 8 | loss: 0.0026611653855921776\n",
      "epoch 622 | step 9 | loss: 0.0029419353952493334\n",
      "epoch 622 | step 10 | loss: 0.0032357664783435975\n",
      "epoch 622 | step 11 | loss: 0.0034868152287205697\n",
      "epoch 623 | step 0 | loss: 0.0002923229606253246\n",
      "epoch 623 | step 1 | loss: 0.0005720284053980688\n",
      "epoch 623 | step 2 | loss: 0.0008626684463243501\n",
      "epoch 623 | step 3 | loss: 0.0011354831468299146\n",
      "epoch 623 | step 4 | loss: 0.0014261358231889708\n",
      "epoch 623 | step 5 | loss: 0.0017116540337866388\n",
      "epoch 623 | step 6 | loss: 0.00200321813889994\n",
      "epoch 623 | step 7 | loss: 0.002296202527784619\n",
      "epoch 623 | step 8 | loss: 0.002586873833261319\n",
      "epoch 623 | step 9 | loss: 0.002869545343645139\n",
      "epoch 623 | step 10 | loss: 0.0032019426684747132\n",
      "epoch 623 | step 11 | loss: 0.0034998406901570215\n",
      "epoch 624 | step 0 | loss: 0.0002915199398731431\n",
      "epoch 624 | step 1 | loss: 0.0005722063342814144\n",
      "epoch 624 | step 2 | loss: 0.0008890307981886103\n",
      "epoch 624 | step 3 | loss: 0.0011785742386497408\n",
      "epoch 624 | step 4 | loss: 0.001456231832093204\n",
      "epoch 624 | step 5 | loss: 0.0017418583231317102\n",
      "epoch 624 | step 6 | loss: 0.0020266060350042194\n",
      "epoch 624 | step 7 | loss: 0.002311963870045978\n",
      "epoch 624 | step 8 | loss: 0.0025982330987620672\n",
      "epoch 624 | step 9 | loss: 0.002900136133596689\n",
      "epoch 624 | step 10 | loss: 0.0032148883716856965\n",
      "epoch 624 | step 11 | loss: 0.0034947872402613218\n",
      "epoch 625 | step 0 | loss: 0.00028119356062353696\n",
      "epoch 625 | step 1 | loss: 0.0005828969670918371\n",
      "epoch 625 | step 2 | loss: 0.0008607190823283392\n",
      "epoch 625 | step 3 | loss: 0.0011654884773184427\n",
      "epoch 625 | step 4 | loss: 0.0014428741944792554\n",
      "epoch 625 | step 5 | loss: 0.0017206836323260671\n",
      "epoch 625 | step 6 | loss: 0.002049915648188756\n",
      "epoch 625 | step 7 | loss: 0.0023377515074401315\n",
      "epoch 625 | step 8 | loss: 0.0026350307824011846\n",
      "epoch 625 | step 9 | loss: 0.002919523945708203\n",
      "epoch 625 | step 10 | loss: 0.0032033972956677973\n",
      "epoch 625 | step 11 | loss: 0.0034991702226461587\n",
      "epoch 626 | step 0 | loss: 0.0002979424384453681\n",
      "epoch 626 | step 1 | loss: 0.0005791859512899166\n",
      "epoch 626 | step 2 | loss: 0.0008607336984737139\n",
      "epoch 626 | step 3 | loss: 0.0011397329722113094\n",
      "epoch 626 | step 4 | loss: 0.0014047404642776005\n",
      "epoch 626 | step 5 | loss: 0.0016961358449386313\n",
      "epoch 626 | step 6 | loss: 0.0020102977654992927\n",
      "epoch 626 | step 7 | loss: 0.002303277258472242\n",
      "epoch 626 | step 8 | loss: 0.0025774640236188814\n",
      "epoch 626 | step 9 | loss: 0.00288097532265206\n",
      "epoch 626 | step 10 | loss: 0.003193437010774346\n",
      "epoch 626 | step 11 | loss: 0.003503009046776106\n",
      "epoch 627 | step 0 | loss: 0.0002866338910355961\n",
      "epoch 627 | step 1 | loss: 0.0006024642492697462\n",
      "epoch 627 | step 2 | loss: 0.0009015310574434158\n",
      "epoch 627 | step 3 | loss: 0.0011945296145035506\n",
      "epoch 627 | step 4 | loss: 0.0014981322447141419\n",
      "epoch 627 | step 5 | loss: 0.0017855353141473147\n",
      "epoch 627 | step 6 | loss: 0.0020464646702410003\n",
      "epoch 627 | step 7 | loss: 0.002343710955679038\n",
      "epoch 627 | step 8 | loss: 0.002655285429858809\n",
      "epoch 627 | step 9 | loss: 0.0029371177820922737\n",
      "epoch 627 | step 10 | loss: 0.003212406265524704\n",
      "epoch 627 | step 11 | loss: 0.003495772756830302\n",
      "epoch 628 | step 0 | loss: 0.00027716487883635416\n",
      "epoch 628 | step 1 | loss: 0.0005693980355069882\n",
      "epoch 628 | step 2 | loss: 0.0008629670234136883\n",
      "epoch 628 | step 3 | loss: 0.0011682723597179995\n",
      "epoch 628 | step 4 | loss: 0.0014612259418248712\n",
      "epoch 628 | step 5 | loss: 0.0017338980206960577\n",
      "epoch 628 | step 6 | loss: 0.0020338289347744663\n",
      "epoch 628 | step 7 | loss: 0.0023123366176807385\n",
      "epoch 628 | step 8 | loss: 0.002606849069430943\n",
      "epoch 628 | step 9 | loss: 0.0028779466259707264\n",
      "epoch 628 | step 10 | loss: 0.00318854650978674\n",
      "epoch 628 | step 11 | loss: 0.003504811924313205\n",
      "epoch 629 | step 0 | loss: 0.00029630509886160626\n",
      "epoch 629 | step 1 | loss: 0.0005985429692105144\n",
      "epoch 629 | step 2 | loss: 0.000872998818698023\n",
      "epoch 629 | step 3 | loss: 0.0011771290211204295\n",
      "epoch 629 | step 4 | loss: 0.0014722262028974346\n",
      "epoch 629 | step 5 | loss: 0.0017778788115475727\n",
      "epoch 629 | step 6 | loss: 0.002100985323417691\n",
      "epoch 629 | step 7 | loss: 0.0023989260905193147\n",
      "epoch 629 | step 8 | loss: 0.0026761931105633587\n",
      "epoch 629 | step 9 | loss: 0.0029483453377575214\n",
      "epoch 629 | step 10 | loss: 0.003209944010641038\n",
      "epoch 629 | step 11 | loss: 0.0034960562843125652\n",
      "epoch 630 | step 0 | loss: 0.000284068899292124\n",
      "epoch 630 | step 1 | loss: 0.0005804140132828478\n",
      "epoch 630 | step 2 | loss: 0.0008712115562382161\n",
      "epoch 630 | step 3 | loss: 0.001128785928236483\n",
      "epoch 630 | step 4 | loss: 0.0014192454787831901\n",
      "epoch 630 | step 5 | loss: 0.0016999692610671607\n",
      "epoch 630 | step 6 | loss: 0.002015703247796724\n",
      "epoch 630 | step 7 | loss: 0.0023088096929556024\n",
      "epoch 630 | step 8 | loss: 0.0026070111689696996\n",
      "epoch 630 | step 9 | loss: 0.002896953636697687\n",
      "epoch 630 | step 10 | loss: 0.0032127489629994697\n",
      "epoch 630 | step 11 | loss: 0.003494961207352315\n",
      "epoch 631 | step 0 | loss: 0.0003083011965409066\n",
      "epoch 631 | step 1 | loss: 0.0006068762911012142\n",
      "epoch 631 | step 2 | loss: 0.0009018237309615429\n",
      "epoch 631 | step 3 | loss: 0.001185350117458729\n",
      "epoch 631 | step 4 | loss: 0.001493617612908375\n",
      "epoch 631 | step 5 | loss: 0.001783481123494464\n",
      "epoch 631 | step 6 | loss: 0.0020673913949355895\n",
      "epoch 631 | step 7 | loss: 0.0023740163668512404\n",
      "epoch 631 | step 8 | loss: 0.0026541635322368906\n",
      "epoch 631 | step 9 | loss: 0.002920246633178728\n",
      "epoch 631 | step 10 | loss: 0.0032391242643343018\n",
      "epoch 631 | step 11 | loss: 0.0034845217393780986\n",
      "epoch 632 | step 0 | loss: 0.0002912203731014971\n",
      "epoch 632 | step 1 | loss: 0.0005895652467610912\n",
      "epoch 632 | step 2 | loss: 0.0008699469818553268\n",
      "epoch 632 | step 3 | loss: 0.001177588713886035\n",
      "epoch 632 | step 4 | loss: 0.0014759773611789436\n",
      "epoch 632 | step 5 | loss: 0.0017940343566480773\n",
      "epoch 632 | step 6 | loss: 0.0020769606738155685\n",
      "epoch 632 | step 7 | loss: 0.002370033502959551\n",
      "epoch 632 | step 8 | loss: 0.002634846064535015\n",
      "epoch 632 | step 9 | loss: 0.002926958883373149\n",
      "epoch 632 | step 10 | loss: 0.0032091749428923948\n",
      "epoch 632 | step 11 | loss: 0.003496130831226495\n",
      "epoch 633 | step 0 | loss: 0.00030117299019663\n",
      "epoch 633 | step 1 | loss: 0.000599602335009413\n",
      "epoch 633 | step 2 | loss: 0.0009082070993313528\n",
      "epoch 633 | step 3 | loss: 0.0011722026771829245\n",
      "epoch 633 | step 4 | loss: 0.001468170693810201\n",
      "epoch 633 | step 5 | loss: 0.0017535290516148346\n",
      "epoch 633 | step 6 | loss: 0.0020139166151442152\n",
      "epoch 633 | step 7 | loss: 0.0022849497641637155\n",
      "epoch 633 | step 8 | loss: 0.002561749136024476\n",
      "epoch 633 | step 9 | loss: 0.002861482848831197\n",
      "epoch 633 | step 10 | loss: 0.003175989855318833\n",
      "epoch 633 | step 11 | loss: 0.0035091951209451203\n",
      "epoch 634 | step 0 | loss: 0.0003153203539244021\n",
      "epoch 634 | step 1 | loss: 0.0006243188749057632\n",
      "epoch 634 | step 2 | loss: 0.0009024933045006626\n",
      "epoch 634 | step 3 | loss: 0.0011864929007647754\n",
      "epoch 634 | step 4 | loss: 0.0014694030545986223\n",
      "epoch 634 | step 5 | loss: 0.0017661298124861647\n",
      "epoch 634 | step 6 | loss: 0.0020286931374445994\n",
      "epoch 634 | step 7 | loss: 0.0023431478835135764\n",
      "epoch 634 | step 8 | loss: 0.0026556663309211027\n",
      "epoch 634 | step 9 | loss: 0.00293372961937089\n",
      "epoch 634 | step 10 | loss: 0.003204232480696476\n",
      "epoch 634 | step 11 | loss: 0.0034981166806375918\n",
      "epoch 635 | step 0 | loss: 0.0002865668223920757\n",
      "epoch 635 | step 1 | loss: 0.0006024257525183631\n",
      "epoch 635 | step 2 | loss: 0.0008833281126975354\n",
      "epoch 635 | step 3 | loss: 0.0011363548292210757\n",
      "epoch 635 | step 4 | loss: 0.0014294187985933752\n",
      "epoch 635 | step 5 | loss: 0.0017383408871753305\n",
      "epoch 635 | step 6 | loss: 0.002032454114205035\n",
      "epoch 635 | step 7 | loss: 0.002324000601030213\n",
      "epoch 635 | step 8 | loss: 0.0026292972878014325\n",
      "epoch 635 | step 9 | loss: 0.0029077082818064126\n",
      "epoch 635 | step 10 | loss: 0.0032105852258113806\n",
      "epoch 635 | step 11 | loss: 0.0034953721517467567\n",
      "epoch 636 | step 0 | loss: 0.00027358582457625137\n",
      "epoch 636 | step 1 | loss: 0.0005451452559794491\n",
      "epoch 636 | step 2 | loss: 0.0008441740130032254\n",
      "epoch 636 | step 3 | loss: 0.0011341045759303303\n",
      "epoch 636 | step 4 | loss: 0.001409243438269796\n",
      "epoch 636 | step 5 | loss: 0.0017103250324359606\n",
      "epoch 636 | step 6 | loss: 0.001996319866332365\n",
      "epoch 636 | step 7 | loss: 0.0023051022025456667\n",
      "epoch 636 | step 8 | loss: 0.0025639861033951595\n",
      "epoch 636 | step 9 | loss: 0.002866697034083052\n",
      "epoch 636 | step 10 | loss: 0.0031949772436776027\n",
      "epoch 636 | step 11 | loss: 0.00350126451854402\n",
      "epoch 637 | step 0 | loss: 0.00027969208364855705\n",
      "epoch 637 | step 1 | loss: 0.0005558885620862497\n",
      "epoch 637 | step 2 | loss: 0.0008562516105512173\n",
      "epoch 637 | step 3 | loss: 0.0011381255763045247\n",
      "epoch 637 | step 4 | loss: 0.001466769507951739\n",
      "epoch 637 | step 5 | loss: 0.0017668881303246286\n",
      "epoch 637 | step 6 | loss: 0.002053481479358964\n",
      "epoch 637 | step 7 | loss: 0.002346305024546045\n",
      "epoch 637 | step 8 | loss: 0.002611507388317421\n",
      "epoch 637 | step 9 | loss: 0.0029186760524371577\n",
      "epoch 637 | step 10 | loss: 0.003219418932668501\n",
      "epoch 637 | step 11 | loss: 0.0034916317322697247\n",
      "epoch 638 | step 0 | loss: 0.0002696975218246913\n",
      "epoch 638 | step 1 | loss: 0.0005673557112179716\n",
      "epoch 638 | step 2 | loss: 0.0008678904222818825\n",
      "epoch 638 | step 3 | loss: 0.0011794549920469918\n",
      "epoch 638 | step 4 | loss: 0.0014512542728239224\n",
      "epoch 638 | step 5 | loss: 0.0017491559460711657\n",
      "epoch 638 | step 6 | loss: 0.002051515365646394\n",
      "epoch 638 | step 7 | loss: 0.002334668019308854\n",
      "epoch 638 | step 8 | loss: 0.0026321011831577693\n",
      "epoch 638 | step 9 | loss: 0.0029280788465058574\n",
      "epoch 638 | step 10 | loss: 0.0031982095647046667\n",
      "epoch 638 | step 11 | loss: 0.0035002042004283426\n",
      "epoch 639 | step 0 | loss: 0.0003086644709895976\n",
      "epoch 639 | step 1 | loss: 0.0005819816165521099\n",
      "epoch 639 | step 2 | loss: 0.0008395835677740139\n",
      "epoch 639 | step 3 | loss: 0.00113492225836908\n",
      "epoch 639 | step 4 | loss: 0.001436455681591123\n",
      "epoch 639 | step 5 | loss: 0.0017424822174523461\n",
      "epoch 639 | step 6 | loss: 0.00204222492208433\n",
      "epoch 639 | step 7 | loss: 0.002328876454192708\n",
      "epoch 639 | step 8 | loss: 0.0026324456340689638\n",
      "epoch 639 | step 9 | loss: 0.0029436227388867794\n",
      "epoch 639 | step 10 | loss: 0.0032227296224919197\n",
      "epoch 639 | step 11 | loss: 0.003490196568685331\n",
      "epoch 640 | step 0 | loss: 0.00028999624360686433\n",
      "epoch 640 | step 1 | loss: 0.0005699200924006463\n",
      "epoch 640 | step 2 | loss: 0.0008468627750515201\n",
      "epoch 640 | step 3 | loss: 0.001115793202750989\n",
      "epoch 640 | step 4 | loss: 0.0014146114789944466\n",
      "epoch 640 | step 5 | loss: 0.0017267636462794736\n",
      "epoch 640 | step 6 | loss: 0.0020184793061838084\n",
      "epoch 640 | step 7 | loss: 0.0023328796208869833\n",
      "epoch 640 | step 8 | loss: 0.002610429592552231\n",
      "epoch 640 | step 9 | loss: 0.0028981560691470672\n",
      "epoch 640 | step 10 | loss: 0.0031846018184807576\n",
      "epoch 640 | step 11 | loss: 0.00350471655884508\n",
      "epoch 641 | step 0 | loss: 0.0002991587849476768\n",
      "epoch 641 | step 1 | loss: 0.0005811117934456961\n",
      "epoch 641 | step 2 | loss: 0.0008473176067617506\n",
      "epoch 641 | step 3 | loss: 0.0011311199260136506\n",
      "epoch 641 | step 4 | loss: 0.001427828335585026\n",
      "epoch 641 | step 5 | loss: 0.001734481942242666\n",
      "epoch 641 | step 6 | loss: 0.0020425630748905237\n",
      "epoch 641 | step 7 | loss: 0.0023230799645135108\n",
      "epoch 641 | step 8 | loss: 0.0026038555866861345\n",
      "epoch 641 | step 9 | loss: 0.002912344365113968\n",
      "epoch 641 | step 10 | loss: 0.0031969324413832954\n",
      "epoch 641 | step 11 | loss: 0.0035002287793733282\n",
      "epoch 642 | step 0 | loss: 0.00031903926515539056\n",
      "epoch 642 | step 1 | loss: 0.0006038746129922319\n",
      "epoch 642 | step 2 | loss: 0.0008983877435436436\n",
      "epoch 642 | step 3 | loss: 0.0012080528599248406\n",
      "epoch 642 | step 4 | loss: 0.0014648794493283333\n",
      "epoch 642 | step 5 | loss: 0.0017534718814771733\n",
      "epoch 642 | step 6 | loss: 0.0020489708032493887\n",
      "epoch 642 | step 7 | loss: 0.002322599493929679\n",
      "epoch 642 | step 8 | loss: 0.002612484652233204\n",
      "epoch 642 | step 9 | loss: 0.002912766456076143\n",
      "epoch 642 | step 10 | loss: 0.0032149232852897447\n",
      "epoch 642 | step 11 | loss: 0.003493045573310577\n",
      "epoch 643 | step 0 | loss: 0.0002829780085541689\n",
      "epoch 643 | step 1 | loss: 0.0006089300238203688\n",
      "epoch 643 | step 2 | loss: 0.0008895606071899771\n",
      "epoch 643 | step 3 | loss: 0.0011759115188409203\n",
      "epoch 643 | step 4 | loss: 0.0014554402216576665\n",
      "epoch 643 | step 5 | loss: 0.0017484836823074476\n",
      "epoch 643 | step 6 | loss: 0.0020559931069412773\n",
      "epoch 643 | step 7 | loss: 0.002342760848449739\n",
      "epoch 643 | step 8 | loss: 0.002639026707116613\n",
      "epoch 643 | step 9 | loss: 0.0029007447079820578\n",
      "epoch 643 | step 10 | loss: 0.0032108960722799437\n",
      "epoch 643 | step 11 | loss: 0.0034944336458111068\n",
      "epoch 644 | step 0 | loss: 0.00027794161946728387\n",
      "epoch 644 | step 1 | loss: 0.000585781140919682\n",
      "epoch 644 | step 2 | loss: 0.0008731365469948017\n",
      "epoch 644 | step 3 | loss: 0.0011977172894321474\n",
      "epoch 644 | step 4 | loss: 0.001479763315511717\n",
      "epoch 644 | step 5 | loss: 0.0017857981051145684\n",
      "epoch 644 | step 6 | loss: 0.002074562411623221\n",
      "epoch 644 | step 7 | loss: 0.0023707995581176526\n",
      "epoch 644 | step 8 | loss: 0.002645909214417385\n",
      "epoch 644 | step 9 | loss: 0.002917818472311567\n",
      "epoch 644 | step 10 | loss: 0.0032123832884569914\n",
      "epoch 644 | step 11 | loss: 0.0034935311579555595\n",
      "epoch 645 | step 0 | loss: 0.00028279040761281584\n",
      "epoch 645 | step 1 | loss: 0.0005773909953122569\n",
      "epoch 645 | step 2 | loss: 0.0008654824158745948\n",
      "epoch 645 | step 3 | loss: 0.0011457085210582116\n",
      "epoch 645 | step 4 | loss: 0.0014351076772556588\n",
      "epoch 645 | step 5 | loss: 0.001727789238157745\n",
      "epoch 645 | step 6 | loss: 0.0020304464212762808\n",
      "epoch 645 | step 7 | loss: 0.00233402821026015\n",
      "epoch 645 | step 8 | loss: 0.00261221558621893\n",
      "epoch 645 | step 9 | loss: 0.0029005325161809902\n",
      "epoch 645 | step 10 | loss: 0.0031754845876988515\n",
      "epoch 645 | step 11 | loss: 0.003507794876011862\n",
      "epoch 646 | step 0 | loss: 0.0003103384633949004\n",
      "epoch 646 | step 1 | loss: 0.0006140944026064597\n",
      "epoch 646 | step 2 | loss: 0.0008816058236865894\n",
      "epoch 646 | step 3 | loss: 0.0011783349659472745\n",
      "epoch 646 | step 4 | loss: 0.0014488743881237124\n",
      "epoch 646 | step 5 | loss: 0.0017618519512088936\n",
      "epoch 646 | step 6 | loss: 0.0020532656166168564\n",
      "epoch 646 | step 7 | loss: 0.0023460068117966723\n",
      "epoch 646 | step 8 | loss: 0.0026336773606365645\n",
      "epoch 646 | step 9 | loss: 0.0029242382203018566\n",
      "epoch 646 | step 10 | loss: 0.003206562917106971\n",
      "epoch 646 | step 11 | loss: 0.00349575743133143\n",
      "epoch 647 | step 0 | loss: 0.00027511697822991054\n",
      "epoch 647 | step 1 | loss: 0.0006287742466003027\n",
      "epoch 647 | step 2 | loss: 0.000899764308486406\n",
      "epoch 647 | step 3 | loss: 0.001174859409811386\n",
      "epoch 647 | step 4 | loss: 0.00149503740598901\n",
      "epoch 647 | step 5 | loss: 0.0017646092785769632\n",
      "epoch 647 | step 6 | loss: 0.002044034143703991\n",
      "epoch 647 | step 7 | loss: 0.0023277374868753463\n",
      "epoch 647 | step 8 | loss: 0.0026244507947479267\n",
      "epoch 647 | step 9 | loss: 0.0029007070648046237\n",
      "epoch 647 | step 10 | loss: 0.003216922861515216\n",
      "epoch 647 | step 11 | loss: 0.003491949681825395\n",
      "epoch 648 | step 0 | loss: 0.0002876204031940088\n",
      "epoch 648 | step 1 | loss: 0.0005999486450064743\n",
      "epoch 648 | step 2 | loss: 0.0008746425054408334\n",
      "epoch 648 | step 3 | loss: 0.001153631877205431\n",
      "epoch 648 | step 4 | loss: 0.0014600116344634392\n",
      "epoch 648 | step 5 | loss: 0.0017447610012683694\n",
      "epoch 648 | step 6 | loss: 0.002045658921361351\n",
      "epoch 648 | step 7 | loss: 0.002353636481348175\n",
      "epoch 648 | step 8 | loss: 0.002653551009027251\n",
      "epoch 648 | step 9 | loss: 0.0029169784002700655\n",
      "epoch 648 | step 10 | loss: 0.0032048170848370063\n",
      "epoch 648 | step 11 | loss: 0.0034962671109651324\n",
      "epoch 649 | step 0 | loss: 0.0003014746618477107\n",
      "epoch 649 | step 1 | loss: 0.0006292078512283363\n",
      "epoch 649 | step 2 | loss: 0.000949481434460853\n",
      "epoch 649 | step 3 | loss: 0.001248495166847135\n",
      "epoch 649 | step 4 | loss: 0.0015181789867171492\n",
      "epoch 649 | step 5 | loss: 0.001789132625342444\n",
      "epoch 649 | step 6 | loss: 0.00206845051629899\n",
      "epoch 649 | step 7 | loss: 0.0023651533164460847\n",
      "epoch 649 | step 8 | loss: 0.0026491235750747463\n",
      "epoch 649 | step 9 | loss: 0.002936980368216403\n",
      "epoch 649 | step 10 | loss: 0.0032283335874002074\n",
      "epoch 649 | step 11 | loss: 0.0034870369328687837\n",
      "epoch 650 | step 0 | loss: 0.00031233848505937917\n",
      "epoch 650 | step 1 | loss: 0.0006026196324668057\n",
      "epoch 650 | step 2 | loss: 0.0009138977382345981\n",
      "epoch 650 | step 3 | loss: 0.0012040339968828154\n",
      "epoch 650 | step 4 | loss: 0.0015074500737284877\n",
      "epoch 650 | step 5 | loss: 0.001781859253708378\n",
      "epoch 650 | step 6 | loss: 0.0020566658003051018\n",
      "epoch 650 | step 7 | loss: 0.002342249392789399\n",
      "epoch 650 | step 8 | loss: 0.0026313168988102045\n",
      "epoch 650 | step 9 | loss: 0.0029170404020452704\n",
      "epoch 650 | step 10 | loss: 0.003198368687061661\n",
      "epoch 650 | step 11 | loss: 0.0034986322898590792\n",
      "epoch 651 | step 0 | loss: 0.00028618510071581996\n",
      "epoch 651 | step 1 | loss: 0.0005998417809291594\n",
      "epoch 651 | step 2 | loss: 0.0008723955904679868\n",
      "epoch 651 | step 3 | loss: 0.0011662839016594036\n",
      "epoch 651 | step 4 | loss: 0.0014250058037711328\n",
      "epoch 651 | step 5 | loss: 0.0017512974083063525\n",
      "epoch 651 | step 6 | loss: 0.0020386651663743703\n",
      "epoch 651 | step 7 | loss: 0.0023388078856185707\n",
      "epoch 651 | step 8 | loss: 0.002614036965069153\n",
      "epoch 651 | step 9 | loss: 0.0029136570819792663\n",
      "epoch 651 | step 10 | loss: 0.003211921519513182\n",
      "epoch 651 | step 11 | loss: 0.0034931759917738434\n",
      "epoch 652 | step 0 | loss: 0.0002674400028133961\n",
      "epoch 652 | step 1 | loss: 0.0005593314793511976\n",
      "epoch 652 | step 2 | loss: 0.0008616618428900112\n",
      "epoch 652 | step 3 | loss: 0.0011808138417540488\n",
      "epoch 652 | step 4 | loss: 0.0014602579228461775\n",
      "epoch 652 | step 5 | loss: 0.0017363288488210639\n",
      "epoch 652 | step 6 | loss: 0.0020229178515196525\n",
      "epoch 652 | step 7 | loss: 0.0023054345949185514\n",
      "epoch 652 | step 8 | loss: 0.002596727666418145\n",
      "epoch 652 | step 9 | loss: 0.002893952609788128\n",
      "epoch 652 | step 10 | loss: 0.0031981843825393424\n",
      "epoch 652 | step 11 | loss: 0.0034985170137046785\n",
      "epoch 653 | step 0 | loss: 0.0002921910181039225\n",
      "epoch 653 | step 1 | loss: 0.0005725468547888773\n",
      "epoch 653 | step 2 | loss: 0.0008722249258302347\n",
      "epoch 653 | step 3 | loss: 0.0011544512816137908\n",
      "epoch 653 | step 4 | loss: 0.001438214371589311\n",
      "epoch 653 | step 5 | loss: 0.001735156490968688\n",
      "epoch 653 | step 6 | loss: 0.0020216711863199543\n",
      "epoch 653 | step 7 | loss: 0.0023161820149182645\n",
      "epoch 653 | step 8 | loss: 0.002641878570257364\n",
      "epoch 653 | step 9 | loss: 0.0029019846456235653\n",
      "epoch 653 | step 10 | loss: 0.0031991891381077664\n",
      "epoch 653 | step 11 | loss: 0.00349799842648243\n",
      "epoch 654 | step 0 | loss: 0.00028619994577830783\n",
      "epoch 654 | step 1 | loss: 0.0005681865183155268\n",
      "epoch 654 | step 2 | loss: 0.000904017124152231\n",
      "epoch 654 | step 3 | loss: 0.0011899519782047157\n",
      "epoch 654 | step 4 | loss: 0.0014788366069405962\n",
      "epoch 654 | step 5 | loss: 0.001776261971847023\n",
      "epoch 654 | step 6 | loss: 0.002063544325706008\n",
      "epoch 654 | step 7 | loss: 0.002358526719905452\n",
      "epoch 654 | step 8 | loss: 0.0026494360754784905\n",
      "epoch 654 | step 9 | loss: 0.0029196208779403037\n",
      "epoch 654 | step 10 | loss: 0.0032188530200645076\n",
      "epoch 654 | step 11 | loss: 0.003490069679300879\n",
      "epoch 655 | step 0 | loss: 0.0003265950492800623\n",
      "epoch 655 | step 1 | loss: 0.0006071135386140206\n",
      "epoch 655 | step 2 | loss: 0.0008992405194824634\n",
      "epoch 655 | step 3 | loss: 0.001188888626520182\n",
      "epoch 655 | step 4 | loss: 0.0014887353870865212\n",
      "epoch 655 | step 5 | loss: 0.0018007576156697704\n",
      "epoch 655 | step 6 | loss: 0.0020935315285610986\n",
      "epoch 655 | step 7 | loss: 0.002396852694549214\n",
      "epoch 655 | step 8 | loss: 0.0026542150199511585\n",
      "epoch 655 | step 9 | loss: 0.0029468681576739148\n",
      "epoch 655 | step 10 | loss: 0.0032123856527829925\n",
      "epoch 655 | step 11 | loss: 0.0034926034144688936\n",
      "epoch 656 | step 0 | loss: 0.00029371672333025075\n",
      "epoch 656 | step 1 | loss: 0.0005774588493171512\n",
      "epoch 656 | step 2 | loss: 0.0008752956364963751\n",
      "epoch 656 | step 3 | loss: 0.0011452534753475048\n",
      "epoch 656 | step 4 | loss: 0.0014626629609473887\n",
      "epoch 656 | step 5 | loss: 0.0017358802089804594\n",
      "epoch 656 | step 6 | loss: 0.0020015972381852284\n",
      "epoch 656 | step 7 | loss: 0.002284154309285263\n",
      "epoch 656 | step 8 | loss: 0.0025856125299751334\n",
      "epoch 656 | step 9 | loss: 0.0028745643499918086\n",
      "epoch 656 | step 10 | loss: 0.0032002255245026845\n",
      "epoch 656 | step 11 | loss: 0.003497054931676216\n",
      "epoch 657 | step 0 | loss: 0.00028488776820631925\n",
      "epoch 657 | step 1 | loss: 0.0005893630320004271\n",
      "epoch 657 | step 2 | loss: 0.0008578378143866644\n",
      "epoch 657 | step 3 | loss: 0.0011549631108955065\n",
      "epoch 657 | step 4 | loss: 0.0014619108790775621\n",
      "epoch 657 | step 5 | loss: 0.0017464562558218843\n",
      "epoch 657 | step 6 | loss: 0.002043342623478421\n",
      "epoch 657 | step 7 | loss: 0.0023298066477009764\n",
      "epoch 657 | step 8 | loss: 0.0026224869710222915\n",
      "epoch 657 | step 9 | loss: 0.002898508637103875\n",
      "epoch 657 | step 10 | loss: 0.0031882886350085293\n",
      "epoch 657 | step 11 | loss: 0.003501671457357529\n",
      "epoch 658 | step 0 | loss: 0.0002876278781443422\n",
      "epoch 658 | step 1 | loss: 0.0005694097194381632\n",
      "epoch 658 | step 2 | loss: 0.0008829344056334116\n",
      "epoch 658 | step 3 | loss: 0.001150954903039928\n",
      "epoch 658 | step 4 | loss: 0.0014776541157756474\n",
      "epoch 658 | step 5 | loss: 0.0017530249906849416\n",
      "epoch 658 | step 6 | loss: 0.0020450853234078624\n",
      "epoch 658 | step 7 | loss: 0.0023290697182082242\n",
      "epoch 658 | step 8 | loss: 0.002621582691932357\n",
      "epoch 658 | step 9 | loss: 0.002921298522341566\n",
      "epoch 658 | step 10 | loss: 0.003220591994200772\n",
      "epoch 658 | step 11 | loss: 0.003489242667237328\n",
      "epoch 659 | step 0 | loss: 0.00027906727034069534\n",
      "epoch 659 | step 1 | loss: 0.0005751690736658652\n",
      "epoch 659 | step 2 | loss: 0.0008964151122541515\n",
      "epoch 659 | step 3 | loss: 0.0011982359743970545\n",
      "epoch 659 | step 4 | loss: 0.0014798581693597195\n",
      "epoch 659 | step 5 | loss: 0.0017860680547564837\n",
      "epoch 659 | step 6 | loss: 0.0020769128041744087\n",
      "epoch 659 | step 7 | loss: 0.002339565728635153\n",
      "epoch 659 | step 8 | loss: 0.002599588613353943\n",
      "epoch 659 | step 9 | loss: 0.0029101543367172607\n",
      "epoch 659 | step 10 | loss: 0.003200921806936733\n",
      "epoch 659 | step 11 | loss: 0.0034966879871835025\n",
      "epoch 660 | step 0 | loss: 0.00027729378103663203\n",
      "epoch 660 | step 1 | loss: 0.0005795218248772063\n",
      "epoch 660 | step 2 | loss: 0.000861285171019554\n",
      "epoch 660 | step 3 | loss: 0.0011602654686712472\n",
      "epoch 660 | step 4 | loss: 0.0014404594926279608\n",
      "epoch 660 | step 5 | loss: 0.0017447442802968594\n",
      "epoch 660 | step 6 | loss: 0.002028056878445807\n",
      "epoch 660 | step 7 | loss: 0.0023286188018794562\n",
      "epoch 660 | step 8 | loss: 0.0026266853851273234\n",
      "epoch 660 | step 9 | loss: 0.002921364463133219\n",
      "epoch 660 | step 10 | loss: 0.0032047495306048944\n",
      "epoch 660 | step 11 | loss: 0.0034949978409160596\n",
      "epoch 661 | step 0 | loss: 0.0003002123878713763\n",
      "epoch 661 | step 1 | loss: 0.0006059275155501254\n",
      "epoch 661 | step 2 | loss: 0.0008893633233223536\n",
      "epoch 661 | step 3 | loss: 0.0011607979589318138\n",
      "epoch 661 | step 4 | loss: 0.001456528669734497\n",
      "epoch 661 | step 5 | loss: 0.0017613614151785494\n",
      "epoch 661 | step 6 | loss: 0.002038874626197274\n",
      "epoch 661 | step 7 | loss: 0.0023298850112392524\n",
      "epoch 661 | step 8 | loss: 0.0026168778656474336\n",
      "epoch 661 | step 9 | loss: 0.002942093345689274\n",
      "epoch 661 | step 10 | loss: 0.003210778092548153\n",
      "epoch 661 | step 11 | loss: 0.003492492806114233\n",
      "epoch 662 | step 0 | loss: 0.0003068181309307262\n",
      "epoch 662 | step 1 | loss: 0.0005993964780282052\n",
      "epoch 662 | step 2 | loss: 0.0008820000835077887\n",
      "epoch 662 | step 3 | loss: 0.0011632277192463\n",
      "epoch 662 | step 4 | loss: 0.001443894638445279\n",
      "epoch 662 | step 5 | loss: 0.0017167156567487363\n",
      "epoch 662 | step 6 | loss: 0.002007620206872782\n",
      "epoch 662 | step 7 | loss: 0.00230825716206966\n",
      "epoch 662 | step 8 | loss: 0.0025887855218820494\n",
      "epoch 662 | step 9 | loss: 0.0029090598005925153\n",
      "epoch 662 | step 10 | loss: 0.003206812898715535\n",
      "epoch 662 | step 11 | loss: 0.0034939997719989776\n",
      "epoch 663 | step 0 | loss: 0.00030925066756098675\n",
      "epoch 663 | step 1 | loss: 0.0005822721108275658\n",
      "epoch 663 | step 2 | loss: 0.0008848337497976363\n",
      "epoch 663 | step 3 | loss: 0.0011558713054135306\n",
      "epoch 663 | step 4 | loss: 0.0014473562103085266\n",
      "epoch 663 | step 5 | loss: 0.001750030862037385\n",
      "epoch 663 | step 6 | loss: 0.0020359755828904718\n",
      "epoch 663 | step 7 | loss: 0.002328117242423537\n",
      "epoch 663 | step 8 | loss: 0.0026244800842587577\n",
      "epoch 663 | step 9 | loss: 0.00290793106391537\n",
      "epoch 663 | step 10 | loss: 0.003206646263976339\n",
      "epoch 663 | step 11 | loss: 0.0034942436961375757\n",
      "epoch 664 | step 0 | loss: 0.00032401697575537667\n",
      "epoch 664 | step 1 | loss: 0.0005879504023411768\n",
      "epoch 664 | step 2 | loss: 0.0008640409873417173\n",
      "epoch 664 | step 3 | loss: 0.0011369121942551596\n",
      "epoch 664 | step 4 | loss: 0.001445755068274013\n",
      "epoch 664 | step 5 | loss: 0.0017303309518455355\n",
      "epoch 664 | step 6 | loss: 0.0020170194551685145\n",
      "epoch 664 | step 7 | loss: 0.002297996386332468\n",
      "epoch 664 | step 8 | loss: 0.0025776183458148826\n",
      "epoch 664 | step 9 | loss: 0.002862297934748422\n",
      "epoch 664 | step 10 | loss: 0.0031773652421246508\n",
      "epoch 664 | step 11 | loss: 0.0035057845423949554\n",
      "epoch 665 | step 0 | loss: 0.00030388365037358454\n",
      "epoch 665 | step 1 | loss: 0.0006297360307784426\n",
      "epoch 665 | step 2 | loss: 0.0009224756085458308\n",
      "epoch 665 | step 3 | loss: 0.001208473607511152\n",
      "epoch 665 | step 4 | loss: 0.0014703472869100058\n",
      "epoch 665 | step 5 | loss: 0.0017385108317602954\n",
      "epoch 665 | step 6 | loss: 0.0020397561877794294\n",
      "epoch 665 | step 7 | loss: 0.002327177004707995\n",
      "epoch 665 | step 8 | loss: 0.0026182607698605692\n",
      "epoch 665 | step 9 | loss: 0.0029201947110129196\n",
      "epoch 665 | step 10 | loss: 0.0032210191410255253\n",
      "epoch 665 | step 11 | loss: 0.00348851230134807\n",
      "epoch 666 | step 0 | loss: 0.00027430729410051705\n",
      "epoch 666 | step 1 | loss: 0.0005771870654193929\n",
      "epoch 666 | step 2 | loss: 0.0008513687900412042\n",
      "epoch 666 | step 3 | loss: 0.0011327115537882152\n",
      "epoch 666 | step 4 | loss: 0.001455626750004262\n",
      "epoch 666 | step 5 | loss: 0.0017440069106056106\n",
      "epoch 666 | step 6 | loss: 0.002029849752150319\n",
      "epoch 666 | step 7 | loss: 0.002306705019334046\n",
      "epoch 666 | step 8 | loss: 0.0025999880252109606\n",
      "epoch 666 | step 9 | loss: 0.0028987157950791233\n",
      "epoch 666 | step 10 | loss: 0.0031928161875859075\n",
      "epoch 666 | step 11 | loss: 0.003499396251943844\n",
      "epoch 667 | step 0 | loss: 0.0003090782264487944\n",
      "epoch 667 | step 1 | loss: 0.0005966805160641921\n",
      "epoch 667 | step 2 | loss: 0.0008778816527243063\n",
      "epoch 667 | step 3 | loss: 0.0011759428457601734\n",
      "epoch 667 | step 4 | loss: 0.0014682823559607888\n",
      "epoch 667 | step 5 | loss: 0.0017711495966593713\n",
      "epoch 667 | step 6 | loss: 0.0020792221690713853\n",
      "epoch 667 | step 7 | loss: 0.0023570695511449307\n",
      "epoch 667 | step 8 | loss: 0.002638802591875767\n",
      "epoch 667 | step 9 | loss: 0.0029329799339019016\n",
      "epoch 667 | step 10 | loss: 0.0032199725371999154\n",
      "epoch 667 | step 11 | loss: 0.00348863979525625\n",
      "epoch 668 | step 0 | loss: 0.0002749486849647883\n",
      "epoch 668 | step 1 | loss: 0.0005732233808927773\n",
      "epoch 668 | step 2 | loss: 0.0008785598681915145\n",
      "epoch 668 | step 3 | loss: 0.00118860358926898\n",
      "epoch 668 | step 4 | loss: 0.0014777634912530716\n",
      "epoch 668 | step 5 | loss: 0.0017396273255063286\n",
      "epoch 668 | step 6 | loss: 0.0020324313704336468\n",
      "epoch 668 | step 7 | loss: 0.002328577953091351\n",
      "epoch 668 | step 8 | loss: 0.002604496559987486\n",
      "epoch 668 | step 9 | loss: 0.0028944125613805686\n",
      "epoch 668 | step 10 | loss: 0.0032012880986297897\n",
      "epoch 668 | step 11 | loss: 0.0034955806836727945\n",
      "epoch 669 | step 0 | loss: 0.0003153146909360883\n",
      "epoch 669 | step 1 | loss: 0.0005683978566132223\n",
      "epoch 669 | step 2 | loss: 0.0008643880886195752\n",
      "epoch 669 | step 3 | loss: 0.0011433951497304376\n",
      "epoch 669 | step 4 | loss: 0.0014372639331589935\n",
      "epoch 669 | step 5 | loss: 0.0017617058464421624\n",
      "epoch 669 | step 6 | loss: 0.0020616619333739365\n",
      "epoch 669 | step 7 | loss: 0.002343992746584132\n",
      "epoch 669 | step 8 | loss: 0.00263265748970643\n",
      "epoch 669 | step 9 | loss: 0.0029170769560759612\n",
      "epoch 669 | step 10 | loss: 0.003203442053333419\n",
      "epoch 669 | step 11 | loss: 0.003494802697793372\n",
      "epoch 670 | step 0 | loss: 0.0002709192407847099\n",
      "epoch 670 | step 1 | loss: 0.0005590951060627945\n",
      "epoch 670 | step 2 | loss: 0.0008571066683020091\n",
      "epoch 670 | step 3 | loss: 0.0011501974964131794\n",
      "epoch 670 | step 4 | loss: 0.0014276765347284279\n",
      "epoch 670 | step 5 | loss: 0.0017494989094288837\n",
      "epoch 670 | step 6 | loss: 0.0020298877396140924\n",
      "epoch 670 | step 7 | loss: 0.0023207234598904146\n",
      "epoch 670 | step 8 | loss: 0.0026202025255493296\n",
      "epoch 670 | step 9 | loss: 0.0029212949550713636\n",
      "epoch 670 | step 10 | loss: 0.003209336338786987\n",
      "epoch 670 | step 11 | loss: 0.003492042968093493\n",
      "epoch 671 | step 0 | loss: 0.00029112438759875325\n",
      "epoch 671 | step 1 | loss: 0.0005862859030336222\n",
      "epoch 671 | step 2 | loss: 0.0009045517787971315\n",
      "epoch 671 | step 3 | loss: 0.0011808349858267\n",
      "epoch 671 | step 4 | loss: 0.001455682675080898\n",
      "epoch 671 | step 5 | loss: 0.001748233451508114\n",
      "epoch 671 | step 6 | loss: 0.0020295806450722417\n",
      "epoch 671 | step 7 | loss: 0.0023466262760797564\n",
      "epoch 671 | step 8 | loss: 0.002617684360216056\n",
      "epoch 671 | step 9 | loss: 0.0029083623940359985\n",
      "epoch 671 | step 10 | loss: 0.0032109005963411367\n",
      "epoch 671 | step 11 | loss: 0.003491557824467606\n",
      "epoch 672 | step 0 | loss: 0.00032682153520578207\n",
      "epoch 672 | step 1 | loss: 0.0006288649560386398\n",
      "epoch 672 | step 2 | loss: 0.000906685024111906\n",
      "epoch 672 | step 3 | loss: 0.0012126327772022595\n",
      "epoch 672 | step 4 | loss: 0.001465680145375473\n",
      "epoch 672 | step 5 | loss: 0.0017720648369814805\n",
      "epoch 672 | step 6 | loss: 0.0020616418800588356\n",
      "epoch 672 | step 7 | loss: 0.0023239112672949237\n",
      "epoch 672 | step 8 | loss: 0.0026095149458032576\n",
      "epoch 672 | step 9 | loss: 0.0028943740439814295\n",
      "epoch 672 | step 10 | loss: 0.0031962791071986412\n",
      "epoch 672 | step 11 | loss: 0.003497211190524474\n",
      "epoch 673 | step 0 | loss: 0.00029533571311370077\n",
      "epoch 673 | step 1 | loss: 0.0005763046553943789\n",
      "epoch 673 | step 2 | loss: 0.0008642641462145373\n",
      "epoch 673 | step 3 | loss: 0.0011572202264080312\n",
      "epoch 673 | step 4 | loss: 0.0014419521201676495\n",
      "epoch 673 | step 5 | loss: 0.001718153052833184\n",
      "epoch 673 | step 6 | loss: 0.002027173950877784\n",
      "epoch 673 | step 7 | loss: 0.0023155168891229798\n",
      "epoch 673 | step 8 | loss: 0.0026243431715423727\n",
      "epoch 673 | step 9 | loss: 0.0028921166189365155\n",
      "epoch 673 | step 10 | loss: 0.0032056393955195795\n",
      "epoch 673 | step 11 | loss: 0.0034935031047674397\n",
      "epoch 674 | step 0 | loss: 0.0002794947013725199\n",
      "epoch 674 | step 1 | loss: 0.0005676108906730099\n",
      "epoch 674 | step 2 | loss: 0.0008567378053397192\n",
      "epoch 674 | step 3 | loss: 0.0011418458904082997\n",
      "epoch 674 | step 4 | loss: 0.001434717299934001\n",
      "epoch 674 | step 5 | loss: 0.0017094871179439246\n",
      "epoch 674 | step 6 | loss: 0.002016991687428204\n",
      "epoch 674 | step 7 | loss: 0.0023400494968034637\n",
      "epoch 674 | step 8 | loss: 0.0026276589146700265\n",
      "epoch 674 | step 9 | loss: 0.0028981203568786045\n",
      "epoch 674 | step 10 | loss: 0.0032148149800243922\n",
      "epoch 674 | step 11 | loss: 0.003489417120152797\n",
      "epoch 675 | step 0 | loss: 0.0002858035383938606\n",
      "epoch 675 | step 1 | loss: 0.0005739989726689595\n",
      "epoch 675 | step 2 | loss: 0.0008577648258102467\n",
      "epoch 675 | step 3 | loss: 0.0011141601908586922\n",
      "epoch 675 | step 4 | loss: 0.0014064005839393868\n",
      "epoch 675 | step 5 | loss: 0.0016857595385515612\n",
      "epoch 675 | step 6 | loss: 0.00198097393449171\n",
      "epoch 675 | step 7 | loss: 0.0022963687693423026\n",
      "epoch 675 | step 8 | loss: 0.0025794744593098766\n",
      "epoch 675 | step 9 | loss: 0.0029029740894363307\n",
      "epoch 675 | step 10 | loss: 0.0031980853173797035\n",
      "epoch 675 | step 11 | loss: 0.003495969618789474\n",
      "epoch 676 | step 0 | loss: 0.0003057167784246453\n",
      "epoch 676 | step 1 | loss: 0.0005971687099229694\n",
      "epoch 676 | step 2 | loss: 0.0008849441877859293\n",
      "epoch 676 | step 3 | loss: 0.00119989687362395\n",
      "epoch 676 | step 4 | loss: 0.001487640430667286\n",
      "epoch 676 | step 5 | loss: 0.0017829959270675932\n",
      "epoch 676 | step 6 | loss: 0.0020771084889010514\n",
      "epoch 676 | step 7 | loss: 0.002378403519707869\n",
      "epoch 676 | step 8 | loss: 0.002636563843790465\n",
      "epoch 676 | step 9 | loss: 0.002919466630070289\n",
      "epoch 676 | step 10 | loss: 0.003194740074861814\n",
      "epoch 676 | step 11 | loss: 0.003497653365402937\n",
      "epoch 677 | step 0 | loss: 0.00030498408908595455\n",
      "epoch 677 | step 1 | loss: 0.0006086059135459457\n",
      "epoch 677 | step 2 | loss: 0.0008948044379777909\n",
      "epoch 677 | step 3 | loss: 0.0011851932161696832\n",
      "epoch 677 | step 4 | loss: 0.001478500873989159\n",
      "epoch 677 | step 5 | loss: 0.0017825453572457568\n",
      "epoch 677 | step 6 | loss: 0.0021005443402282373\n",
      "epoch 677 | step 7 | loss: 0.0023773973870959584\n",
      "epoch 677 | step 8 | loss: 0.0026526994997628883\n",
      "epoch 677 | step 9 | loss: 0.002930111783964624\n",
      "epoch 677 | step 10 | loss: 0.0031955158792959662\n",
      "epoch 677 | step 11 | loss: 0.003496997296936189\n",
      "epoch 678 | step 0 | loss: 0.00026507714373765325\n",
      "epoch 678 | step 1 | loss: 0.0005877875036105546\n",
      "epoch 678 | step 2 | loss: 0.0009007791709576852\n",
      "epoch 678 | step 3 | loss: 0.0011807924564152597\n",
      "epoch 678 | step 4 | loss: 0.0014892007915706849\n",
      "epoch 678 | step 5 | loss: 0.001782983524989995\n",
      "epoch 678 | step 6 | loss: 0.0020413637583732713\n",
      "epoch 678 | step 7 | loss: 0.0023435012506917902\n",
      "epoch 678 | step 8 | loss: 0.002646576495626083\n",
      "epoch 678 | step 9 | loss: 0.002933410304163376\n",
      "epoch 678 | step 10 | loss: 0.003205594885491013\n",
      "epoch 678 | step 11 | loss: 0.0034928795833449543\n",
      "epoch 679 | step 0 | loss: 0.0002893042509156818\n",
      "epoch 679 | step 1 | loss: 0.0005858389695706241\n",
      "epoch 679 | step 2 | loss: 0.0008642717787036445\n",
      "epoch 679 | step 3 | loss: 0.0011498878032521044\n",
      "epoch 679 | step 4 | loss: 0.0014622095896955367\n",
      "epoch 679 | step 5 | loss: 0.001744479232101867\n",
      "epoch 679 | step 6 | loss: 0.0020261647326182373\n",
      "epoch 679 | step 7 | loss: 0.0023342865524792187\n",
      "epoch 679 | step 8 | loss: 0.0026179424388633075\n",
      "epoch 679 | step 9 | loss: 0.00291389621515608\n",
      "epoch 679 | step 10 | loss: 0.0032087729576524004\n",
      "epoch 679 | step 11 | loss: 0.0034915962263970075\n",
      "epoch 680 | step 0 | loss: 0.0002944886600793344\n",
      "epoch 680 | step 1 | loss: 0.0006210942382663548\n",
      "epoch 680 | step 2 | loss: 0.0009282851021467673\n",
      "epoch 680 | step 3 | loss: 0.0012079270076998947\n",
      "epoch 680 | step 4 | loss: 0.0015215671625403843\n",
      "epoch 680 | step 5 | loss: 0.0018083947559310704\n",
      "epoch 680 | step 6 | loss: 0.002117978721256555\n",
      "epoch 680 | step 7 | loss: 0.0023916162602622302\n",
      "epoch 680 | step 8 | loss: 0.0026916284476008216\n",
      "epoch 680 | step 9 | loss: 0.002952859426102887\n",
      "epoch 680 | step 10 | loss: 0.0032236221421504214\n",
      "epoch 680 | step 11 | loss: 0.0034854734905379373\n",
      "epoch 681 | step 0 | loss: 0.0002722313178026987\n",
      "epoch 681 | step 1 | loss: 0.0005747342506988724\n",
      "epoch 681 | step 2 | loss: 0.0008829718340429166\n",
      "epoch 681 | step 3 | loss: 0.0011940220174270328\n",
      "epoch 681 | step 4 | loss: 0.0014634576355147038\n",
      "epoch 681 | step 5 | loss: 0.0017437094359344187\n",
      "epoch 681 | step 6 | loss: 0.002043952958678627\n",
      "epoch 681 | step 7 | loss: 0.0023401386607484567\n",
      "epoch 681 | step 8 | loss: 0.0026234198612819716\n",
      "epoch 681 | step 9 | loss: 0.002914740717164661\n",
      "epoch 681 | step 10 | loss: 0.003197355471402665\n",
      "epoch 681 | step 11 | loss: 0.003495699374991162\n",
      "epoch 682 | step 0 | loss: 0.00024941460489725603\n",
      "epoch 682 | step 1 | loss: 0.0005704912828388509\n",
      "epoch 682 | step 2 | loss: 0.0008754569412420014\n",
      "epoch 682 | step 3 | loss: 0.0011497966173941436\n",
      "epoch 682 | step 4 | loss: 0.001417953583255203\n",
      "epoch 682 | step 5 | loss: 0.0017292826663584094\n",
      "epoch 682 | step 6 | loss: 0.0020345909415411187\n",
      "epoch 682 | step 7 | loss: 0.0023421975391235243\n",
      "epoch 682 | step 8 | loss: 0.0026221417067036927\n",
      "epoch 682 | step 9 | loss: 0.002916854502330101\n",
      "epoch 682 | step 10 | loss: 0.0031933799102574906\n",
      "epoch 682 | step 11 | loss: 0.0034973194013286552\n",
      "epoch 683 | step 0 | loss: 0.0002626517303433155\n",
      "epoch 683 | step 1 | loss: 0.0005594676662779267\n",
      "epoch 683 | step 2 | loss: 0.000857595594620178\n",
      "epoch 683 | step 3 | loss: 0.0011563774343306815\n",
      "epoch 683 | step 4 | loss: 0.0014340346357519715\n",
      "epoch 683 | step 5 | loss: 0.001723928532127617\n",
      "epoch 683 | step 6 | loss: 0.00199903693996502\n",
      "epoch 683 | step 7 | loss: 0.0022945851047912373\n",
      "epoch 683 | step 8 | loss: 0.0025890286904701595\n",
      "epoch 683 | step 9 | loss: 0.002878612532683649\n",
      "epoch 683 | step 10 | loss: 0.003189259315386572\n",
      "epoch 683 | step 11 | loss: 0.0034987487768532942\n",
      "epoch 684 | step 0 | loss: 0.000322747413874013\n",
      "epoch 684 | step 1 | loss: 0.0006209515292840429\n",
      "epoch 684 | step 2 | loss: 0.0008875484782738455\n",
      "epoch 684 | step 3 | loss: 0.001168234428629999\n",
      "epoch 684 | step 4 | loss: 0.0014753058717719401\n",
      "epoch 684 | step 5 | loss: 0.0017513518238934922\n",
      "epoch 684 | step 6 | loss: 0.0020290212952519724\n",
      "epoch 684 | step 7 | loss: 0.002333903249863419\n",
      "epoch 684 | step 8 | loss: 0.0026284917457882794\n",
      "epoch 684 | step 9 | loss: 0.0029144648331746485\n",
      "epoch 684 | step 10 | loss: 0.003219121154262083\n",
      "epoch 684 | step 11 | loss: 0.003486917874140185\n",
      "epoch 685 | step 0 | loss: 0.00029436060571461943\n",
      "epoch 685 | step 1 | loss: 0.000592715592911631\n",
      "epoch 685 | step 2 | loss: 0.0008585181731130844\n",
      "epoch 685 | step 3 | loss: 0.0011676884756981394\n",
      "epoch 685 | step 4 | loss: 0.0014795773619896725\n",
      "epoch 685 | step 5 | loss: 0.0017804148150472286\n",
      "epoch 685 | step 6 | loss: 0.0020787426669803153\n",
      "epoch 685 | step 7 | loss: 0.0023658443837843565\n",
      "epoch 685 | step 8 | loss: 0.0026581087115668074\n",
      "epoch 685 | step 9 | loss: 0.0029411254039889687\n",
      "epoch 685 | step 10 | loss: 0.0032245778648715\n",
      "epoch 685 | step 11 | loss: 0.0034846008890650135\n",
      "epoch 686 | step 0 | loss: 0.0002947520159613581\n",
      "epoch 686 | step 1 | loss: 0.0005945899325831547\n",
      "epoch 686 | step 2 | loss: 0.0008475276708669478\n",
      "epoch 686 | step 3 | loss: 0.0011560921395879996\n",
      "epoch 686 | step 4 | loss: 0.0014284532488115\n",
      "epoch 686 | step 5 | loss: 0.0017337362991883202\n",
      "epoch 686 | step 6 | loss: 0.0020398697984771183\n",
      "epoch 686 | step 7 | loss: 0.002329845614270096\n",
      "epoch 686 | step 8 | loss: 0.0026309266425191997\n",
      "epoch 686 | step 9 | loss: 0.0029212250080508347\n",
      "epoch 686 | step 10 | loss: 0.00319289402274552\n",
      "epoch 686 | step 11 | loss: 0.0034972663683466207\n",
      "epoch 687 | step 0 | loss: 0.0002833292120680786\n",
      "epoch 687 | step 1 | loss: 0.0005709037997537593\n",
      "epoch 687 | step 2 | loss: 0.00088021091786354\n",
      "epoch 687 | step 3 | loss: 0.0011727587252470893\n",
      "epoch 687 | step 4 | loss: 0.0014759041070847162\n",
      "epoch 687 | step 5 | loss: 0.0017476641295069075\n",
      "epoch 687 | step 6 | loss: 0.0020159423047439993\n",
      "epoch 687 | step 7 | loss: 0.002304867668258349\n",
      "epoch 687 | step 8 | loss: 0.002623749338019023\n",
      "epoch 687 | step 9 | loss: 0.0029379484548347897\n",
      "epoch 687 | step 10 | loss: 0.0032318628162119533\n",
      "epoch 687 | step 11 | loss: 0.0034817969398541508\n",
      "epoch 688 | step 0 | loss: 0.0002937132994095324\n",
      "epoch 688 | step 1 | loss: 0.0006323404751146103\n",
      "epoch 688 | step 2 | loss: 0.0009313187564784989\n",
      "epoch 688 | step 3 | loss: 0.001200732227330066\n",
      "epoch 688 | step 4 | loss: 0.0014777296635581072\n",
      "epoch 688 | step 5 | loss: 0.0017595978818026044\n",
      "epoch 688 | step 6 | loss: 0.002039242352638411\n",
      "epoch 688 | step 7 | loss: 0.0023404133075425266\n",
      "epoch 688 | step 8 | loss: 0.0026148415177289904\n",
      "epoch 688 | step 9 | loss: 0.0029106132134859657\n",
      "epoch 688 | step 10 | loss: 0.003207483269816031\n",
      "epoch 688 | step 11 | loss: 0.0034910072134955164\n",
      "epoch 689 | step 0 | loss: 0.00029344191972242157\n",
      "epoch 689 | step 1 | loss: 0.0005939666632235958\n",
      "epoch 689 | step 2 | loss: 0.0008833989716948577\n",
      "epoch 689 | step 3 | loss: 0.0011716967458177089\n",
      "epoch 689 | step 4 | loss: 0.0014676475405869752\n",
      "epoch 689 | step 5 | loss: 0.0017700684107495091\n",
      "epoch 689 | step 6 | loss: 0.0020683509023392116\n",
      "epoch 689 | step 7 | loss: 0.002377847885419086\n",
      "epoch 689 | step 8 | loss: 0.002661735423644782\n",
      "epoch 689 | step 9 | loss: 0.002955334761987232\n",
      "epoch 689 | step 10 | loss: 0.0032180501326976507\n",
      "epoch 689 | step 11 | loss: 0.0034871558514134615\n",
      "epoch 690 | step 0 | loss: 0.000302473618504137\n",
      "epoch 690 | step 1 | loss: 0.0005972696364925369\n",
      "epoch 690 | step 2 | loss: 0.0008886924955028092\n",
      "epoch 690 | step 3 | loss: 0.0011460336210332243\n",
      "epoch 690 | step 4 | loss: 0.001446037073969812\n",
      "epoch 690 | step 5 | loss: 0.0017316752971910676\n",
      "epoch 690 | step 6 | loss: 0.002044206246113129\n",
      "epoch 690 | step 7 | loss: 0.0023481045572928045\n",
      "epoch 690 | step 8 | loss: 0.002629641107867154\n",
      "epoch 690 | step 9 | loss: 0.0029265554063510877\n",
      "epoch 690 | step 10 | loss: 0.003209251183967038\n",
      "epoch 690 | step 11 | loss: 0.00348996439462725\n",
      "epoch 691 | step 0 | loss: 0.00030173264396731874\n",
      "epoch 691 | step 1 | loss: 0.0005897464091024747\n",
      "epoch 691 | step 2 | loss: 0.0008904246940952059\n",
      "epoch 691 | step 3 | loss: 0.0011628145376074506\n",
      "epoch 691 | step 4 | loss: 0.0014463035134104267\n",
      "epoch 691 | step 5 | loss: 0.001725099075553718\n",
      "epoch 691 | step 6 | loss: 0.0020437343925846702\n",
      "epoch 691 | step 7 | loss: 0.0023205926143946717\n",
      "epoch 691 | step 8 | loss: 0.0026396026637110805\n",
      "epoch 691 | step 9 | loss: 0.0029209382486307537\n",
      "epoch 691 | step 10 | loss: 0.003219437739394507\n",
      "epoch 691 | step 11 | loss: 0.003486175153532614\n",
      "epoch 692 | step 0 | loss: 0.0002995867338486958\n",
      "epoch 692 | step 1 | loss: 0.0006001286367832685\n",
      "epoch 692 | step 2 | loss: 0.0008777417500574657\n",
      "epoch 692 | step 3 | loss: 0.0011561823629737942\n",
      "epoch 692 | step 4 | loss: 0.001456309391160099\n",
      "epoch 692 | step 5 | loss: 0.0017324324125980322\n",
      "epoch 692 | step 6 | loss: 0.0020090897730719584\n",
      "epoch 692 | step 7 | loss: 0.002291880042845686\n",
      "epoch 692 | step 8 | loss: 0.0025861292776378003\n",
      "epoch 692 | step 9 | loss: 0.0029018614411065197\n",
      "epoch 692 | step 10 | loss: 0.003187205550472157\n",
      "epoch 692 | step 11 | loss: 0.0034984704925783124\n",
      "epoch 693 | step 0 | loss: 0.00028216154531775586\n",
      "epoch 693 | step 1 | loss: 0.0005813859532942626\n",
      "epoch 693 | step 2 | loss: 0.0008893135691552826\n",
      "epoch 693 | step 3 | loss: 0.0011830451676829354\n",
      "epoch 693 | step 4 | loss: 0.0014886598589251687\n",
      "epoch 693 | step 5 | loss: 0.0017994506903038104\n",
      "epoch 693 | step 6 | loss: 0.002079543406198179\n",
      "epoch 693 | step 7 | loss: 0.0023539053958500497\n",
      "epoch 693 | step 8 | loss: 0.0026399335693072677\n",
      "epoch 693 | step 9 | loss: 0.002927098258299217\n",
      "epoch 693 | step 10 | loss: 0.003224596991403576\n",
      "epoch 693 | step 11 | loss: 0.0034837652291353923\n",
      "epoch 694 | step 0 | loss: 0.00029514517322521485\n",
      "epoch 694 | step 1 | loss: 0.0005600452024156688\n",
      "epoch 694 | step 2 | loss: 0.0008478714190630704\n",
      "epoch 694 | step 3 | loss: 0.0011521617792783578\n",
      "epoch 694 | step 4 | loss: 0.0014216489153372031\n",
      "epoch 694 | step 5 | loss: 0.0017439186335367514\n",
      "epoch 694 | step 6 | loss: 0.0020369841554576116\n",
      "epoch 694 | step 7 | loss: 0.0023231356958994865\n",
      "epoch 694 | step 8 | loss: 0.0026160845635000607\n",
      "epoch 694 | step 9 | loss: 0.0029228815521794467\n",
      "epoch 694 | step 10 | loss: 0.0032130142764689785\n",
      "epoch 694 | step 11 | loss: 0.0034882793030054913\n",
      "epoch 695 | step 0 | loss: 0.0002898663385068814\n",
      "epoch 695 | step 1 | loss: 0.0005702759138443143\n",
      "epoch 695 | step 2 | loss: 0.0008542936325007088\n",
      "epoch 695 | step 3 | loss: 0.0011378781321027028\n",
      "epoch 695 | step 4 | loss: 0.0014454976427065818\n",
      "epoch 695 | step 5 | loss: 0.0017503552550440227\n",
      "epoch 695 | step 6 | loss: 0.00204054210674872\n",
      "epoch 695 | step 7 | loss: 0.0023449033415666324\n",
      "epoch 695 | step 8 | loss: 0.002636057347844872\n",
      "epoch 695 | step 9 | loss: 0.002929949786489892\n",
      "epoch 695 | step 10 | loss: 0.0032034464597904453\n",
      "epoch 695 | step 11 | loss: 0.0034919065861137486\n",
      "epoch 696 | step 0 | loss: 0.0002563751694404954\n",
      "epoch 696 | step 1 | loss: 0.000543896346588592\n",
      "epoch 696 | step 2 | loss: 0.000824981846621798\n",
      "epoch 696 | step 3 | loss: 0.0011402776680880542\n",
      "epoch 696 | step 4 | loss: 0.0014323202750153822\n",
      "epoch 696 | step 5 | loss: 0.001706102876994255\n",
      "epoch 696 | step 6 | loss: 0.002007359204432489\n",
      "epoch 696 | step 7 | loss: 0.0022960011406518224\n",
      "epoch 696 | step 8 | loss: 0.0026149725267869345\n",
      "epoch 696 | step 9 | loss: 0.002908100987696981\n",
      "epoch 696 | step 10 | loss: 0.003199239789423386\n",
      "epoch 696 | step 11 | loss: 0.003493440302080404\n",
      "epoch 697 | step 0 | loss: 0.00027670318519879575\n",
      "epoch 697 | step 1 | loss: 0.0005746621453859794\n",
      "epoch 697 | step 2 | loss: 0.0008957875165447801\n",
      "epoch 697 | step 3 | loss: 0.0011951837169110822\n",
      "epoch 697 | step 4 | loss: 0.0014641545682232064\n",
      "epoch 697 | step 5 | loss: 0.001759742094752095\n",
      "epoch 697 | step 6 | loss: 0.0020360585165204977\n",
      "epoch 697 | step 7 | loss: 0.0023076504417953716\n",
      "epoch 697 | step 8 | loss: 0.002581798301369991\n",
      "epoch 697 | step 9 | loss: 0.0028694560419109125\n",
      "epoch 697 | step 10 | loss: 0.0031948562173625223\n",
      "epoch 697 | step 11 | loss: 0.003494987672196348\n",
      "epoch 698 | step 0 | loss: 0.00029485153782054954\n",
      "epoch 698 | step 1 | loss: 0.0005732714649777268\n",
      "epoch 698 | step 2 | loss: 0.000850531757742918\n",
      "epoch 698 | step 3 | loss: 0.0011155915538805915\n",
      "epoch 698 | step 4 | loss: 0.0014337312510635152\n",
      "epoch 698 | step 5 | loss: 0.0017410227327284505\n",
      "epoch 698 | step 6 | loss: 0.0020100171948463754\n",
      "epoch 698 | step 7 | loss: 0.002312016804982613\n",
      "epoch 698 | step 8 | loss: 0.002600800522129826\n",
      "epoch 698 | step 9 | loss: 0.002888866866595732\n",
      "epoch 698 | step 10 | loss: 0.003202697478376381\n",
      "epoch 698 | step 11 | loss: 0.003491810207353141\n",
      "epoch 699 | step 0 | loss: 0.0003390448698731108\n",
      "epoch 699 | step 1 | loss: 0.0006475098829099369\n",
      "epoch 699 | step 2 | loss: 0.0009648833255045807\n",
      "epoch 699 | step 3 | loss: 0.0012490263887793434\n",
      "epoch 699 | step 4 | loss: 0.0015188297468820393\n",
      "epoch 699 | step 5 | loss: 0.0017870493061175655\n",
      "epoch 699 | step 6 | loss: 0.0020725474636480376\n",
      "epoch 699 | step 7 | loss: 0.0023435924150260875\n",
      "epoch 699 | step 8 | loss: 0.002631802566889085\n",
      "epoch 699 | step 9 | loss: 0.002919273150674302\n",
      "epoch 699 | step 10 | loss: 0.0032084064114769376\n",
      "epoch 699 | step 11 | loss: 0.0034897442994020257\n",
      "epoch 700 | step 0 | loss: 0.0003142392646834747\n",
      "epoch 700 | step 1 | loss: 0.0005844990419848284\n",
      "epoch 700 | step 2 | loss: 0.0008864148616164486\n",
      "epoch 700 | step 3 | loss: 0.0011773914508850254\n",
      "epoch 700 | step 4 | loss: 0.001469844273446284\n",
      "epoch 700 | step 5 | loss: 0.0017796562428185757\n",
      "epoch 700 | step 6 | loss: 0.002066873186574748\n",
      "epoch 700 | step 7 | loss: 0.0023602488366115176\n",
      "epoch 700 | step 8 | loss: 0.002636164531581077\n",
      "epoch 700 | step 9 | loss: 0.002919970467445967\n",
      "epoch 700 | step 10 | loss: 0.00321962632662124\n",
      "epoch 700 | step 11 | loss: 0.003485016213792018\n",
      "epoch 701 | step 0 | loss: 0.0002947846782086514\n",
      "epoch 701 | step 1 | loss: 0.0006078763499535427\n",
      "epoch 701 | step 2 | loss: 0.000925201008364363\n",
      "epoch 701 | step 3 | loss: 0.0012121803023916078\n",
      "epoch 701 | step 4 | loss: 0.0014813108351245015\n",
      "epoch 701 | step 5 | loss: 0.0017660446600280626\n",
      "epoch 701 | step 6 | loss: 0.0020555163237675244\n",
      "epoch 701 | step 7 | loss: 0.002350530640769012\n",
      "epoch 701 | step 8 | loss: 0.002629274905688206\n",
      "epoch 701 | step 9 | loss: 0.002926055283835121\n",
      "epoch 701 | step 10 | loss: 0.0032215267824998424\n",
      "epoch 701 | step 11 | loss: 0.003484245564698411\n",
      "epoch 702 | step 0 | loss: 0.0003186204419100943\n",
      "epoch 702 | step 1 | loss: 0.0006080361027555611\n",
      "epoch 702 | step 2 | loss: 0.0009002017189052271\n",
      "epoch 702 | step 3 | loss: 0.0011867220350161088\n",
      "epoch 702 | step 4 | loss: 0.0014613927242354203\n",
      "epoch 702 | step 5 | loss: 0.0017583582950512274\n",
      "epoch 702 | step 6 | loss: 0.002040130553515898\n",
      "epoch 702 | step 7 | loss: 0.0023382629228515203\n",
      "epoch 702 | step 8 | loss: 0.002620118189310861\n",
      "epoch 702 | step 9 | loss: 0.0029139965704097293\n",
      "epoch 702 | step 10 | loss: 0.003207717353174033\n",
      "epoch 702 | step 11 | loss: 0.003489711439060363\n",
      "epoch 703 | step 0 | loss: 0.00029105756924441056\n",
      "epoch 703 | step 1 | loss: 0.0005775191705775186\n",
      "epoch 703 | step 2 | loss: 0.0008711120184080653\n",
      "epoch 703 | step 3 | loss: 0.0011879975068060256\n",
      "epoch 703 | step 4 | loss: 0.0014862042891064958\n",
      "epoch 703 | step 5 | loss: 0.0017725747843455983\n",
      "epoch 703 | step 6 | loss: 0.002064338266167819\n",
      "epoch 703 | step 7 | loss: 0.0023427601827794653\n",
      "epoch 703 | step 8 | loss: 0.0026341850838448467\n",
      "epoch 703 | step 9 | loss: 0.00294306063557882\n",
      "epoch 703 | step 10 | loss: 0.0032112903170048125\n",
      "epoch 703 | step 11 | loss: 0.0034880058098497578\n",
      "epoch 704 | step 0 | loss: 0.0002872468402732637\n",
      "epoch 704 | step 1 | loss: 0.0005593026875639752\n",
      "epoch 704 | step 2 | loss: 0.0008499527470155168\n",
      "epoch 704 | step 3 | loss: 0.001161020746613586\n",
      "epoch 704 | step 4 | loss: 0.001454755226321072\n",
      "epoch 704 | step 5 | loss: 0.001738332107922811\n",
      "epoch 704 | step 6 | loss: 0.0020334670577227794\n",
      "epoch 704 | step 7 | loss: 0.002277118609221579\n",
      "epoch 704 | step 8 | loss: 0.0026072423635523836\n",
      "epoch 704 | step 9 | loss: 0.002908913432232035\n",
      "epoch 704 | step 10 | loss: 0.0032130462140442387\n",
      "epoch 704 | step 11 | loss: 0.0034875061622018444\n",
      "epoch 705 | step 0 | loss: 0.0002745209433694212\n",
      "epoch 705 | step 1 | loss: 0.0005845349481299191\n",
      "epoch 705 | step 2 | loss: 0.0008771529929968346\n",
      "epoch 705 | step 3 | loss: 0.0011402990145116172\n",
      "epoch 705 | step 4 | loss: 0.0014279952101439598\n",
      "epoch 705 | step 5 | loss: 0.0017337944059117789\n",
      "epoch 705 | step 6 | loss: 0.0020159208953148627\n",
      "epoch 705 | step 7 | loss: 0.0023079458887133282\n",
      "epoch 705 | step 8 | loss: 0.0026035441358852347\n",
      "epoch 705 | step 9 | loss: 0.002925319739744079\n",
      "epoch 705 | step 10 | loss: 0.003203087866020192\n",
      "epoch 705 | step 11 | loss: 0.003490800465752352\n",
      "epoch 706 | step 0 | loss: 0.00029826483889877094\n",
      "epoch 706 | step 1 | loss: 0.0005935943710315875\n",
      "epoch 706 | step 2 | loss: 0.0008910261045200053\n",
      "epoch 706 | step 3 | loss: 0.0011738422268785897\n",
      "epoch 706 | step 4 | loss: 0.0014441247043306678\n",
      "epoch 706 | step 5 | loss: 0.0017511780634148794\n",
      "epoch 706 | step 6 | loss: 0.0020370253305741354\n",
      "epoch 706 | step 7 | loss: 0.0023344228174885023\n",
      "epoch 706 | step 8 | loss: 0.0026251159033150125\n",
      "epoch 706 | step 9 | loss: 0.0029090938590002778\n",
      "epoch 706 | step 10 | loss: 0.00319838336971215\n",
      "epoch 706 | step 11 | loss: 0.0034927046014841734\n",
      "epoch 707 | step 0 | loss: 0.00029337541204023577\n",
      "epoch 707 | step 1 | loss: 0.0005465855775008729\n",
      "epoch 707 | step 2 | loss: 0.000857751338144681\n",
      "epoch 707 | step 3 | loss: 0.0011552762838617004\n",
      "epoch 707 | step 4 | loss: 0.001461062111264454\n",
      "epoch 707 | step 5 | loss: 0.0017498625387423563\n",
      "epoch 707 | step 6 | loss: 0.002039851918026627\n",
      "epoch 707 | step 7 | loss: 0.002334880731715276\n",
      "epoch 707 | step 8 | loss: 0.0026426730073939053\n",
      "epoch 707 | step 9 | loss: 0.0029381126627983935\n",
      "epoch 707 | step 10 | loss: 0.00321288300945851\n",
      "epoch 707 | step 11 | loss: 0.003486875826790769\n",
      "epoch 708 | step 0 | loss: 0.0003102995214056604\n",
      "epoch 708 | step 1 | loss: 0.0006102317784594535\n",
      "epoch 708 | step 2 | loss: 0.0009255589280434591\n",
      "epoch 708 | step 3 | loss: 0.001201820476924018\n",
      "epoch 708 | step 4 | loss: 0.0015029531986124924\n",
      "epoch 708 | step 5 | loss: 0.0017779798843106803\n",
      "epoch 708 | step 6 | loss: 0.002057991921370585\n",
      "epoch 708 | step 7 | loss: 0.002348828949334524\n",
      "epoch 708 | step 8 | loss: 0.002617333555868858\n",
      "epoch 708 | step 9 | loss: 0.002926617992313994\n",
      "epoch 708 | step 10 | loss: 0.003221350738165539\n",
      "epoch 708 | step 11 | loss: 0.0034835605486402644\n",
      "epoch 709 | step 0 | loss: 0.0002780979955726881\n",
      "epoch 709 | step 1 | loss: 0.0005609443838021594\n",
      "epoch 709 | step 2 | loss: 0.0008501670686434763\n",
      "epoch 709 | step 3 | loss: 0.0011694952519729553\n",
      "epoch 709 | step 4 | loss: 0.001468021650424867\n",
      "epoch 709 | step 5 | loss: 0.0017713624673563685\n",
      "epoch 709 | step 6 | loss: 0.0020196802095808325\n",
      "epoch 709 | step 7 | loss: 0.0023271439516913987\n",
      "epoch 709 | step 8 | loss: 0.002618118949521441\n",
      "epoch 709 | step 9 | loss: 0.002902946991542309\n",
      "epoch 709 | step 10 | loss: 0.003216200194060852\n",
      "epoch 709 | step 11 | loss: 0.00348525146853891\n",
      "epoch 710 | step 0 | loss: 0.00027724221653085084\n",
      "epoch 710 | step 1 | loss: 0.0005633200913844202\n",
      "epoch 710 | step 2 | loss: 0.000875265555456517\n",
      "epoch 710 | step 3 | loss: 0.0011683129726087424\n",
      "epoch 710 | step 4 | loss: 0.00146917399435977\n",
      "epoch 710 | step 5 | loss: 0.0017525545714863148\n",
      "epoch 710 | step 6 | loss: 0.002038888293719271\n",
      "epoch 710 | step 7 | loss: 0.0023407267091914794\n",
      "epoch 710 | step 8 | loss: 0.0026285027228949943\n",
      "epoch 710 | step 9 | loss: 0.002918441273024842\n",
      "epoch 710 | step 10 | loss: 0.003212263195744814\n",
      "epoch 710 | step 11 | loss: 0.003487120661004472\n",
      "epoch 711 | step 0 | loss: 0.0002937918182695516\n",
      "epoch 711 | step 1 | loss: 0.0005997874144935856\n",
      "epoch 711 | step 2 | loss: 0.0008790509985387336\n",
      "epoch 711 | step 3 | loss: 0.001177867895926294\n",
      "epoch 711 | step 4 | loss: 0.0014739484394257628\n",
      "epoch 711 | step 5 | loss: 0.0017460287464376868\n",
      "epoch 711 | step 6 | loss: 0.0020428243016638358\n",
      "epoch 711 | step 7 | loss: 0.0023241285050280788\n",
      "epoch 711 | step 8 | loss: 0.0025901054949396336\n",
      "epoch 711 | step 9 | loss: 0.0028783674712555473\n",
      "epoch 711 | step 10 | loss: 0.003178964413616156\n",
      "epoch 711 | step 11 | loss: 0.003499752190604528\n",
      "epoch 712 | step 0 | loss: 0.0002955311856102554\n",
      "epoch 712 | step 1 | loss: 0.0005868972068961429\n",
      "epoch 712 | step 2 | loss: 0.0008908911758962728\n",
      "epoch 712 | step 3 | loss: 0.001180671128928994\n",
      "epoch 712 | step 4 | loss: 0.0014488539336645124\n",
      "epoch 712 | step 5 | loss: 0.0017572749654218994\n",
      "epoch 712 | step 6 | loss: 0.0020352753050454538\n",
      "epoch 712 | step 7 | loss: 0.002306873107947601\n",
      "epoch 712 | step 8 | loss: 0.0026000774938090302\n",
      "epoch 712 | step 9 | loss: 0.002899829220173443\n",
      "epoch 712 | step 10 | loss: 0.003205022979736717\n",
      "epoch 712 | step 11 | loss: 0.0034892760487750364\n",
      "epoch 713 | step 0 | loss: 0.00026482891074271827\n",
      "epoch 713 | step 1 | loss: 0.0005451940412496784\n",
      "epoch 713 | step 2 | loss: 0.0008707207939864043\n",
      "epoch 713 | step 3 | loss: 0.0011587564886835627\n",
      "epoch 713 | step 4 | loss: 0.0014584605748093802\n",
      "epoch 713 | step 5 | loss: 0.0017532820497169674\n",
      "epoch 713 | step 6 | loss: 0.0020600031923348494\n",
      "epoch 713 | step 7 | loss: 0.0023608315477702986\n",
      "epoch 713 | step 8 | loss: 0.0026198058895051885\n",
      "epoch 713 | step 9 | loss: 0.0029241409642302473\n",
      "epoch 713 | step 10 | loss: 0.003198105998308763\n",
      "epoch 713 | step 11 | loss: 0.003492118175557594\n",
      "epoch 714 | step 0 | loss: 0.0003078491046090144\n",
      "epoch 714 | step 1 | loss: 0.000611574415784366\n",
      "epoch 714 | step 2 | loss: 0.0008968998169547592\n",
      "epoch 714 | step 3 | loss: 0.0011815144393493447\n",
      "epoch 714 | step 4 | loss: 0.001449411660234936\n",
      "epoch 714 | step 5 | loss: 0.0017369462227970803\n",
      "epoch 714 | step 6 | loss: 0.0020512460134641764\n",
      "epoch 714 | step 7 | loss: 0.002333527075378712\n",
      "epoch 714 | step 8 | loss: 0.0026191218745404117\n",
      "epoch 714 | step 9 | loss: 0.0028772111630593882\n",
      "epoch 714 | step 10 | loss: 0.003176834587844285\n",
      "epoch 714 | step 11 | loss: 0.0035001937774348847\n",
      "epoch 715 | step 0 | loss: 0.0003004919384813223\n",
      "epoch 715 | step 1 | loss: 0.0005972749482599517\n",
      "epoch 715 | step 2 | loss: 0.0008939471037961135\n",
      "epoch 715 | step 3 | loss: 0.0011784338719867855\n",
      "epoch 715 | step 4 | loss: 0.0014768003117059535\n",
      "epoch 715 | step 5 | loss: 0.0017830394966005768\n",
      "epoch 715 | step 6 | loss: 0.0020755345063533897\n",
      "epoch 715 | step 7 | loss: 0.0023555142802746893\n",
      "epoch 715 | step 8 | loss: 0.002620114987760769\n",
      "epoch 715 | step 9 | loss: 0.002901195590565611\n",
      "epoch 715 | step 10 | loss: 0.003200230540653828\n",
      "epoch 715 | step 11 | loss: 0.00349087435112223\n",
      "epoch 716 | step 0 | loss: 0.0003025617396475768\n",
      "epoch 716 | step 1 | loss: 0.0005679435251895147\n",
      "epoch 716 | step 2 | loss: 0.0008507679848930881\n",
      "epoch 716 | step 3 | loss: 0.001143425991124802\n",
      "epoch 716 | step 4 | loss: 0.0014431333937224768\n",
      "epoch 716 | step 5 | loss: 0.0017475000028896349\n",
      "epoch 716 | step 6 | loss: 0.002039389503839465\n",
      "epoch 716 | step 7 | loss: 0.0023371035164440855\n",
      "epoch 716 | step 8 | loss: 0.0026208072758132943\n",
      "epoch 716 | step 9 | loss: 0.0029033683510350135\n",
      "epoch 716 | step 10 | loss: 0.0031978411178335024\n",
      "epoch 716 | step 11 | loss: 0.0034915963249056658\n",
      "epoch 717 | step 0 | loss: 0.00030702394174395204\n",
      "epoch 717 | step 1 | loss: 0.0005871579770283821\n",
      "epoch 717 | step 2 | loss: 0.0008780200992908067\n",
      "epoch 717 | step 3 | loss: 0.0011767707984306464\n",
      "epoch 717 | step 4 | loss: 0.0014903646128178844\n",
      "epoch 717 | step 5 | loss: 0.001782972816843434\n",
      "epoch 717 | step 6 | loss: 0.002073609329913075\n",
      "epoch 717 | step 7 | loss: 0.0023592433719566617\n",
      "epoch 717 | step 8 | loss: 0.002631318469762126\n",
      "epoch 717 | step 9 | loss: 0.0029169650816957956\n",
      "epoch 717 | step 10 | loss: 0.003205705665696847\n",
      "epoch 717 | step 11 | loss: 0.003488682719003091\n",
      "epoch 718 | step 0 | loss: 0.0002946537524216561\n",
      "epoch 718 | step 1 | loss: 0.000605167429768916\n",
      "epoch 718 | step 2 | loss: 0.0009159086660962467\n",
      "epoch 718 | step 3 | loss: 0.0011859281001260905\n",
      "epoch 718 | step 4 | loss: 0.0014773806237391451\n",
      "epoch 718 | step 5 | loss: 0.0017639638380566689\n",
      "epoch 718 | step 6 | loss: 0.002057258083315148\n",
      "epoch 718 | step 7 | loss: 0.0023385335305363283\n",
      "epoch 718 | step 8 | loss: 0.0026123175769503905\n",
      "epoch 718 | step 9 | loss: 0.0029112741301714398\n",
      "epoch 718 | step 10 | loss: 0.003206633811319321\n",
      "epoch 718 | step 11 | loss: 0.003488137173796492\n",
      "epoch 719 | step 0 | loss: 0.00030135106829849066\n",
      "epoch 719 | step 1 | loss: 0.0006015196129294355\n",
      "epoch 719 | step 2 | loss: 0.0008880876688009112\n",
      "epoch 719 | step 3 | loss: 0.0011921672947399532\n",
      "epoch 719 | step 4 | loss: 0.0014782000289758675\n",
      "epoch 719 | step 5 | loss: 0.001756249177610429\n",
      "epoch 719 | step 6 | loss: 0.0020502379283025667\n",
      "epoch 719 | step 7 | loss: 0.0023431688183221068\n",
      "epoch 719 | step 8 | loss: 0.0026352946339670644\n",
      "epoch 719 | step 9 | loss: 0.0029286262983906344\n",
      "epoch 719 | step 10 | loss: 0.0032156122677922663\n",
      "epoch 719 | step 11 | loss: 0.003484910011299744\n",
      "epoch 720 | step 0 | loss: 0.00028267918199740064\n",
      "epoch 720 | step 1 | loss: 0.0005650643222346009\n",
      "epoch 720 | step 2 | loss: 0.0008565441597769361\n",
      "epoch 720 | step 3 | loss: 0.0011392469683972042\n",
      "epoch 720 | step 4 | loss: 0.0014393199077557509\n",
      "epoch 720 | step 5 | loss: 0.001733861859164797\n",
      "epoch 720 | step 6 | loss: 0.002013277126612602\n",
      "epoch 720 | step 7 | loss: 0.002314260798393461\n",
      "epoch 720 | step 8 | loss: 0.002633627717108215\n",
      "epoch 720 | step 9 | loss: 0.0029006663932268015\n",
      "epoch 720 | step 10 | loss: 0.0031825651191525915\n",
      "epoch 720 | step 11 | loss: 0.0034973563623550037\n",
      "epoch 721 | step 0 | loss: 0.0002646870518719905\n",
      "epoch 721 | step 1 | loss: 0.0005630738539571317\n",
      "epoch 721 | step 2 | loss: 0.0008627018056855758\n",
      "epoch 721 | step 3 | loss: 0.0011394792268434404\n",
      "epoch 721 | step 4 | loss: 0.0014176330033128764\n",
      "epoch 721 | step 5 | loss: 0.0017287083369127044\n",
      "epoch 721 | step 6 | loss: 0.0020422262901057813\n",
      "epoch 721 | step 7 | loss: 0.00233867919922679\n",
      "epoch 721 | step 8 | loss: 0.002623936197901061\n",
      "epoch 721 | step 9 | loss: 0.002929610061910389\n",
      "epoch 721 | step 10 | loss: 0.0032097321326934576\n",
      "epoch 721 | step 11 | loss: 0.0034870108839171058\n",
      "epoch 722 | step 0 | loss: 0.0003076121364511407\n",
      "epoch 722 | step 1 | loss: 0.0005979935526934865\n",
      "epoch 722 | step 2 | loss: 0.0008695336976147239\n",
      "epoch 722 | step 3 | loss: 0.0011670070981582303\n",
      "epoch 722 | step 4 | loss: 0.0014345193353954683\n",
      "epoch 722 | step 5 | loss: 0.001758689822076403\n",
      "epoch 722 | step 6 | loss: 0.0020289370578142336\n",
      "epoch 722 | step 7 | loss: 0.0023302721318510656\n",
      "epoch 722 | step 8 | loss: 0.002618384415426946\n",
      "epoch 722 | step 9 | loss: 0.002928154916689268\n",
      "epoch 722 | step 10 | loss: 0.00320993684791422\n",
      "epoch 722 | step 11 | loss: 0.003486381284120197\n",
      "epoch 723 | step 0 | loss: 0.0002840005744905377\n",
      "epoch 723 | step 1 | loss: 0.0005722838657939742\n",
      "epoch 723 | step 2 | loss: 0.0008843387584922697\n",
      "epoch 723 | step 3 | loss: 0.0011595936591932096\n",
      "epoch 723 | step 4 | loss: 0.001478193495435062\n",
      "epoch 723 | step 5 | loss: 0.0017660406379311536\n",
      "epoch 723 | step 6 | loss: 0.002058428037665116\n",
      "epoch 723 | step 7 | loss: 0.0023254888227570657\n",
      "epoch 723 | step 8 | loss: 0.0026133485704721584\n",
      "epoch 723 | step 9 | loss: 0.002890837424541286\n",
      "epoch 723 | step 10 | loss: 0.0031935470707840233\n",
      "epoch 723 | step 11 | loss: 0.0034928220471844807\n",
      "epoch 724 | step 0 | loss: 0.000280738232287267\n",
      "epoch 724 | step 1 | loss: 0.0006017630893610735\n",
      "epoch 724 | step 2 | loss: 0.0008947663383283109\n",
      "epoch 724 | step 3 | loss: 0.0011719415903772995\n",
      "epoch 724 | step 4 | loss: 0.0014574973145272145\n",
      "epoch 724 | step 5 | loss: 0.00175061255075343\n",
      "epoch 724 | step 6 | loss: 0.0020644772179745125\n",
      "epoch 724 | step 7 | loss: 0.00235106231789278\n",
      "epoch 724 | step 8 | loss: 0.0026088827162228656\n",
      "epoch 724 | step 9 | loss: 0.0029266367061483623\n",
      "epoch 724 | step 10 | loss: 0.003204535857622754\n",
      "epoch 724 | step 11 | loss: 0.003488925019775646\n",
      "epoch 725 | step 0 | loss: 0.0002965764403007892\n",
      "epoch 725 | step 1 | loss: 0.0005673795961215655\n",
      "epoch 725 | step 2 | loss: 0.0008577823054834483\n",
      "epoch 725 | step 3 | loss: 0.0011437765434391845\n",
      "epoch 725 | step 4 | loss: 0.0014533834163683642\n",
      "epoch 725 | step 5 | loss: 0.001746395780477025\n",
      "epoch 725 | step 6 | loss: 0.002021169953861444\n",
      "epoch 725 | step 7 | loss: 0.002322061524380175\n",
      "epoch 725 | step 8 | loss: 0.0026293011184000173\n",
      "epoch 725 | step 9 | loss: 0.002938212909307293\n",
      "epoch 725 | step 10 | loss: 0.0032252190140466368\n",
      "epoch 725 | step 11 | loss: 0.003480219772071428\n",
      "epoch 726 | step 0 | loss: 0.0002633417532369693\n",
      "epoch 726 | step 1 | loss: 0.0005498740375091632\n",
      "epoch 726 | step 2 | loss: 0.0008169298396660781\n",
      "epoch 726 | step 3 | loss: 0.0011320728144150537\n",
      "epoch 726 | step 4 | loss: 0.00139887357733816\n",
      "epoch 726 | step 5 | loss: 0.0017534162617437155\n",
      "epoch 726 | step 6 | loss: 0.0020595245839477817\n",
      "epoch 726 | step 7 | loss: 0.00234962992348477\n",
      "epoch 726 | step 8 | loss: 0.002620287252604355\n",
      "epoch 726 | step 9 | loss: 0.0029063845320436367\n",
      "epoch 726 | step 10 | loss: 0.003178371390309725\n",
      "epoch 726 | step 11 | loss: 0.0034982530937869988\n",
      "epoch 727 | step 0 | loss: 0.00027622010661080843\n",
      "epoch 727 | step 1 | loss: 0.0005458183796026165\n",
      "epoch 727 | step 2 | loss: 0.0008564767518016603\n",
      "epoch 727 | step 3 | loss: 0.001132584281424744\n",
      "epoch 727 | step 4 | loss: 0.001425081371907511\n",
      "epoch 727 | step 5 | loss: 0.0017115193525774105\n",
      "epoch 727 | step 6 | loss: 0.0020049782115535105\n",
      "epoch 727 | step 7 | loss: 0.0022902046428607816\n",
      "epoch 727 | step 8 | loss: 0.002590956129982766\n",
      "epoch 727 | step 9 | loss: 0.002892529451136963\n",
      "epoch 727 | step 10 | loss: 0.003198825443036147\n",
      "epoch 727 | step 11 | loss: 0.0034900648757694172\n",
      "epoch 728 | step 0 | loss: 0.00030738029639627244\n",
      "epoch 728 | step 1 | loss: 0.0005959570860963126\n",
      "epoch 728 | step 2 | loss: 0.0008907052643922112\n",
      "epoch 728 | step 3 | loss: 0.0011905177126977403\n",
      "epoch 728 | step 4 | loss: 0.0015301171058286422\n",
      "epoch 728 | step 5 | loss: 0.0018403461406762116\n",
      "epoch 728 | step 6 | loss: 0.0021190304039603114\n",
      "epoch 728 | step 7 | loss: 0.002403279132306813\n",
      "epoch 728 | step 8 | loss: 0.0026748212266954376\n",
      "epoch 728 | step 9 | loss: 0.002956414405755778\n",
      "epoch 728 | step 10 | loss: 0.0032042780573137987\n",
      "epoch 728 | step 11 | loss: 0.0034880776424055853\n",
      "epoch 729 | step 0 | loss: 0.0002942971288676536\n",
      "epoch 729 | step 1 | loss: 0.0005921255755059552\n",
      "epoch 729 | step 2 | loss: 0.0008794763438194394\n",
      "epoch 729 | step 3 | loss: 0.0011874924271128512\n",
      "epoch 729 | step 4 | loss: 0.0014970923238881214\n",
      "epoch 729 | step 5 | loss: 0.0017980130076329392\n",
      "epoch 729 | step 6 | loss: 0.0020462648018905157\n",
      "epoch 729 | step 7 | loss: 0.0023156830803330104\n",
      "epoch 729 | step 8 | loss: 0.0025953679862025453\n",
      "epoch 729 | step 9 | loss: 0.0028945639941558698\n",
      "epoch 729 | step 10 | loss: 0.0032088420145539053\n",
      "epoch 729 | step 11 | loss: 0.003486234807880044\n",
      "epoch 730 | step 0 | loss: 0.000273948007493857\n",
      "epoch 730 | step 1 | loss: 0.000555192725925756\n",
      "epoch 730 | step 2 | loss: 0.0008490021412705616\n",
      "epoch 730 | step 3 | loss: 0.001128826312982624\n",
      "epoch 730 | step 4 | loss: 0.0014315401483319713\n",
      "epoch 730 | step 5 | loss: 0.0017144887573672756\n",
      "epoch 730 | step 6 | loss: 0.00202902022759373\n",
      "epoch 730 | step 7 | loss: 0.0023343925243973053\n",
      "epoch 730 | step 8 | loss: 0.0026192419442652346\n",
      "epoch 730 | step 9 | loss: 0.002895325748474234\n",
      "epoch 730 | step 10 | loss: 0.003207562051198453\n",
      "epoch 730 | step 11 | loss: 0.0034867058857615775\n",
      "epoch 731 | step 0 | loss: 0.00029988532073961635\n",
      "epoch 731 | step 1 | loss: 0.0005695881548182112\n",
      "epoch 731 | step 2 | loss: 0.0008630779627522043\n",
      "epoch 731 | step 3 | loss: 0.0011374458132167135\n",
      "epoch 731 | step 4 | loss: 0.0014478649898825708\n",
      "epoch 731 | step 5 | loss: 0.001759871941633222\n",
      "epoch 731 | step 6 | loss: 0.002029611764299601\n",
      "epoch 731 | step 7 | loss: 0.002341632435149352\n",
      "epoch 731 | step 8 | loss: 0.002635847467392932\n",
      "epoch 731 | step 9 | loss: 0.00290843307285175\n",
      "epoch 731 | step 10 | loss: 0.0032067877579209586\n",
      "epoch 731 | step 11 | loss: 0.003486541651288891\n",
      "epoch 732 | step 0 | loss: 0.0002812335718617289\n",
      "epoch 732 | step 1 | loss: 0.000580928540974154\n",
      "epoch 732 | step 2 | loss: 0.0008775963206660988\n",
      "epoch 732 | step 3 | loss: 0.0011689471611837682\n",
      "epoch 732 | step 4 | loss: 0.0014351853427346751\n",
      "epoch 732 | step 5 | loss: 0.0017221113578657972\n",
      "epoch 732 | step 6 | loss: 0.0020116963264690153\n",
      "epoch 732 | step 7 | loss: 0.0023221602004030576\n",
      "epoch 732 | step 8 | loss: 0.0026053720875260492\n",
      "epoch 732 | step 9 | loss: 0.0028967524530316307\n",
      "epoch 732 | step 10 | loss: 0.0031698048817962463\n",
      "epoch 732 | step 11 | loss: 0.003500907887390394\n",
      "epoch 733 | step 0 | loss: 0.000296662361656267\n",
      "epoch 733 | step 1 | loss: 0.0005886498939016802\n",
      "epoch 733 | step 2 | loss: 0.0009157372098169026\n",
      "epoch 733 | step 3 | loss: 0.0011966922723519115\n",
      "epoch 733 | step 4 | loss: 0.0014852683798401304\n",
      "epoch 733 | step 5 | loss: 0.0017810101374418669\n",
      "epoch 733 | step 6 | loss: 0.002062238865966039\n",
      "epoch 733 | step 7 | loss: 0.0023385823481342186\n",
      "epoch 733 | step 8 | loss: 0.002645946114086658\n",
      "epoch 733 | step 9 | loss: 0.002909426770793577\n",
      "epoch 733 | step 10 | loss: 0.0031846493779765727\n",
      "epoch 733 | step 11 | loss: 0.003495187614450204\n",
      "epoch 734 | step 0 | loss: 0.00028100391821875135\n",
      "epoch 734 | step 1 | loss: 0.0005451324059916582\n",
      "epoch 734 | step 2 | loss: 0.0008313510231022836\n",
      "epoch 734 | step 3 | loss: 0.001142880950288763\n",
      "epoch 734 | step 4 | loss: 0.0014426572287550468\n",
      "epoch 734 | step 5 | loss: 0.0017479989781451686\n",
      "epoch 734 | step 6 | loss: 0.0020592519261216813\n",
      "epoch 734 | step 7 | loss: 0.0023431503353403475\n",
      "epoch 734 | step 8 | loss: 0.0026280559280261713\n",
      "epoch 734 | step 9 | loss: 0.0029294032287425662\n",
      "epoch 734 | step 10 | loss: 0.0032220480065450106\n",
      "epoch 734 | step 11 | loss: 0.0034803147904802975\n",
      "epoch 735 | step 0 | loss: 0.00031843154520306535\n",
      "epoch 735 | step 1 | loss: 0.0006109757658954337\n",
      "epoch 735 | step 2 | loss: 0.0008924911172536471\n",
      "epoch 735 | step 3 | loss: 0.0012023711268079975\n",
      "epoch 735 | step 4 | loss: 0.0014890925152825723\n",
      "epoch 735 | step 5 | loss: 0.001755180115452328\n",
      "epoch 735 | step 6 | loss: 0.0020496508599955722\n",
      "epoch 735 | step 7 | loss: 0.002349444538650159\n",
      "epoch 735 | step 8 | loss: 0.0026435123264545043\n",
      "epoch 735 | step 9 | loss: 0.002911214562306184\n",
      "epoch 735 | step 10 | loss: 0.003212178472719377\n",
      "epoch 735 | step 11 | loss: 0.003484032237537216\n",
      "epoch 736 | step 0 | loss: 0.0002699104224577823\n",
      "epoch 736 | step 1 | loss: 0.0005641883579200051\n",
      "epoch 736 | step 2 | loss: 0.0008513052497215292\n",
      "epoch 736 | step 3 | loss: 0.0011254803376857668\n",
      "epoch 736 | step 4 | loss: 0.0014325979466077604\n",
      "epoch 736 | step 5 | loss: 0.0017155460440271253\n",
      "epoch 736 | step 6 | loss: 0.0020048773543113905\n",
      "epoch 736 | step 7 | loss: 0.002307554974862756\n",
      "epoch 736 | step 8 | loss: 0.0025831609756619277\n",
      "epoch 736 | step 9 | loss: 0.0028851523546413096\n",
      "epoch 736 | step 10 | loss: 0.0031798203819391343\n",
      "epoch 736 | step 11 | loss: 0.0034968665080425806\n",
      "epoch 737 | step 0 | loss: 0.0003012521676431879\n",
      "epoch 737 | step 1 | loss: 0.0006335713074800772\n",
      "epoch 737 | step 2 | loss: 0.0009023510658479821\n",
      "epoch 737 | step 3 | loss: 0.0012037304261875554\n",
      "epoch 737 | step 4 | loss: 0.001498679009764536\n",
      "epoch 737 | step 5 | loss: 0.0017768694977781635\n",
      "epoch 737 | step 6 | loss: 0.00205728876754246\n",
      "epoch 737 | step 7 | loss: 0.002328083330360969\n",
      "epoch 737 | step 8 | loss: 0.002611525603943698\n",
      "epoch 737 | step 9 | loss: 0.0029136388151015445\n",
      "epoch 737 | step 10 | loss: 0.003199952483739897\n",
      "epoch 737 | step 11 | loss: 0.0034886387468000845\n",
      "epoch 738 | step 0 | loss: 0.0002935293184790272\n",
      "epoch 738 | step 1 | loss: 0.000565632683667157\n",
      "epoch 738 | step 2 | loss: 0.0008576775425557448\n",
      "epoch 738 | step 3 | loss: 0.0011439644587021207\n",
      "epoch 738 | step 4 | loss: 0.0014382301127901138\n",
      "epoch 738 | step 5 | loss: 0.0017485215416475565\n",
      "epoch 738 | step 6 | loss: 0.002035076607673373\n",
      "epoch 738 | step 7 | loss: 0.0023099864578046713\n",
      "epoch 738 | step 8 | loss: 0.002603647332143047\n",
      "epoch 738 | step 9 | loss: 0.0028786025870140273\n",
      "epoch 738 | step 10 | loss: 0.0031761988626067593\n",
      "epoch 738 | step 11 | loss: 0.003498037272595627\n",
      "epoch 739 | step 0 | loss: 0.0002736702890938708\n",
      "epoch 739 | step 1 | loss: 0.0005819813707236626\n",
      "epoch 739 | step 2 | loss: 0.0008884718487571968\n",
      "epoch 739 | step 3 | loss: 0.0011960677773829391\n",
      "epoch 739 | step 4 | loss: 0.0014700013032380847\n",
      "epoch 739 | step 5 | loss: 0.0017509621859353145\n",
      "epoch 739 | step 6 | loss: 0.002076761165409913\n",
      "epoch 739 | step 7 | loss: 0.0023671216425633287\n",
      "epoch 739 | step 8 | loss: 0.002636093904639156\n",
      "epoch 739 | step 9 | loss: 0.0029288192058091102\n",
      "epoch 739 | step 10 | loss: 0.0032081777690091675\n",
      "epoch 739 | step 11 | loss: 0.003485244111117984\n",
      "epoch 740 | step 0 | loss: 0.00027914853896673323\n",
      "epoch 740 | step 1 | loss: 0.0005460025974368033\n",
      "epoch 740 | step 2 | loss: 0.0008308645286213935\n",
      "epoch 740 | step 3 | loss: 0.0011241915144834106\n",
      "epoch 740 | step 4 | loss: 0.001430504958725078\n",
      "epoch 740 | step 5 | loss: 0.0017277004939785126\n",
      "epoch 740 | step 6 | loss: 0.0020079909872128482\n",
      "epoch 740 | step 7 | loss: 0.0022992638469853605\n",
      "epoch 740 | step 8 | loss: 0.0025853193761461836\n",
      "epoch 740 | step 9 | loss: 0.002890405685731615\n",
      "epoch 740 | step 10 | loss: 0.0031865661524613557\n",
      "epoch 740 | step 11 | loss: 0.0034935369170005274\n",
      "epoch 741 | step 0 | loss: 0.0003080143443280211\n",
      "epoch 741 | step 1 | loss: 0.0005742822795892922\n",
      "epoch 741 | step 2 | loss: 0.0008725566624456155\n",
      "epoch 741 | step 3 | loss: 0.0011714016645106332\n",
      "epoch 741 | step 4 | loss: 0.001454207182188471\n",
      "epoch 741 | step 5 | loss: 0.0017460015471512426\n",
      "epoch 741 | step 6 | loss: 0.00206114261027811\n",
      "epoch 741 | step 7 | loss: 0.0023393060544145633\n",
      "epoch 741 | step 8 | loss: 0.0026417537786570175\n",
      "epoch 741 | step 9 | loss: 0.0029126095929548185\n",
      "epoch 741 | step 10 | loss: 0.0031906686397020685\n",
      "epoch 741 | step 11 | loss: 0.003492238336048747\n",
      "epoch 742 | step 0 | loss: 0.0002889261446980515\n",
      "epoch 742 | step 1 | loss: 0.0005654782327440658\n",
      "epoch 742 | step 2 | loss: 0.000846403640576952\n",
      "epoch 742 | step 3 | loss: 0.0011814898779069273\n",
      "epoch 742 | step 4 | loss: 0.001458037324542717\n",
      "epoch 742 | step 5 | loss: 0.001723877480261567\n",
      "epoch 742 | step 6 | loss: 0.001995018691111159\n",
      "epoch 742 | step 7 | loss: 0.002300987877619862\n",
      "epoch 742 | step 8 | loss: 0.0026164048562693485\n",
      "epoch 742 | step 9 | loss: 0.0029211100451336297\n",
      "epoch 742 | step 10 | loss: 0.003187281366168608\n",
      "epoch 742 | step 11 | loss: 0.00349300305819726\n",
      "epoch 743 | step 0 | loss: 0.00031010751543604877\n",
      "epoch 743 | step 1 | loss: 0.0006027641548689096\n",
      "epoch 743 | step 2 | loss: 0.0008798534602768211\n",
      "epoch 743 | step 3 | loss: 0.001143786672937601\n",
      "epoch 743 | step 4 | loss: 0.001445796967488881\n",
      "epoch 743 | step 5 | loss: 0.0017476664460266452\n",
      "epoch 743 | step 6 | loss: 0.0020533193452208914\n",
      "epoch 743 | step 7 | loss: 0.002343098270627144\n",
      "epoch 743 | step 8 | loss: 0.002617730921460629\n",
      "epoch 743 | step 9 | loss: 0.0029207527096486974\n",
      "epoch 743 | step 10 | loss: 0.003210771146421925\n",
      "epoch 743 | step 11 | loss: 0.0034837048150431387\n",
      "epoch 744 | step 0 | loss: 0.00028168379288444096\n",
      "epoch 744 | step 1 | loss: 0.0005835676413328565\n",
      "epoch 744 | step 2 | loss: 0.0008592151429001077\n",
      "epoch 744 | step 3 | loss: 0.0011742369956554508\n",
      "epoch 744 | step 4 | loss: 0.0014399630718179787\n",
      "epoch 744 | step 5 | loss: 0.001741741639403855\n",
      "epoch 744 | step 6 | loss: 0.002059721248637106\n",
      "epoch 744 | step 7 | loss: 0.0023421360325990652\n",
      "epoch 744 | step 8 | loss: 0.002616882937935059\n",
      "epoch 744 | step 9 | loss: 0.0029043941140950136\n",
      "epoch 744 | step 10 | loss: 0.003218263508571096\n",
      "epoch 744 | step 11 | loss: 0.003480820709705887\n",
      "epoch 745 | step 0 | loss: 0.00026386680008668893\n",
      "epoch 745 | step 1 | loss: 0.0005502233952431382\n",
      "epoch 745 | step 2 | loss: 0.0008624370053072393\n",
      "epoch 745 | step 3 | loss: 0.0011563350616101951\n",
      "epoch 745 | step 4 | loss: 0.0014340387939489202\n",
      "epoch 745 | step 5 | loss: 0.0017353738100488862\n",
      "epoch 745 | step 6 | loss: 0.00203782476020171\n",
      "epoch 745 | step 7 | loss: 0.002300257541835316\n",
      "epoch 745 | step 8 | loss: 0.002610956284948998\n",
      "epoch 745 | step 9 | loss: 0.002920678954191594\n",
      "epoch 745 | step 10 | loss: 0.0032144795198265795\n",
      "epoch 745 | step 11 | loss: 0.0034823160817899954\n",
      "epoch 746 | step 0 | loss: 0.00026602598785367834\n",
      "epoch 746 | step 1 | loss: 0.0005354902450954028\n",
      "epoch 746 | step 2 | loss: 0.0008651037537988355\n",
      "epoch 746 | step 3 | loss: 0.001126885850973727\n",
      "epoch 746 | step 4 | loss: 0.0014009598446202674\n",
      "epoch 746 | step 5 | loss: 0.0016967852400350918\n",
      "epoch 746 | step 6 | loss: 0.00201759843115411\n",
      "epoch 746 | step 7 | loss: 0.0023174983145242785\n",
      "epoch 746 | step 8 | loss: 0.002616501141722612\n",
      "epoch 746 | step 9 | loss: 0.002905329840800222\n",
      "epoch 746 | step 10 | loss: 0.0031977982381142882\n",
      "epoch 746 | step 11 | loss: 0.0034884881381163912\n",
      "epoch 747 | step 0 | loss: 0.00028087336791718\n",
      "epoch 747 | step 1 | loss: 0.0005742705283088965\n",
      "epoch 747 | step 2 | loss: 0.0008517480403136858\n",
      "epoch 747 | step 3 | loss: 0.0011526632899202408\n",
      "epoch 747 | step 4 | loss: 0.001435884117605991\n",
      "epoch 747 | step 5 | loss: 0.0017247623510213768\n",
      "epoch 747 | step 6 | loss: 0.001996033859992213\n",
      "epoch 747 | step 7 | loss: 0.0022825317324332997\n",
      "epoch 747 | step 8 | loss: 0.002604993282884322\n",
      "epoch 747 | step 9 | loss: 0.0028985723776306376\n",
      "epoch 747 | step 10 | loss: 0.003183996440463588\n",
      "epoch 747 | step 11 | loss: 0.0034939158700101553\n",
      "epoch 748 | step 0 | loss: 0.0003101456794791575\n",
      "epoch 748 | step 1 | loss: 0.0005801981970586511\n",
      "epoch 748 | step 2 | loss: 0.0008918107325980539\n",
      "epoch 748 | step 3 | loss: 0.0011575014401195604\n",
      "epoch 748 | step 4 | loss: 0.0014661478584183517\n",
      "epoch 748 | step 5 | loss: 0.001749806702417027\n",
      "epoch 748 | step 6 | loss: 0.002053625714938777\n",
      "epoch 748 | step 7 | loss: 0.0023338711457269097\n",
      "epoch 748 | step 8 | loss: 0.002610555313034216\n",
      "epoch 748 | step 9 | loss: 0.0029103712301668952\n",
      "epoch 748 | step 10 | loss: 0.003179525061171537\n",
      "epoch 748 | step 11 | loss: 0.0034956749571912497\n",
      "epoch 749 | step 0 | loss: 0.00027407322258747027\n",
      "epoch 749 | step 1 | loss: 0.0005612723124132849\n",
      "epoch 749 | step 2 | loss: 0.0008314674108641146\n",
      "epoch 749 | step 3 | loss: 0.0010977200337133385\n",
      "epoch 749 | step 4 | loss: 0.0013893614073930533\n",
      "epoch 749 | step 5 | loss: 0.0016691612511876986\n",
      "epoch 749 | step 6 | loss: 0.001949897552317169\n",
      "epoch 749 | step 7 | loss: 0.0022774068810096112\n",
      "epoch 749 | step 8 | loss: 0.0025527732678603064\n",
      "epoch 749 | step 9 | loss: 0.002860935766761049\n",
      "epoch 749 | step 10 | loss: 0.003161939397620092\n",
      "epoch 749 | step 11 | loss: 0.003502391527682982\n",
      "epoch 750 | step 0 | loss: 0.00027519892261613084\n",
      "epoch 750 | step 1 | loss: 0.0005361503699658749\n",
      "epoch 750 | step 2 | loss: 0.0008517036730965172\n",
      "epoch 750 | step 3 | loss: 0.0011526589688492675\n",
      "epoch 750 | step 4 | loss: 0.0014149296378007663\n",
      "epoch 750 | step 5 | loss: 0.0017098033963738161\n",
      "epoch 750 | step 6 | loss: 0.0019990981511800567\n",
      "epoch 750 | step 7 | loss: 0.002319205207203515\n",
      "epoch 750 | step 8 | loss: 0.0026272702321897467\n",
      "epoch 750 | step 9 | loss: 0.002924410841796569\n",
      "epoch 750 | step 10 | loss: 0.003213948733450684\n",
      "epoch 750 | step 11 | loss: 0.003481644541878594\n",
      "epoch 751 | step 0 | loss: 0.0002758313548115665\n",
      "epoch 751 | step 1 | loss: 0.0005779757410337494\n",
      "epoch 751 | step 2 | loss: 0.0008758746745050115\n",
      "epoch 751 | step 3 | loss: 0.0011914121181689633\n",
      "epoch 751 | step 4 | loss: 0.001473081861645113\n",
      "epoch 751 | step 5 | loss: 0.0017985955845522292\n",
      "epoch 751 | step 6 | loss: 0.0020912228591948865\n",
      "epoch 751 | step 7 | loss: 0.0023899494594987075\n",
      "epoch 751 | step 8 | loss: 0.0026598652371947564\n",
      "epoch 751 | step 9 | loss: 0.0029231558729982715\n",
      "epoch 751 | step 10 | loss: 0.0031976267936090026\n",
      "epoch 751 | step 11 | loss: 0.0034880948909430816\n",
      "epoch 752 | step 0 | loss: 0.0002980998974454499\n",
      "epoch 752 | step 1 | loss: 0.0005771766271962552\n",
      "epoch 752 | step 2 | loss: 0.0008643651903203733\n",
      "epoch 752 | step 3 | loss: 0.0011709942922329662\n",
      "epoch 752 | step 4 | loss: 0.001479394674814756\n",
      "epoch 752 | step 5 | loss: 0.0017608938768820775\n",
      "epoch 752 | step 6 | loss: 0.002038477539769106\n",
      "epoch 752 | step 7 | loss: 0.0023226374851001348\n",
      "epoch 752 | step 8 | loss: 0.0025996459386038477\n",
      "epoch 752 | step 9 | loss: 0.002901983751610361\n",
      "epoch 752 | step 10 | loss: 0.0032012362907965337\n",
      "epoch 752 | step 11 | loss: 0.0034863777630588476\n",
      "epoch 753 | step 0 | loss: 0.0003133699639433471\n",
      "epoch 753 | step 1 | loss: 0.0005961522367810576\n",
      "epoch 753 | step 2 | loss: 0.000871704354252387\n",
      "epoch 753 | step 3 | loss: 0.0011521234804338033\n",
      "epoch 753 | step 4 | loss: 0.0014508184978411422\n",
      "epoch 753 | step 5 | loss: 0.0017787563200381297\n",
      "epoch 753 | step 6 | loss: 0.002035526628097263\n",
      "epoch 753 | step 7 | loss: 0.0023234338144165\n",
      "epoch 753 | step 8 | loss: 0.00262209759273264\n",
      "epoch 753 | step 9 | loss: 0.002903179710340577\n",
      "epoch 753 | step 10 | loss: 0.003201244475056893\n",
      "epoch 753 | step 11 | loss: 0.0034864593649301277\n",
      "epoch 754 | step 0 | loss: 0.0002741992652127071\n",
      "epoch 754 | step 1 | loss: 0.0005659665119902635\n",
      "epoch 754 | step 2 | loss: 0.0008701080162881816\n",
      "epoch 754 | step 3 | loss: 0.0011684803839255111\n",
      "epoch 754 | step 4 | loss: 0.0014566740385780006\n",
      "epoch 754 | step 5 | loss: 0.0017139448129905438\n",
      "epoch 754 | step 6 | loss: 0.001988910858804747\n",
      "epoch 754 | step 7 | loss: 0.0022666234482820605\n",
      "epoch 754 | step 8 | loss: 0.002570412792409154\n",
      "epoch 754 | step 9 | loss: 0.0029182087873574594\n",
      "epoch 754 | step 10 | loss: 0.0032035871153825784\n",
      "epoch 754 | step 11 | loss: 0.0034855964378536324\n",
      "epoch 755 | step 0 | loss: 0.0002760211476877865\n",
      "epoch 755 | step 1 | loss: 0.0005602728718500707\n",
      "epoch 755 | step 2 | loss: 0.0008623723747134947\n",
      "epoch 755 | step 3 | loss: 0.0011768333106802803\n",
      "epoch 755 | step 4 | loss: 0.0014506519482499816\n",
      "epoch 755 | step 5 | loss: 0.001742396550320562\n",
      "epoch 755 | step 6 | loss: 0.00199226485657999\n",
      "epoch 755 | step 7 | loss: 0.0022957861900016682\n",
      "epoch 755 | step 8 | loss: 0.002592769249614388\n",
      "epoch 755 | step 9 | loss: 0.0028874264439314673\n",
      "epoch 755 | step 10 | loss: 0.0031964359382282786\n",
      "epoch 755 | step 11 | loss: 0.0034879930596036337\n",
      "epoch 756 | step 0 | loss: 0.0002815083495694429\n",
      "epoch 756 | step 1 | loss: 0.0005552594080636328\n",
      "epoch 756 | step 2 | loss: 0.0008341264348934495\n",
      "epoch 756 | step 3 | loss: 0.0011331462367118951\n",
      "epoch 756 | step 4 | loss: 0.0014235555688737948\n",
      "epoch 756 | step 5 | loss: 0.0017252656220837644\n",
      "epoch 756 | step 6 | loss: 0.0020256603701128703\n",
      "epoch 756 | step 7 | loss: 0.0023221818245035123\n",
      "epoch 756 | step 8 | loss: 0.002594729637036349\n",
      "epoch 756 | step 9 | loss: 0.0028998130470622337\n",
      "epoch 756 | step 10 | loss: 0.0031936967128266187\n",
      "epoch 756 | step 11 | loss: 0.003489042629117498\n",
      "epoch 757 | step 0 | loss: 0.00029100128763998197\n",
      "epoch 757 | step 1 | loss: 0.000594150978709325\n",
      "epoch 757 | step 2 | loss: 0.0008989934833808549\n",
      "epoch 757 | step 3 | loss: 0.0011920888115988298\n",
      "epoch 757 | step 4 | loss: 0.0014963107488548638\n",
      "epoch 757 | step 5 | loss: 0.0018062166478648375\n",
      "epoch 757 | step 6 | loss: 0.002075067292891749\n",
      "epoch 757 | step 7 | loss: 0.002355128463465788\n",
      "epoch 757 | step 8 | loss: 0.0026174685567810917\n",
      "epoch 757 | step 9 | loss: 0.002924889495566681\n",
      "epoch 757 | step 10 | loss: 0.0032296789747024096\n",
      "epoch 757 | step 11 | loss: 0.0034749190279525512\n",
      "epoch 758 | step 0 | loss: 0.0002975852296374307\n",
      "epoch 758 | step 1 | loss: 0.0005789157879357629\n",
      "epoch 758 | step 2 | loss: 0.000864550493254118\n",
      "epoch 758 | step 3 | loss: 0.0011640819747938155\n",
      "epoch 758 | step 4 | loss: 0.0014394624786191687\n",
      "epoch 758 | step 5 | loss: 0.0017022147009636186\n",
      "epoch 758 | step 6 | loss: 0.0019932567038097597\n",
      "epoch 758 | step 7 | loss: 0.0023006363828300997\n",
      "epoch 758 | step 8 | loss: 0.002622572292425051\n",
      "epoch 758 | step 9 | loss: 0.002908848073204726\n",
      "epoch 758 | step 10 | loss: 0.003187341820083286\n",
      "epoch 758 | step 11 | loss: 0.003491636356803814\n",
      "epoch 759 | step 0 | loss: 0.0002907869781370316\n",
      "epoch 759 | step 1 | loss: 0.0006085522741210308\n",
      "epoch 759 | step 2 | loss: 0.0009024533554077514\n",
      "epoch 759 | step 3 | loss: 0.001159867963533593\n",
      "epoch 759 | step 4 | loss: 0.0014686560997368206\n",
      "epoch 759 | step 5 | loss: 0.0017419349911911805\n",
      "epoch 759 | step 6 | loss: 0.002049162744054422\n",
      "epoch 759 | step 7 | loss: 0.0023215329276168773\n",
      "epoch 759 | step 8 | loss: 0.0026142965777924657\n",
      "epoch 759 | step 9 | loss: 0.0028940097472544375\n",
      "epoch 759 | step 10 | loss: 0.003187435729401997\n",
      "epoch 759 | step 11 | loss: 0.003491495656886782\n",
      "epoch 760 | step 0 | loss: 0.00031801726355527345\n",
      "epoch 760 | step 1 | loss: 0.0005980279936258781\n",
      "epoch 760 | step 2 | loss: 0.0008984931137048303\n",
      "epoch 760 | step 3 | loss: 0.0012028718572627715\n",
      "epoch 760 | step 4 | loss: 0.0014869451184104296\n",
      "epoch 760 | step 5 | loss: 0.0017447529019838266\n",
      "epoch 760 | step 6 | loss: 0.002017858374263312\n",
      "epoch 760 | step 7 | loss: 0.0023194561105609243\n",
      "epoch 760 | step 8 | loss: 0.0025963072742616715\n",
      "epoch 760 | step 9 | loss: 0.0028775752969796\n",
      "epoch 760 | step 10 | loss: 0.003187486413102337\n",
      "epoch 760 | step 11 | loss: 0.0034908920078462564\n",
      "epoch 761 | step 0 | loss: 0.00032041439111135867\n",
      "epoch 761 | step 1 | loss: 0.0006117421997495779\n",
      "epoch 761 | step 2 | loss: 0.0009125938771315546\n",
      "epoch 761 | step 3 | loss: 0.0012185505537813317\n",
      "epoch 761 | step 4 | loss: 0.0015047635516450954\n",
      "epoch 761 | step 5 | loss: 0.0017809274273076672\n",
      "epoch 761 | step 6 | loss: 0.0020553239008462266\n",
      "epoch 761 | step 7 | loss: 0.002317953943111007\n",
      "epoch 761 | step 8 | loss: 0.0025987894448942428\n",
      "epoch 761 | step 9 | loss: 0.0028993065085568632\n",
      "epoch 761 | step 10 | loss: 0.003187088875213671\n",
      "epoch 761 | step 11 | loss: 0.003491288393526231\n",
      "epoch 762 | step 0 | loss: 0.00028003190794782064\n",
      "epoch 762 | step 1 | loss: 0.000559474432740451\n",
      "epoch 762 | step 2 | loss: 0.0008575374885837373\n",
      "epoch 762 | step 3 | loss: 0.0011415910858386595\n",
      "epoch 762 | step 4 | loss: 0.0014526722881455522\n",
      "epoch 762 | step 5 | loss: 0.0017397985335701513\n",
      "epoch 762 | step 6 | loss: 0.002033223664797402\n",
      "epoch 762 | step 7 | loss: 0.002329950394152439\n",
      "epoch 762 | step 8 | loss: 0.0026137752140516324\n",
      "epoch 762 | step 9 | loss: 0.0029261427835060717\n",
      "epoch 762 | step 10 | loss: 0.003192676373098563\n",
      "epoch 762 | step 11 | loss: 0.003488545296865743\n",
      "epoch 763 | step 0 | loss: 0.00028821167606266627\n",
      "epoch 763 | step 1 | loss: 0.0005561099911750103\n",
      "epoch 763 | step 2 | loss: 0.0008457223550226761\n",
      "epoch 763 | step 3 | loss: 0.001143734169462822\n",
      "epoch 763 | step 4 | loss: 0.0014270345536362898\n",
      "epoch 763 | step 5 | loss: 0.0017286498087341362\n",
      "epoch 763 | step 6 | loss: 0.002012853577785455\n",
      "epoch 763 | step 7 | loss: 0.0023316198774215252\n",
      "epoch 763 | step 8 | loss: 0.0026101865654279614\n",
      "epoch 763 | step 9 | loss: 0.0029041109417894295\n",
      "epoch 763 | step 10 | loss: 0.0031826165353738313\n",
      "epoch 763 | step 11 | loss: 0.0034926801706426174\n",
      "epoch 764 | step 0 | loss: 0.0003041136939533921\n",
      "epoch 764 | step 1 | loss: 0.0006223577875055779\n",
      "epoch 764 | step 2 | loss: 0.0009047433693468778\n",
      "epoch 764 | step 3 | loss: 0.001172633799672816\n",
      "epoch 764 | step 4 | loss: 0.0014631790420571139\n",
      "epoch 764 | step 5 | loss: 0.0017384296539215284\n",
      "epoch 764 | step 6 | loss: 0.002029777219250416\n",
      "epoch 764 | step 7 | loss: 0.0023100577804651716\n",
      "epoch 764 | step 8 | loss: 0.0025926081270974555\n",
      "epoch 764 | step 9 | loss: 0.002905053535929404\n",
      "epoch 764 | step 10 | loss: 0.0032093027302939926\n",
      "epoch 764 | step 11 | loss: 0.0034823895593903836\n",
      "epoch 765 | step 0 | loss: 0.00028156466258243915\n",
      "epoch 765 | step 1 | loss: 0.0005683118069808308\n",
      "epoch 765 | step 2 | loss: 0.0008725256447411955\n",
      "epoch 765 | step 3 | loss: 0.0011775101839021456\n",
      "epoch 765 | step 4 | loss: 0.001463258278615485\n",
      "epoch 765 | step 5 | loss: 0.001747517978238342\n",
      "epoch 765 | step 6 | loss: 0.0020509734771712146\n",
      "epoch 765 | step 7 | loss: 0.002350544572674966\n",
      "epoch 765 | step 8 | loss: 0.002634650296161789\n",
      "epoch 765 | step 9 | loss: 0.0029514216476973425\n",
      "epoch 765 | step 10 | loss: 0.003220777075379441\n",
      "epoch 765 | step 11 | loss: 0.00347746269421113\n",
      "epoch 766 | step 0 | loss: 0.0002700662689144575\n",
      "epoch 766 | step 1 | loss: 0.0005666155666381509\n",
      "epoch 766 | step 2 | loss: 0.0009007914617521401\n",
      "epoch 766 | step 3 | loss: 0.00116677992305988\n",
      "epoch 766 | step 4 | loss: 0.0014507323651490813\n",
      "epoch 766 | step 5 | loss: 0.0017490817787549164\n",
      "epoch 766 | step 6 | loss: 0.002039897191628888\n",
      "epoch 766 | step 7 | loss: 0.0023631187264309883\n",
      "epoch 766 | step 8 | loss: 0.00263086565396916\n",
      "epoch 766 | step 9 | loss: 0.002924804522669039\n",
      "epoch 766 | step 10 | loss: 0.0032043342997314225\n",
      "epoch 766 | step 11 | loss: 0.0034836891602222316\n",
      "epoch 767 | step 0 | loss: 0.0002846071581944937\n",
      "epoch 767 | step 1 | loss: 0.0005861545870353367\n",
      "epoch 767 | step 2 | loss: 0.0008702885141815573\n",
      "epoch 767 | step 3 | loss: 0.001167027821269532\n",
      "epoch 767 | step 4 | loss: 0.0014412750226554323\n",
      "epoch 767 | step 5 | loss: 0.0017126647334387325\n",
      "epoch 767 | step 6 | loss: 0.0019995785372120744\n",
      "epoch 767 | step 7 | loss: 0.002293488995647809\n",
      "epoch 767 | step 8 | loss: 0.0025897185351335056\n",
      "epoch 767 | step 9 | loss: 0.002877187244300355\n",
      "epoch 767 | step 10 | loss: 0.003205703209056052\n",
      "epoch 767 | step 11 | loss: 0.0034832569109023\n",
      "epoch 768 | step 0 | loss: 0.00030124153495057776\n",
      "epoch 768 | step 1 | loss: 0.0005912856687542383\n",
      "epoch 768 | step 2 | loss: 0.0008793170845649182\n",
      "epoch 768 | step 3 | loss: 0.0011396714705257317\n",
      "epoch 768 | step 4 | loss: 0.0014234821074226016\n",
      "epoch 768 | step 5 | loss: 0.0017218777398623195\n",
      "epoch 768 | step 6 | loss: 0.0020060754116307316\n",
      "epoch 768 | step 7 | loss: 0.0022977433238264206\n",
      "epoch 768 | step 8 | loss: 0.002589552758195033\n",
      "epoch 768 | step 9 | loss: 0.002884687747841073\n",
      "epoch 768 | step 10 | loss: 0.0031826097524392553\n",
      "epoch 768 | step 11 | loss: 0.0034921240381405503\n",
      "epoch 769 | step 0 | loss: 0.0002777304858956581\n",
      "epoch 769 | step 1 | loss: 0.0005502258251446748\n",
      "epoch 769 | step 2 | loss: 0.0008246727168417858\n",
      "epoch 769 | step 3 | loss: 0.0011170449577798673\n",
      "epoch 769 | step 4 | loss: 0.0014196682551493416\n",
      "epoch 769 | step 5 | loss: 0.0017109331204946386\n",
      "epoch 769 | step 6 | loss: 0.0020118471289031934\n",
      "epoch 769 | step 7 | loss: 0.002296051618080581\n",
      "epoch 769 | step 8 | loss: 0.0026106160906425096\n",
      "epoch 769 | step 9 | loss: 0.0029178665796107247\n",
      "epoch 769 | step 10 | loss: 0.003207437625232841\n",
      "epoch 769 | step 11 | loss: 0.003482470708627828\n",
      "epoch 770 | step 0 | loss: 0.00029312538501986564\n",
      "epoch 770 | step 1 | loss: 0.0005490488081579221\n",
      "epoch 770 | step 2 | loss: 0.0008383016821725622\n",
      "epoch 770 | step 3 | loss: 0.0011438631959705108\n",
      "epoch 770 | step 4 | loss: 0.0014362070491102286\n",
      "epoch 770 | step 5 | loss: 0.0017469853509186211\n",
      "epoch 770 | step 6 | loss: 0.002032643252614372\n",
      "epoch 770 | step 7 | loss: 0.002328543801521515\n",
      "epoch 770 | step 8 | loss: 0.0026221799673601205\n",
      "epoch 770 | step 9 | loss: 0.0029152089418021396\n",
      "epoch 770 | step 10 | loss: 0.003190428909448817\n",
      "epoch 770 | step 11 | loss: 0.0034887138033143734\n",
      "epoch 771 | step 0 | loss: 0.00027116835422427336\n",
      "epoch 771 | step 1 | loss: 0.0005522470091827951\n",
      "epoch 771 | step 2 | loss: 0.0008522563209046492\n",
      "epoch 771 | step 3 | loss: 0.0011413312274107129\n",
      "epoch 771 | step 4 | loss: 0.0014445348006367226\n",
      "epoch 771 | step 5 | loss: 0.0017428147455135796\n",
      "epoch 771 | step 6 | loss: 0.001984395258905637\n",
      "epoch 771 | step 7 | loss: 0.0022776877942905253\n",
      "epoch 771 | step 8 | loss: 0.0026067407750559194\n",
      "epoch 771 | step 9 | loss: 0.002906180205188379\n",
      "epoch 771 | step 10 | loss: 0.0032130660172863027\n",
      "epoch 771 | step 11 | loss: 0.0034802742500234405\n",
      "epoch 772 | step 0 | loss: 0.0002867010862473422\n",
      "epoch 772 | step 1 | loss: 0.0005504884249909235\n",
      "epoch 772 | step 2 | loss: 0.0008185382291604777\n",
      "epoch 772 | step 3 | loss: 0.0011288965597585185\n",
      "epoch 772 | step 4 | loss: 0.0014439898096146645\n",
      "epoch 772 | step 5 | loss: 0.0017373776744976429\n",
      "epoch 772 | step 6 | loss: 0.0020242583313043222\n",
      "epoch 772 | step 7 | loss: 0.002307709395549996\n",
      "epoch 772 | step 8 | loss: 0.0026006696975191847\n",
      "epoch 772 | step 9 | loss: 0.002884115536673945\n",
      "epoch 772 | step 10 | loss: 0.0031890417785575284\n",
      "epoch 772 | step 11 | loss: 0.003489205793029629\n",
      "epoch 773 | step 0 | loss: 0.0002623857527797407\n",
      "epoch 773 | step 1 | loss: 0.0005655949408542108\n",
      "epoch 773 | step 2 | loss: 0.0008416257928778754\n",
      "epoch 773 | step 3 | loss: 0.0011813520612156557\n",
      "epoch 773 | step 4 | loss: 0.0014699157456144136\n",
      "epoch 773 | step 5 | loss: 0.0017340634907593356\n",
      "epoch 773 | step 6 | loss: 0.0020300334572599605\n",
      "epoch 773 | step 7 | loss: 0.0023449919271629296\n",
      "epoch 773 | step 8 | loss: 0.0026370293909518698\n",
      "epoch 773 | step 9 | loss: 0.0029246097896066855\n",
      "epoch 773 | step 10 | loss: 0.0032087635042178653\n",
      "epoch 773 | step 11 | loss: 0.003481400213747655\n",
      "epoch 774 | step 0 | loss: 0.00027905205824982506\n",
      "epoch 774 | step 1 | loss: 0.0005492199243696874\n",
      "epoch 774 | step 2 | loss: 0.0008310385578069026\n",
      "epoch 774 | step 3 | loss: 0.0011261436009329699\n",
      "epoch 774 | step 4 | loss: 0.0014335260870372984\n",
      "epoch 774 | step 5 | loss: 0.0017245097216996384\n",
      "epoch 774 | step 6 | loss: 0.0020080924799470417\n",
      "epoch 774 | step 7 | loss: 0.002291005909874111\n",
      "epoch 774 | step 8 | loss: 0.002575934718423268\n",
      "epoch 774 | step 9 | loss: 0.002884912346077948\n",
      "epoch 774 | step 10 | loss: 0.0031834430373002274\n",
      "epoch 774 | step 11 | loss: 0.003491021309538414\n",
      "epoch 775 | step 0 | loss: 0.000281660175137579\n",
      "epoch 775 | step 1 | loss: 0.0005686217932877312\n",
      "epoch 775 | step 2 | loss: 0.0008435269686188223\n",
      "epoch 775 | step 3 | loss: 0.0011374347760494067\n",
      "epoch 775 | step 4 | loss: 0.0014411991975929975\n",
      "epoch 775 | step 5 | loss: 0.001732122366927719\n",
      "epoch 775 | step 6 | loss: 0.002052627634174285\n",
      "epoch 775 | step 7 | loss: 0.0023377764898257947\n",
      "epoch 775 | step 8 | loss: 0.0026176399425621878\n",
      "epoch 775 | step 9 | loss: 0.0029065939230492163\n",
      "epoch 775 | step 10 | loss: 0.0031900755339166163\n",
      "epoch 775 | step 11 | loss: 0.003488547123593608\n",
      "epoch 776 | step 0 | loss: 0.00030249891564734067\n",
      "epoch 776 | step 1 | loss: 0.0005924185099236287\n",
      "epoch 776 | step 2 | loss: 0.0009107616830663365\n",
      "epoch 776 | step 3 | loss: 0.0011883367876816533\n",
      "epoch 776 | step 4 | loss: 0.001492678883847218\n",
      "epoch 776 | step 5 | loss: 0.0017761049879047824\n",
      "epoch 776 | step 6 | loss: 0.002070088855797423\n",
      "epoch 776 | step 7 | loss: 0.0023533533386853613\n",
      "epoch 776 | step 8 | loss: 0.0026600491327234015\n",
      "epoch 776 | step 9 | loss: 0.0029394525793280768\n",
      "epoch 776 | step 10 | loss: 0.003210178063969059\n",
      "epoch 776 | step 11 | loss: 0.003480413497240587\n",
      "epoch 777 | step 0 | loss: 0.00027741156920613544\n",
      "epoch 777 | step 1 | loss: 0.0005930493675817156\n",
      "epoch 777 | step 2 | loss: 0.0008753553663551336\n",
      "epoch 777 | step 3 | loss: 0.0011705399451590545\n",
      "epoch 777 | step 4 | loss: 0.0014443362689577474\n",
      "epoch 777 | step 5 | loss: 0.0017488199141598448\n",
      "epoch 777 | step 6 | loss: 0.0020234324615215427\n",
      "epoch 777 | step 7 | loss: 0.0023239055086536125\n",
      "epoch 777 | step 8 | loss: 0.0026255045705430744\n",
      "epoch 777 | step 9 | loss: 0.002903532158264889\n",
      "epoch 777 | step 10 | loss: 0.0031791417143576897\n",
      "epoch 777 | step 11 | loss: 0.0034923672522585046\n",
      "epoch 778 | step 0 | loss: 0.000295137439207272\n",
      "epoch 778 | step 1 | loss: 0.0006069907226475446\n",
      "epoch 778 | step 2 | loss: 0.0008970730413362894\n",
      "epoch 778 | step 3 | loss: 0.0011749226425582212\n",
      "epoch 778 | step 4 | loss: 0.0014494437457170338\n",
      "epoch 778 | step 5 | loss: 0.0017386820499630535\n",
      "epoch 778 | step 6 | loss: 0.0020509026999683327\n",
      "epoch 778 | step 7 | loss: 0.0023466692629024706\n",
      "epoch 778 | step 8 | loss: 0.0026396324556037343\n",
      "epoch 778 | step 9 | loss: 0.0029208943446510103\n",
      "epoch 778 | step 10 | loss: 0.003190117122583126\n",
      "epoch 778 | step 11 | loss: 0.003488010396269\n",
      "epoch 779 | step 0 | loss: 0.00027849349107522225\n",
      "epoch 779 | step 1 | loss: 0.0005715656609625594\n",
      "epoch 779 | step 2 | loss: 0.0008736102464600968\n",
      "epoch 779 | step 3 | loss: 0.00117196417426652\n",
      "epoch 779 | step 4 | loss: 0.00144559510336208\n",
      "epoch 779 | step 5 | loss: 0.0017428179022757797\n",
      "epoch 779 | step 6 | loss: 0.0020338289990004714\n",
      "epoch 779 | step 7 | loss: 0.0023335764336623634\n",
      "epoch 779 | step 8 | loss: 0.002639399812655075\n",
      "epoch 779 | step 9 | loss: 0.0029193500644798384\n",
      "epoch 779 | step 10 | loss: 0.003216614565179942\n",
      "epoch 779 | step 11 | loss: 0.0034776840256916484\n",
      "epoch 780 | step 0 | loss: 0.0003163362187411938\n",
      "epoch 780 | step 1 | loss: 0.0006155321277382169\n",
      "epoch 780 | step 2 | loss: 0.0009114726122609656\n",
      "epoch 780 | step 3 | loss: 0.001194682847781447\n",
      "epoch 780 | step 4 | loss: 0.0014935358254794845\n",
      "epoch 780 | step 5 | loss: 0.0018025227206226167\n",
      "epoch 780 | step 6 | loss: 0.002074340314974192\n",
      "epoch 780 | step 7 | loss: 0.002349807845476225\n",
      "epoch 780 | step 8 | loss: 0.0026392249230030246\n",
      "epoch 780 | step 9 | loss: 0.002905922555106217\n",
      "epoch 780 | step 10 | loss: 0.003180888087110637\n",
      "epoch 780 | step 11 | loss: 0.0034913903509525947\n",
      "epoch 781 | step 0 | loss: 0.00028140975628020555\n",
      "epoch 781 | step 1 | loss: 0.0005935151369075875\n",
      "epoch 781 | step 2 | loss: 0.000879814757945817\n",
      "epoch 781 | step 3 | loss: 0.0011596051357370082\n",
      "epoch 781 | step 4 | loss: 0.0014388211048411335\n",
      "epoch 781 | step 5 | loss: 0.0017077258438091716\n",
      "epoch 781 | step 6 | loss: 0.002016131591106969\n",
      "epoch 781 | step 7 | loss: 0.0022905934076065835\n",
      "epoch 781 | step 8 | loss: 0.0025763530380720104\n",
      "epoch 781 | step 9 | loss: 0.0028745525129455895\n",
      "epoch 781 | step 10 | loss: 0.0031469794562732677\n",
      "epoch 781 | step 11 | loss: 0.0035043673066037445\n",
      "epoch 782 | step 0 | loss: 0.0002730306193327351\n",
      "epoch 782 | step 1 | loss: 0.0005850302082819135\n",
      "epoch 782 | step 2 | loss: 0.0008776037120796585\n",
      "epoch 782 | step 3 | loss: 0.0011931844556541942\n",
      "epoch 782 | step 4 | loss: 0.001453461749124902\n",
      "epoch 782 | step 5 | loss: 0.001750151433035629\n",
      "epoch 782 | step 6 | loss: 0.0020355267271984353\n",
      "epoch 782 | step 7 | loss: 0.002306540818462475\n",
      "epoch 782 | step 8 | loss: 0.0026010636656038366\n",
      "epoch 782 | step 9 | loss: 0.0029002627818443807\n",
      "epoch 782 | step 10 | loss: 0.003205999111743725\n",
      "epoch 782 | step 11 | loss: 0.003481297960318626\n",
      "epoch 783 | step 0 | loss: 0.0003201361739357951\n",
      "epoch 783 | step 1 | loss: 0.0006256691492438402\n",
      "epoch 783 | step 2 | loss: 0.0009422494060016233\n",
      "epoch 783 | step 3 | loss: 0.0012250332047043825\n",
      "epoch 783 | step 4 | loss: 0.0014886449490145205\n",
      "epoch 783 | step 5 | loss: 0.0017767329794287391\n",
      "epoch 783 | step 6 | loss: 0.002060300936476275\n",
      "epoch 783 | step 7 | loss: 0.0023447986949083543\n",
      "epoch 783 | step 8 | loss: 0.002622911570402027\n",
      "epoch 783 | step 9 | loss: 0.002875294938415092\n",
      "epoch 783 | step 10 | loss: 0.003164998112557115\n",
      "epoch 783 | step 11 | loss: 0.003497255876792219\n",
      "epoch 784 | step 0 | loss: 0.0003043755656389235\n",
      "epoch 784 | step 1 | loss: 0.0006143060159481869\n",
      "epoch 784 | step 2 | loss: 0.0008965465217608704\n",
      "epoch 784 | step 3 | loss: 0.0011942579619206156\n",
      "epoch 784 | step 4 | loss: 0.0014615790834398303\n",
      "epoch 784 | step 5 | loss: 0.0017576428727693837\n",
      "epoch 784 | step 6 | loss: 0.002023558350952321\n",
      "epoch 784 | step 7 | loss: 0.0023067220133383866\n",
      "epoch 784 | step 8 | loss: 0.002613335712648308\n",
      "epoch 784 | step 9 | loss: 0.0029093965258707886\n",
      "epoch 784 | step 10 | loss: 0.0031875923837947602\n",
      "epoch 784 | step 11 | loss: 0.003488220778967763\n",
      "epoch 785 | step 0 | loss: 0.0002840293086741229\n",
      "epoch 785 | step 1 | loss: 0.0005523318090203354\n",
      "epoch 785 | step 2 | loss: 0.0008608326898395177\n",
      "epoch 785 | step 3 | loss: 0.0011268871402938714\n",
      "epoch 785 | step 4 | loss: 0.0014145362603447125\n",
      "epoch 785 | step 5 | loss: 0.0017299734725898953\n",
      "epoch 785 | step 6 | loss: 0.002027831000446426\n",
      "epoch 785 | step 7 | loss: 0.0023085766237105633\n",
      "epoch 785 | step 8 | loss: 0.0025889225085183277\n",
      "epoch 785 | step 9 | loss: 0.002880447798462813\n",
      "epoch 785 | step 10 | loss: 0.003180550372652633\n",
      "epoch 785 | step 11 | loss: 0.003490892572400257\n",
      "epoch 786 | step 0 | loss: 0.00030440751508605153\n",
      "epoch 786 | step 1 | loss: 0.0005720718683978393\n",
      "epoch 786 | step 2 | loss: 0.000861076790787356\n",
      "epoch 786 | step 3 | loss: 0.0011743747852595895\n",
      "epoch 786 | step 4 | loss: 0.001455280782343781\n",
      "epoch 786 | step 5 | loss: 0.0017772234480976541\n",
      "epoch 786 | step 6 | loss: 0.0020798112676120644\n",
      "epoch 786 | step 7 | loss: 0.002363757108689826\n",
      "epoch 786 | step 8 | loss: 0.002621794642644305\n",
      "epoch 786 | step 9 | loss: 0.0028923138382993797\n",
      "epoch 786 | step 10 | loss: 0.003198798732669124\n",
      "epoch 786 | step 11 | loss: 0.0034835009963400136\n",
      "epoch 787 | step 0 | loss: 0.0003078721089182883\n",
      "epoch 787 | step 1 | loss: 0.000592983512880094\n",
      "epoch 787 | step 2 | loss: 0.0008674650537695012\n",
      "epoch 787 | step 3 | loss: 0.0011647235732323738\n",
      "epoch 787 | step 4 | loss: 0.001453763670484736\n",
      "epoch 787 | step 5 | loss: 0.0017425783310468907\n",
      "epoch 787 | step 6 | loss: 0.0020513178386209856\n",
      "epoch 787 | step 7 | loss: 0.0023546632012027384\n",
      "epoch 787 | step 8 | loss: 0.002638429351614361\n",
      "epoch 787 | step 9 | loss: 0.002921306029337228\n",
      "epoch 787 | step 10 | loss: 0.0031890192184130285\n",
      "epoch 787 | step 11 | loss: 0.003487539534417669\n",
      "epoch 788 | step 0 | loss: 0.0002902924824813065\n",
      "epoch 788 | step 1 | loss: 0.0005853285802058574\n",
      "epoch 788 | step 2 | loss: 0.0008881563739296984\n",
      "epoch 788 | step 3 | loss: 0.0011862209178920217\n",
      "epoch 788 | step 4 | loss: 0.0014681091560986028\n",
      "epoch 788 | step 5 | loss: 0.0017551637453870762\n",
      "epoch 788 | step 6 | loss: 0.002046227348871477\n",
      "epoch 788 | step 7 | loss: 0.002322338790551689\n",
      "epoch 788 | step 8 | loss: 0.0026214795225339266\n",
      "epoch 788 | step 9 | loss: 0.0029270822496535688\n",
      "epoch 788 | step 10 | loss: 0.0032114174746015163\n",
      "epoch 788 | step 11 | loss: 0.003478690680943558\n",
      "epoch 789 | step 0 | loss: 0.0002744169401588963\n",
      "epoch 789 | step 1 | loss: 0.0005978329715002906\n",
      "epoch 789 | step 2 | loss: 0.0008855410164967569\n",
      "epoch 789 | step 3 | loss: 0.001169139784689481\n",
      "epoch 789 | step 4 | loss: 0.0014534709855224734\n",
      "epoch 789 | step 5 | loss: 0.0017332006214874868\n",
      "epoch 789 | step 6 | loss: 0.0020126717995367785\n",
      "epoch 789 | step 7 | loss: 0.0023173546542968747\n",
      "epoch 789 | step 8 | loss: 0.0026231155081087253\n",
      "epoch 789 | step 9 | loss: 0.002907301691875189\n",
      "epoch 789 | step 10 | loss: 0.0031937346452306516\n",
      "epoch 789 | step 11 | loss: 0.003485401607928029\n",
      "epoch 790 | step 0 | loss: 0.0002841937994415125\n",
      "epoch 790 | step 1 | loss: 0.0005423112603579361\n",
      "epoch 790 | step 2 | loss: 0.0008330607189719015\n",
      "epoch 790 | step 3 | loss: 0.0011280423404591046\n",
      "epoch 790 | step 4 | loss: 0.001430114686696992\n",
      "epoch 790 | step 5 | loss: 0.0017163058445791618\n",
      "epoch 790 | step 6 | loss: 0.0020218154042302176\n",
      "epoch 790 | step 7 | loss: 0.0023169260667242546\n",
      "epoch 790 | step 8 | loss: 0.002597576540462916\n",
      "epoch 790 | step 9 | loss: 0.002877679105532888\n",
      "epoch 790 | step 10 | loss: 0.0031637799302516945\n",
      "epoch 790 | step 11 | loss: 0.003497174926852681\n",
      "epoch 791 | step 0 | loss: 0.0002900609223297658\n",
      "epoch 791 | step 1 | loss: 0.0005614036400642088\n",
      "epoch 791 | step 2 | loss: 0.0008594096412571894\n",
      "epoch 791 | step 3 | loss: 0.001139809800149202\n",
      "epoch 791 | step 4 | loss: 0.0014269521701379237\n",
      "epoch 791 | step 5 | loss: 0.0016851706251898295\n",
      "epoch 791 | step 6 | loss: 0.001980253075649109\n",
      "epoch 791 | step 7 | loss: 0.002274899018435483\n",
      "epoch 791 | step 8 | loss: 0.002583598212528355\n",
      "epoch 791 | step 9 | loss: 0.0028645977496213914\n",
      "epoch 791 | step 10 | loss: 0.003187812495697279\n",
      "epoch 791 | step 11 | loss: 0.003487500256078273\n",
      "epoch 792 | step 0 | loss: 0.0003024574889655369\n",
      "epoch 792 | step 1 | loss: 0.000610053102128491\n",
      "epoch 792 | step 2 | loss: 0.000909623341185404\n",
      "epoch 792 | step 3 | loss: 0.0012002141830979643\n",
      "epoch 792 | step 4 | loss: 0.001516828198906819\n",
      "epoch 792 | step 5 | loss: 0.001810349850327741\n",
      "epoch 792 | step 6 | loss: 0.0021020460994089995\n",
      "epoch 792 | step 7 | loss: 0.002361267269664951\n",
      "epoch 792 | step 8 | loss: 0.0026510339889884446\n",
      "epoch 792 | step 9 | loss: 0.0029240933641791313\n",
      "epoch 792 | step 10 | loss: 0.0032073561094517105\n",
      "epoch 792 | step 11 | loss: 0.003480079141582941\n",
      "epoch 793 | step 0 | loss: 0.00028716443692886263\n",
      "epoch 793 | step 1 | loss: 0.0005610211772215232\n",
      "epoch 793 | step 2 | loss: 0.0008535638876027622\n",
      "epoch 793 | step 3 | loss: 0.0011275390709980294\n",
      "epoch 793 | step 4 | loss: 0.0014010412089065117\n",
      "epoch 793 | step 5 | loss: 0.0017016388039236233\n",
      "epoch 793 | step 6 | loss: 0.001995184329625802\n",
      "epoch 793 | step 7 | loss: 0.0023096259017181134\n",
      "epoch 793 | step 8 | loss: 0.0026263142127136738\n",
      "epoch 793 | step 9 | loss: 0.0029033194817239963\n",
      "epoch 793 | step 10 | loss: 0.0031794643787923225\n",
      "epoch 793 | step 11 | loss: 0.0034904085697637496\n",
      "epoch 794 | step 0 | loss: 0.0002646422950516924\n",
      "epoch 794 | step 1 | loss: 0.0005712502056922787\n",
      "epoch 794 | step 2 | loss: 0.0008855926588169701\n",
      "epoch 794 | step 3 | loss: 0.0011781562458276832\n",
      "epoch 794 | step 4 | loss: 0.001468392032237491\n",
      "epoch 794 | step 5 | loss: 0.0017784703130196762\n",
      "epoch 794 | step 6 | loss: 0.002064171775882667\n",
      "epoch 794 | step 7 | loss: 0.0023310341535516038\n",
      "epoch 794 | step 8 | loss: 0.0026236725911291197\n",
      "epoch 794 | step 9 | loss: 0.0028963676135080416\n",
      "epoch 794 | step 10 | loss: 0.0032134908480148474\n",
      "epoch 794 | step 11 | loss: 0.0034769548000896885\n",
      "epoch 795 | step 0 | loss: 0.0003158563285677908\n",
      "epoch 795 | step 1 | loss: 0.000611692985539069\n",
      "epoch 795 | step 2 | loss: 0.0008748627733316113\n",
      "epoch 795 | step 3 | loss: 0.001152202927325939\n",
      "epoch 795 | step 4 | loss: 0.0014674434898284943\n",
      "epoch 795 | step 5 | loss: 0.001777010615858774\n",
      "epoch 795 | step 6 | loss: 0.0020910184135175527\n",
      "epoch 795 | step 7 | loss: 0.0023567719495651823\n",
      "epoch 795 | step 8 | loss: 0.0026595968674143393\n",
      "epoch 795 | step 9 | loss: 0.002919725110202353\n",
      "epoch 795 | step 10 | loss: 0.003185275779593366\n",
      "epoch 795 | step 11 | loss: 0.003487894944450861\n",
      "epoch 796 | step 0 | loss: 0.0002908695877576556\n",
      "epoch 796 | step 1 | loss: 0.0005908817282178945\n",
      "epoch 796 | step 2 | loss: 0.000883280856669964\n",
      "epoch 796 | step 3 | loss: 0.0011734364331862433\n",
      "epoch 796 | step 4 | loss: 0.001460127436338211\n",
      "epoch 796 | step 5 | loss: 0.0017413252647629908\n",
      "epoch 796 | step 6 | loss: 0.002038565848399826\n",
      "epoch 796 | step 7 | loss: 0.0023217443257442676\n",
      "epoch 796 | step 8 | loss: 0.0025999742266908953\n",
      "epoch 796 | step 9 | loss: 0.0028908332780739434\n",
      "epoch 796 | step 10 | loss: 0.003185489921473268\n",
      "epoch 796 | step 11 | loss: 0.003488026601853554\n",
      "epoch 797 | step 0 | loss: 0.0002742644411916716\n",
      "epoch 797 | step 1 | loss: 0.0005681100583885505\n",
      "epoch 797 | step 2 | loss: 0.0008911096166447193\n",
      "epoch 797 | step 3 | loss: 0.0011679189712371664\n",
      "epoch 797 | step 4 | loss: 0.0014733612054096152\n",
      "epoch 797 | step 5 | loss: 0.0017821174070702752\n",
      "epoch 797 | step 6 | loss: 0.0020925439087541705\n",
      "epoch 797 | step 7 | loss: 0.0023756188097752815\n",
      "epoch 797 | step 8 | loss: 0.0026313237934605035\n",
      "epoch 797 | step 9 | loss: 0.002931656740068696\n",
      "epoch 797 | step 10 | loss: 0.003212368266264086\n",
      "epoch 797 | step 11 | loss: 0.003477068639728808\n",
      "epoch 798 | step 0 | loss: 0.0003168516659120068\n",
      "epoch 798 | step 1 | loss: 0.0005845799548547883\n",
      "epoch 798 | step 2 | loss: 0.0009061235559598036\n",
      "epoch 798 | step 3 | loss: 0.001200707871198524\n",
      "epoch 798 | step 4 | loss: 0.0014989718687243368\n",
      "epoch 798 | step 5 | loss: 0.0017924510851510683\n",
      "epoch 798 | step 6 | loss: 0.002076480932537584\n",
      "epoch 798 | step 7 | loss: 0.002352656264703099\n",
      "epoch 798 | step 8 | loss: 0.0026110355780345896\n",
      "epoch 798 | step 9 | loss: 0.0028948209898436925\n",
      "epoch 798 | step 10 | loss: 0.0032085720309525416\n",
      "epoch 798 | step 11 | loss: 0.0034787113295332356\n",
      "epoch 799 | step 0 | loss: 0.0002846787311799287\n",
      "epoch 799 | step 1 | loss: 0.0005785959699646553\n",
      "epoch 799 | step 2 | loss: 0.0008588136566439867\n",
      "epoch 799 | step 3 | loss: 0.0011425582245314571\n",
      "epoch 799 | step 4 | loss: 0.001442577821726946\n",
      "epoch 799 | step 5 | loss: 0.0017352410865228538\n",
      "epoch 799 | step 6 | loss: 0.0020096064991115803\n",
      "epoch 799 | step 7 | loss: 0.002292268861804777\n",
      "epoch 799 | step 8 | loss: 0.00261094289646092\n",
      "epoch 799 | step 9 | loss: 0.002885036495641466\n",
      "epoch 799 | step 10 | loss: 0.003172708485690284\n",
      "epoch 799 | step 11 | loss: 0.0034926011526211454\n",
      "epoch 800 | step 0 | loss: 0.0002709787020544162\n",
      "epoch 800 | step 1 | loss: 0.0005454569766972362\n",
      "epoch 800 | step 2 | loss: 0.0008489617311114153\n",
      "epoch 800 | step 3 | loss: 0.0011257296117259473\n",
      "epoch 800 | step 4 | loss: 0.0014219021656410092\n",
      "epoch 800 | step 5 | loss: 0.0017172757760917329\n",
      "epoch 800 | step 6 | loss: 0.0020097780765680057\n",
      "epoch 800 | step 7 | loss: 0.0022947660828774707\n",
      "epoch 800 | step 8 | loss: 0.0025857032197481858\n",
      "epoch 800 | step 9 | loss: 0.002893946310269425\n",
      "epoch 800 | step 10 | loss: 0.0032114243297294864\n",
      "epoch 800 | step 11 | loss: 0.0034772037575958757\n",
      "epoch 801 | step 0 | loss: 0.00028931061628770856\n",
      "epoch 801 | step 1 | loss: 0.0005680020989354675\n",
      "epoch 801 | step 2 | loss: 0.0008667201622233877\n",
      "epoch 801 | step 3 | loss: 0.0011443539712395006\n",
      "epoch 801 | step 4 | loss: 0.0014549524908668387\n",
      "epoch 801 | step 5 | loss: 0.0017334642321765255\n",
      "epoch 801 | step 6 | loss: 0.0020152345330844987\n",
      "epoch 801 | step 7 | loss: 0.0022738788043452727\n",
      "epoch 801 | step 8 | loss: 0.002566358764292676\n",
      "epoch 801 | step 9 | loss: 0.0028580481371003057\n",
      "epoch 801 | step 10 | loss: 0.003158253485515706\n",
      "epoch 801 | step 11 | loss: 0.003497744436132737\n",
      "epoch 802 | step 0 | loss: 0.000296483337375371\n",
      "epoch 802 | step 1 | loss: 0.0005808532323732998\n",
      "epoch 802 | step 2 | loss: 0.0008714157335638892\n",
      "epoch 802 | step 3 | loss: 0.0011631021743076012\n",
      "epoch 802 | step 4 | loss: 0.0014347554357126574\n",
      "epoch 802 | step 5 | loss: 0.0017086034042158645\n",
      "epoch 802 | step 6 | loss: 0.0019956259584262745\n",
      "epoch 802 | step 7 | loss: 0.002286479346443978\n",
      "epoch 802 | step 8 | loss: 0.0025824590213951236\n",
      "epoch 802 | step 9 | loss: 0.0028731004756337684\n",
      "epoch 802 | step 10 | loss: 0.00317134659363705\n",
      "epoch 802 | step 11 | loss: 0.003492440390764166\n",
      "epoch 803 | step 0 | loss: 0.00030776121594442427\n",
      "epoch 803 | step 1 | loss: 0.0006043966756721995\n",
      "epoch 803 | step 2 | loss: 0.000891167252951079\n",
      "epoch 803 | step 3 | loss: 0.001175064244383461\n",
      "epoch 803 | step 4 | loss: 0.001460048875733189\n",
      "epoch 803 | step 5 | loss: 0.0017379507787425822\n",
      "epoch 803 | step 6 | loss: 0.0020544177537545156\n",
      "epoch 803 | step 7 | loss: 0.002316924906110567\n",
      "epoch 803 | step 8 | loss: 0.0025925157335294673\n",
      "epoch 803 | step 9 | loss: 0.002906742113581581\n",
      "epoch 803 | step 10 | loss: 0.0031819512339698796\n",
      "epoch 803 | step 11 | loss: 0.003488254358678286\n",
      "epoch 804 | step 0 | loss: 0.00027240619703730346\n",
      "epoch 804 | step 1 | loss: 0.0005507405360078212\n",
      "epoch 804 | step 2 | loss: 0.0008531974229450776\n",
      "epoch 804 | step 3 | loss: 0.001138042720548272\n",
      "epoch 804 | step 4 | loss: 0.0014192692201773923\n",
      "epoch 804 | step 5 | loss: 0.0016896840232888594\n",
      "epoch 804 | step 6 | loss: 0.0019837211984501087\n",
      "epoch 804 | step 7 | loss: 0.0022955301537438376\n",
      "epoch 804 | step 8 | loss: 0.00258180615304409\n",
      "epoch 804 | step 9 | loss: 0.0028770287934638287\n",
      "epoch 804 | step 10 | loss: 0.0031676468644950633\n",
      "epoch 804 | step 11 | loss: 0.0034939864252077984\n",
      "epoch 805 | step 0 | loss: 0.0002855387753832858\n",
      "epoch 805 | step 1 | loss: 0.0005923470329971291\n",
      "epoch 805 | step 2 | loss: 0.0008766597852439761\n",
      "epoch 805 | step 3 | loss: 0.0011714959808637783\n",
      "epoch 805 | step 4 | loss: 0.0014499585603187238\n",
      "epoch 805 | step 5 | loss: 0.001743666034085757\n",
      "epoch 805 | step 6 | loss: 0.0020064026564068005\n",
      "epoch 805 | step 7 | loss: 0.002294511329115036\n",
      "epoch 805 | step 8 | loss: 0.002567926629276432\n",
      "epoch 805 | step 9 | loss: 0.0028627251106203137\n",
      "epoch 805 | step 10 | loss: 0.003176382235553659\n",
      "epoch 805 | step 11 | loss: 0.0034909468404878976\n",
      "epoch 806 | step 0 | loss: 0.0002924123436874164\n",
      "epoch 806 | step 1 | loss: 0.0005898545198737658\n",
      "epoch 806 | step 2 | loss: 0.0008848574084331752\n",
      "epoch 806 | step 3 | loss: 0.0011954007188346492\n",
      "epoch 806 | step 4 | loss: 0.0014748305629417964\n",
      "epoch 806 | step 5 | loss: 0.0017691106407764202\n",
      "epoch 806 | step 6 | loss: 0.0020331926168935845\n",
      "epoch 806 | step 7 | loss: 0.0023121171977623376\n",
      "epoch 806 | step 8 | loss: 0.0026091934649552203\n",
      "epoch 806 | step 9 | loss: 0.0028951899864681713\n",
      "epoch 806 | step 10 | loss: 0.0032021665472082494\n",
      "epoch 806 | step 11 | loss: 0.0034803202387146655\n",
      "epoch 807 | step 0 | loss: 0.00031103559026753876\n",
      "epoch 807 | step 1 | loss: 0.0005907349120754548\n",
      "epoch 807 | step 2 | loss: 0.0008709383180331928\n",
      "epoch 807 | step 3 | loss: 0.001162742486676469\n",
      "epoch 807 | step 4 | loss: 0.0014524785469932932\n",
      "epoch 807 | step 5 | loss: 0.0017273115025786146\n",
      "epoch 807 | step 6 | loss: 0.0020181562519810325\n",
      "epoch 807 | step 7 | loss: 0.0023027063510153295\n",
      "epoch 807 | step 8 | loss: 0.0025908275229809367\n",
      "epoch 807 | step 9 | loss: 0.002890766167301704\n",
      "epoch 807 | step 10 | loss: 0.0031921188772467655\n",
      "epoch 807 | step 11 | loss: 0.0034841080328710397\n",
      "epoch 808 | step 0 | loss: 0.0002720080285707891\n",
      "epoch 808 | step 1 | loss: 0.0005682269090112764\n",
      "epoch 808 | step 2 | loss: 0.000867390018145239\n",
      "epoch 808 | step 3 | loss: 0.0011783603555306854\n",
      "epoch 808 | step 4 | loss: 0.001467207395300206\n",
      "epoch 808 | step 5 | loss: 0.0017627734317843288\n",
      "epoch 808 | step 6 | loss: 0.002020696097164608\n",
      "epoch 808 | step 7 | loss: 0.002317960471760391\n",
      "epoch 808 | step 8 | loss: 0.0026163664769815725\n",
      "epoch 808 | step 9 | loss: 0.002940432885486908\n",
      "epoch 808 | step 10 | loss: 0.0032104735762752594\n",
      "epoch 808 | step 11 | loss: 0.0034770617911580757\n",
      "epoch 809 | step 0 | loss: 0.0002919786630716962\n",
      "epoch 809 | step 1 | loss: 0.0006207003025913314\n",
      "epoch 809 | step 2 | loss: 0.0008868004923816985\n",
      "epoch 809 | step 3 | loss: 0.0011697335343368858\n",
      "epoch 809 | step 4 | loss: 0.001452181235217923\n",
      "epoch 809 | step 5 | loss: 0.001755321767745013\n",
      "epoch 809 | step 6 | loss: 0.002069191391622141\n",
      "epoch 809 | step 7 | loss: 0.0023444826778180232\n",
      "epoch 809 | step 8 | loss: 0.002635875411101268\n",
      "epoch 809 | step 9 | loss: 0.002911486168319038\n",
      "epoch 809 | step 10 | loss: 0.0032015584151940495\n",
      "epoch 809 | step 11 | loss: 0.003480164205259301\n",
      "epoch 810 | step 0 | loss: 0.00028144032381153064\n",
      "epoch 810 | step 1 | loss: 0.0005492409004779333\n",
      "epoch 810 | step 2 | loss: 0.0008473787155594768\n",
      "epoch 810 | step 3 | loss: 0.0011609359699511438\n",
      "epoch 810 | step 4 | loss: 0.001435070084815262\n",
      "epoch 810 | step 5 | loss: 0.0017287497150724738\n",
      "epoch 810 | step 6 | loss: 0.002007975302446441\n",
      "epoch 810 | step 7 | loss: 0.002295878596954048\n",
      "epoch 810 | step 8 | loss: 0.0025846839672835653\n",
      "epoch 810 | step 9 | loss: 0.0028668541993315346\n",
      "epoch 810 | step 10 | loss: 0.0031801847355256265\n",
      "epoch 810 | step 11 | loss: 0.003488252557869712\n",
      "epoch 811 | step 0 | loss: 0.0002878723313090526\n",
      "epoch 811 | step 1 | loss: 0.0005645873866994012\n",
      "epoch 811 | step 2 | loss: 0.000859305838355624\n",
      "epoch 811 | step 3 | loss: 0.001171671994140933\n",
      "epoch 811 | step 4 | loss: 0.0014723084081720603\n",
      "epoch 811 | step 5 | loss: 0.001721833437636089\n",
      "epoch 811 | step 6 | loss: 0.0020059637365436958\n",
      "epoch 811 | step 7 | loss: 0.0022837033240036913\n",
      "epoch 811 | step 8 | loss: 0.002600131157607571\n",
      "epoch 811 | step 9 | loss: 0.0028909746814184015\n",
      "epoch 811 | step 10 | loss: 0.003168901395195716\n",
      "epoch 811 | step 11 | loss: 0.0034927554182848605\n",
      "epoch 812 | step 0 | loss: 0.0002851434880823915\n",
      "epoch 812 | step 1 | loss: 0.0005467887193035165\n",
      "epoch 812 | step 2 | loss: 0.0008604027938654191\n",
      "epoch 812 | step 3 | loss: 0.001162316107345447\n",
      "epoch 812 | step 4 | loss: 0.0014497651779669808\n",
      "epoch 812 | step 5 | loss: 0.0017370691024871266\n",
      "epoch 812 | step 6 | loss: 0.0020397765452263954\n",
      "epoch 812 | step 7 | loss: 0.002343918886450418\n",
      "epoch 812 | step 8 | loss: 0.002631416225504951\n",
      "epoch 812 | step 9 | loss: 0.002900345579698977\n",
      "epoch 812 | step 10 | loss: 0.003187311969664311\n",
      "epoch 812 | step 11 | loss: 0.003485280825152244\n",
      "epoch 813 | step 0 | loss: 0.0002637458623394326\n",
      "epoch 813 | step 1 | loss: 0.000584913400744827\n",
      "epoch 813 | step 2 | loss: 0.0009105903895779805\n",
      "epoch 813 | step 3 | loss: 0.0012025770987451367\n",
      "epoch 813 | step 4 | loss: 0.0014825829644935806\n",
      "epoch 813 | step 5 | loss: 0.0017581160880421412\n",
      "epoch 813 | step 6 | loss: 0.0020815245338846587\n",
      "epoch 813 | step 7 | loss: 0.002360486582372651\n",
      "epoch 813 | step 8 | loss: 0.0026473858005384267\n",
      "epoch 813 | step 9 | loss: 0.0029286941710270986\n",
      "epoch 813 | step 10 | loss: 0.0032088278330690253\n",
      "epoch 813 | step 11 | loss: 0.0034767775333962034\n",
      "epoch 814 | step 0 | loss: 0.000279830322418989\n",
      "epoch 814 | step 1 | loss: 0.0005259294076929864\n",
      "epoch 814 | step 2 | loss: 0.0008308190614144562\n",
      "epoch 814 | step 3 | loss: 0.0011189411450102298\n",
      "epoch 814 | step 4 | loss: 0.0014341892348532638\n",
      "epoch 814 | step 5 | loss: 0.001717042217188432\n",
      "epoch 814 | step 6 | loss: 0.0020054717705333294\n",
      "epoch 814 | step 7 | loss: 0.00230196162498516\n",
      "epoch 814 | step 8 | loss: 0.002597347327295228\n",
      "epoch 814 | step 9 | loss: 0.002888051643547202\n",
      "epoch 814 | step 10 | loss: 0.0032031201675675283\n",
      "epoch 814 | step 11 | loss: 0.0034786673163589056\n",
      "epoch 815 | step 0 | loss: 0.0002912164804886397\n",
      "epoch 815 | step 1 | loss: 0.00059625011716291\n",
      "epoch 815 | step 2 | loss: 0.0008710948203125377\n",
      "epoch 815 | step 3 | loss: 0.0011658086602583952\n",
      "epoch 815 | step 4 | loss: 0.0014629007536725556\n",
      "epoch 815 | step 5 | loss: 0.0017842084875751042\n",
      "epoch 815 | step 6 | loss: 0.0020576317955664676\n",
      "epoch 815 | step 7 | loss: 0.002330662888138051\n",
      "epoch 815 | step 8 | loss: 0.0026040631284145\n",
      "epoch 815 | step 9 | loss: 0.0029030525849082626\n",
      "epoch 815 | step 10 | loss: 0.0032010789917384123\n",
      "epoch 815 | step 11 | loss: 0.0034796403262051918\n",
      "epoch 816 | step 0 | loss: 0.00027383547517614734\n",
      "epoch 816 | step 1 | loss: 0.0005676333464238375\n",
      "epoch 816 | step 2 | loss: 0.000859359271024526\n",
      "epoch 816 | step 3 | loss: 0.0011302418669665096\n",
      "epoch 816 | step 4 | loss: 0.0014345071555777248\n",
      "epoch 816 | step 5 | loss: 0.001714086377004324\n",
      "epoch 816 | step 6 | loss: 0.002013800303926058\n",
      "epoch 816 | step 7 | loss: 0.0023274400690539787\n",
      "epoch 816 | step 8 | loss: 0.0026381314687228905\n",
      "epoch 816 | step 9 | loss: 0.002914303078784251\n",
      "epoch 816 | step 10 | loss: 0.0032036164532123533\n",
      "epoch 816 | step 11 | loss: 0.0034782859025552157\n",
      "epoch 817 | step 0 | loss: 0.0002881538108771977\n",
      "epoch 817 | step 1 | loss: 0.0005677095663305361\n",
      "epoch 817 | step 2 | loss: 0.0008630374059140622\n",
      "epoch 817 | step 3 | loss: 0.0011422252604631165\n",
      "epoch 817 | step 4 | loss: 0.0014223054203877147\n",
      "epoch 817 | step 5 | loss: 0.0017084207512970014\n",
      "epoch 817 | step 6 | loss: 0.002027198406199449\n",
      "epoch 817 | step 7 | loss: 0.002318682052896557\n",
      "epoch 817 | step 8 | loss: 0.0025727123404700233\n",
      "epoch 817 | step 9 | loss: 0.002891622287496666\n",
      "epoch 817 | step 10 | loss: 0.0031888894714879094\n",
      "epoch 817 | step 11 | loss: 0.0034843819140949308\n",
      "epoch 818 | step 0 | loss: 0.00028412126624725105\n",
      "epoch 818 | step 1 | loss: 0.0005724885834195771\n",
      "epoch 818 | step 2 | loss: 0.000849184244387244\n",
      "epoch 818 | step 3 | loss: 0.0011551995225826475\n",
      "epoch 818 | step 4 | loss: 0.001426502513697606\n",
      "epoch 818 | step 5 | loss: 0.0017232068953997794\n",
      "epoch 818 | step 6 | loss: 0.001999169190814223\n",
      "epoch 818 | step 7 | loss: 0.002273239798310517\n",
      "epoch 818 | step 8 | loss: 0.002579587694821311\n",
      "epoch 818 | step 9 | loss: 0.002904851548673856\n",
      "epoch 818 | step 10 | loss: 0.0031985014809500665\n",
      "epoch 818 | step 11 | loss: 0.003480326090377126\n",
      "epoch 819 | step 0 | loss: 0.0002837708929055955\n",
      "epoch 819 | step 1 | loss: 0.0005752723829335739\n",
      "epoch 819 | step 2 | loss: 0.0008807525740461026\n",
      "epoch 819 | step 3 | loss: 0.0011681321899063835\n",
      "epoch 819 | step 4 | loss: 0.00145923875672625\n",
      "epoch 819 | step 5 | loss: 0.0017740481861943807\n",
      "epoch 819 | step 6 | loss: 0.0020703413440640926\n",
      "epoch 819 | step 7 | loss: 0.002374842943970914\n",
      "epoch 819 | step 8 | loss: 0.0026447855809956103\n",
      "epoch 819 | step 9 | loss: 0.0029116047643614113\n",
      "epoch 819 | step 10 | loss: 0.0032006597449643582\n",
      "epoch 819 | step 11 | loss: 0.003479532613229963\n",
      "epoch 820 | step 0 | loss: 0.00029952589510560447\n",
      "epoch 820 | step 1 | loss: 0.0005850192174626657\n",
      "epoch 820 | step 2 | loss: 0.0008875493430469684\n",
      "epoch 820 | step 3 | loss: 0.0011526062377673195\n",
      "epoch 820 | step 4 | loss: 0.0014505879585655823\n",
      "epoch 820 | step 5 | loss: 0.001726473502420987\n",
      "epoch 820 | step 6 | loss: 0.0020110182081480824\n",
      "epoch 820 | step 7 | loss: 0.0023025517169313714\n",
      "epoch 820 | step 8 | loss: 0.0026183322809310862\n",
      "epoch 820 | step 9 | loss: 0.002884830702094274\n",
      "epoch 820 | step 10 | loss: 0.003188268980394283\n",
      "epoch 820 | step 11 | loss: 0.0034841366825394933\n",
      "epoch 821 | step 0 | loss: 0.00027929169696871005\n",
      "epoch 821 | step 1 | loss: 0.0005839453927951887\n",
      "epoch 821 | step 2 | loss: 0.0008658887342158833\n",
      "epoch 821 | step 3 | loss: 0.0011645283061697768\n",
      "epoch 821 | step 4 | loss: 0.0014851235980236515\n",
      "epoch 821 | step 5 | loss: 0.0017702007514780525\n",
      "epoch 821 | step 6 | loss: 0.002065452553403665\n",
      "epoch 821 | step 7 | loss: 0.002365853672614465\n",
      "epoch 821 | step 8 | loss: 0.0026316048996119527\n",
      "epoch 821 | step 9 | loss: 0.0028970638199698417\n",
      "epoch 821 | step 10 | loss: 0.0032001495738687227\n",
      "epoch 821 | step 11 | loss: 0.003479103538070213\n",
      "epoch 822 | step 0 | loss: 0.0002516416610084652\n",
      "epoch 822 | step 1 | loss: 0.0005487940554174571\n",
      "epoch 822 | step 2 | loss: 0.0008573505119383384\n",
      "epoch 822 | step 3 | loss: 0.001138052806327374\n",
      "epoch 822 | step 4 | loss: 0.001438175477708574\n",
      "epoch 822 | step 5 | loss: 0.001752802448113127\n",
      "epoch 822 | step 6 | loss: 0.002054613352251993\n",
      "epoch 822 | step 7 | loss: 0.002327046661800516\n",
      "epoch 822 | step 8 | loss: 0.002640631768788337\n",
      "epoch 822 | step 9 | loss: 0.0029183789841908453\n",
      "epoch 822 | step 10 | loss: 0.0032019677851675664\n",
      "epoch 822 | step 11 | loss: 0.003478319815256338\n",
      "epoch 823 | step 0 | loss: 0.0002948333001073123\n",
      "epoch 823 | step 1 | loss: 0.000579200910688554\n",
      "epoch 823 | step 2 | loss: 0.0008537392100908478\n",
      "epoch 823 | step 3 | loss: 0.0011681575804076351\n",
      "epoch 823 | step 4 | loss: 0.0014308958121525017\n",
      "epoch 823 | step 5 | loss: 0.0017113402343917905\n",
      "epoch 823 | step 6 | loss: 0.002014928952103449\n",
      "epoch 823 | step 7 | loss: 0.002293876364171944\n",
      "epoch 823 | step 8 | loss: 0.0025857394535313194\n",
      "epoch 823 | step 9 | loss: 0.0028857621408564954\n",
      "epoch 823 | step 10 | loss: 0.003193183520427866\n",
      "epoch 823 | step 11 | loss: 0.0034819881578639854\n",
      "epoch 824 | step 0 | loss: 0.00028211664412461054\n",
      "epoch 824 | step 1 | loss: 0.0005659440296345508\n",
      "epoch 824 | step 2 | loss: 0.0008520398988616365\n",
      "epoch 824 | step 3 | loss: 0.0011771634676925775\n",
      "epoch 824 | step 4 | loss: 0.0014857326947131828\n",
      "epoch 824 | step 5 | loss: 0.0017876402778320322\n",
      "epoch 824 | step 6 | loss: 0.0020708122736145087\n",
      "epoch 824 | step 7 | loss: 0.0023554920304188504\n",
      "epoch 824 | step 8 | loss: 0.0026376190714975595\n",
      "epoch 824 | step 9 | loss: 0.002919380051252258\n",
      "epoch 824 | step 10 | loss: 0.003213780665726909\n",
      "epoch 824 | step 11 | loss: 0.0034739201713490436\n",
      "epoch 825 | step 0 | loss: 0.0002874385669183842\n",
      "epoch 825 | step 1 | loss: 0.0005357549199080776\n",
      "epoch 825 | step 2 | loss: 0.0008168808211407158\n",
      "epoch 825 | step 3 | loss: 0.0011095973472023844\n",
      "epoch 825 | step 4 | loss: 0.0014014893359609953\n",
      "epoch 825 | step 5 | loss: 0.0016855313929119496\n",
      "epoch 825 | step 6 | loss: 0.0019865109102363823\n",
      "epoch 825 | step 7 | loss: 0.002263297974242064\n",
      "epoch 825 | step 8 | loss: 0.002550767287923548\n",
      "epoch 825 | step 9 | loss: 0.0028692167844888697\n",
      "epoch 825 | step 10 | loss: 0.00318007981890649\n",
      "epoch 825 | step 11 | loss: 0.0034864908956714186\n",
      "epoch 826 | step 0 | loss: 0.0003212400716967736\n",
      "epoch 826 | step 1 | loss: 0.0005926748533571775\n",
      "epoch 826 | step 2 | loss: 0.0008789721796743533\n",
      "epoch 826 | step 3 | loss: 0.0011452158381804016\n",
      "epoch 826 | step 4 | loss: 0.001409653276240504\n",
      "epoch 826 | step 5 | loss: 0.0016900385296077596\n",
      "epoch 826 | step 6 | loss: 0.0019981901455068175\n",
      "epoch 826 | step 7 | loss: 0.0022868533711490674\n",
      "epoch 826 | step 8 | loss: 0.002604029583541784\n",
      "epoch 826 | step 9 | loss: 0.0028941284676457725\n",
      "epoch 826 | step 10 | loss: 0.0031963568870036216\n",
      "epoch 826 | step 11 | loss: 0.003480138458851016\n",
      "epoch 827 | step 0 | loss: 0.00029602161699155327\n",
      "epoch 827 | step 1 | loss: 0.0005797510201811063\n",
      "epoch 827 | step 2 | loss: 0.000855743596949211\n",
      "epoch 827 | step 3 | loss: 0.0011352590666083634\n",
      "epoch 827 | step 4 | loss: 0.0014304646567370374\n",
      "epoch 827 | step 5 | loss: 0.0017218229113195875\n",
      "epoch 827 | step 6 | loss: 0.0020275931127242677\n",
      "epoch 827 | step 7 | loss: 0.0023401309057279863\n",
      "epoch 827 | step 8 | loss: 0.0026217743043807926\n",
      "epoch 827 | step 9 | loss: 0.0028830300740030576\n",
      "epoch 827 | step 10 | loss: 0.0031809353891860174\n",
      "epoch 827 | step 11 | loss: 0.0034860854827044622\n",
      "epoch 828 | step 0 | loss: 0.00029054504167304177\n",
      "epoch 828 | step 1 | loss: 0.0005952036082087832\n",
      "epoch 828 | step 2 | loss: 0.0008822185393884201\n",
      "epoch 828 | step 3 | loss: 0.0011664397049072673\n",
      "epoch 828 | step 4 | loss: 0.0014689887869995953\n",
      "epoch 828 | step 5 | loss: 0.00175710535840834\n",
      "epoch 828 | step 6 | loss: 0.002022660032731824\n",
      "epoch 828 | step 7 | loss: 0.002313093252398491\n",
      "epoch 828 | step 8 | loss: 0.002602299134567603\n",
      "epoch 828 | step 9 | loss: 0.002887869441472751\n",
      "epoch 828 | step 10 | loss: 0.0031839413668962604\n",
      "epoch 828 | step 11 | loss: 0.0034848231668911578\n",
      "epoch 829 | step 0 | loss: 0.0003056589402206592\n",
      "epoch 829 | step 1 | loss: 0.0006087768970113044\n",
      "epoch 829 | step 2 | loss: 0.0009123948730154253\n",
      "epoch 829 | step 3 | loss: 0.0011803954869634742\n",
      "epoch 829 | step 4 | loss: 0.0014583480936999635\n",
      "epoch 829 | step 5 | loss: 0.0017494997729676012\n",
      "epoch 829 | step 6 | loss: 0.0020324442783181294\n",
      "epoch 829 | step 7 | loss: 0.00231114701386794\n",
      "epoch 829 | step 8 | loss: 0.002638438789466045\n",
      "epoch 829 | step 9 | loss: 0.002924153161429378\n",
      "epoch 829 | step 10 | loss: 0.003216075574605126\n",
      "epoch 829 | step 11 | loss: 0.003472071637918881\n",
      "epoch 830 | step 0 | loss: 0.0002953919918703066\n",
      "epoch 830 | step 1 | loss: 0.0005929739500748857\n",
      "epoch 830 | step 2 | loss: 0.0008841873109107217\n",
      "epoch 830 | step 3 | loss: 0.001183591778162367\n",
      "epoch 830 | step 4 | loss: 0.001487093334560578\n",
      "epoch 830 | step 5 | loss: 0.0017735134343395433\n",
      "epoch 830 | step 6 | loss: 0.0020412404237387543\n",
      "epoch 830 | step 7 | loss: 0.002351552831439454\n",
      "epoch 830 | step 8 | loss: 0.0026417429216733924\n",
      "epoch 830 | step 9 | loss: 0.002921285251486512\n",
      "epoch 830 | step 10 | loss: 0.003193197740972793\n",
      "epoch 830 | step 11 | loss: 0.0034809929999555225\n",
      "epoch 831 | step 0 | loss: 0.0002932404672522951\n",
      "epoch 831 | step 1 | loss: 0.0005503160361849429\n",
      "epoch 831 | step 2 | loss: 0.0008388722350358279\n",
      "epoch 831 | step 3 | loss: 0.0011252509270977616\n",
      "epoch 831 | step 4 | loss: 0.001391279899173739\n",
      "epoch 831 | step 5 | loss: 0.0016721532646545062\n",
      "epoch 831 | step 6 | loss: 0.001963908350211776\n",
      "epoch 831 | step 7 | loss: 0.0022642682354388636\n",
      "epoch 831 | step 8 | loss: 0.002585621750495571\n",
      "epoch 831 | step 9 | loss: 0.0028711341160735046\n",
      "epoch 831 | step 10 | loss: 0.003169263783779888\n",
      "epoch 831 | step 11 | loss: 0.00349017478114928\n",
      "epoch 832 | step 0 | loss: 0.00028521284502892164\n",
      "epoch 832 | step 1 | loss: 0.0005708733818284612\n",
      "epoch 832 | step 2 | loss: 0.0008714289111288601\n",
      "epoch 832 | step 3 | loss: 0.0011800765588841021\n",
      "epoch 832 | step 4 | loss: 0.0014863148841298165\n",
      "epoch 832 | step 5 | loss: 0.001761546068284677\n",
      "epoch 832 | step 6 | loss: 0.0020434465126592275\n",
      "epoch 832 | step 7 | loss: 0.002348474401976713\n",
      "epoch 832 | step 8 | loss: 0.002636619417274827\n",
      "epoch 832 | step 9 | loss: 0.0029105542538932893\n",
      "epoch 832 | step 10 | loss: 0.003206331613155187\n",
      "epoch 832 | step 11 | loss: 0.0034757601229556873\n",
      "epoch 833 | step 0 | loss: 0.000314214508929629\n",
      "epoch 833 | step 1 | loss: 0.0005988200855089401\n",
      "epoch 833 | step 2 | loss: 0.0008679965778711201\n",
      "epoch 833 | step 3 | loss: 0.001150397156685213\n",
      "epoch 833 | step 4 | loss: 0.0014022059160175818\n",
      "epoch 833 | step 5 | loss: 0.0016798407125129503\n",
      "epoch 833 | step 6 | loss: 0.0019760929155544517\n",
      "epoch 833 | step 7 | loss: 0.0022776419409070115\n",
      "epoch 833 | step 8 | loss: 0.002598589457492391\n",
      "epoch 833 | step 9 | loss: 0.002876041477060386\n",
      "epoch 833 | step 10 | loss: 0.0031903892470537105\n",
      "epoch 833 | step 11 | loss: 0.003481896007165708\n",
      "epoch 834 | step 0 | loss: 0.00030048483643292596\n",
      "epoch 834 | step 1 | loss: 0.0005876143099907284\n",
      "epoch 834 | step 2 | loss: 0.0008819229867722816\n",
      "epoch 834 | step 3 | loss: 0.0011802116289740153\n",
      "epoch 834 | step 4 | loss: 0.0014624275390938651\n",
      "epoch 834 | step 5 | loss: 0.0017355009363100586\n",
      "epoch 834 | step 6 | loss: 0.0020323547268221954\n",
      "epoch 834 | step 7 | loss: 0.0023400335606616742\n",
      "epoch 834 | step 8 | loss: 0.0026431391034343523\n",
      "epoch 834 | step 9 | loss: 0.0029061878278969326\n",
      "epoch 834 | step 10 | loss: 0.003185344262108064\n",
      "epoch 834 | step 11 | loss: 0.0034836197997933143\n",
      "epoch 835 | step 0 | loss: 0.00028986204331537234\n",
      "epoch 835 | step 1 | loss: 0.0005830437267615238\n",
      "epoch 835 | step 2 | loss: 0.0008663674614759776\n",
      "epoch 835 | step 3 | loss: 0.001173117419886404\n",
      "epoch 835 | step 4 | loss: 0.0014698974623436807\n",
      "epoch 835 | step 5 | loss: 0.0017635732578923813\n",
      "epoch 835 | step 6 | loss: 0.0020444963214013906\n",
      "epoch 835 | step 7 | loss: 0.002365407048345311\n",
      "epoch 835 | step 8 | loss: 0.0026403924914173383\n",
      "epoch 835 | step 9 | loss: 0.0029054402746161497\n",
      "epoch 835 | step 10 | loss: 0.0031868994453442593\n",
      "epoch 835 | step 11 | loss: 0.0034827001957204035\n",
      "epoch 836 | step 0 | loss: 0.0002870143473012193\n",
      "epoch 836 | step 1 | loss: 0.0005655739714029698\n",
      "epoch 836 | step 2 | loss: 0.0008510447141470655\n",
      "epoch 836 | step 3 | loss: 0.0011346837762002636\n",
      "epoch 836 | step 4 | loss: 0.0014686529535234225\n",
      "epoch 836 | step 5 | loss: 0.0017416460014583128\n",
      "epoch 836 | step 6 | loss: 0.0020225813786959052\n",
      "epoch 836 | step 7 | loss: 0.0022978705393195196\n",
      "epoch 836 | step 8 | loss: 0.0025968340442068154\n",
      "epoch 836 | step 9 | loss: 0.0029077671505766353\n",
      "epoch 836 | step 10 | loss: 0.003175433662848351\n",
      "epoch 836 | step 11 | loss: 0.003487227942333671\n",
      "epoch 837 | step 0 | loss: 0.0003091364154172131\n",
      "epoch 837 | step 1 | loss: 0.0005714618041761998\n",
      "epoch 837 | step 2 | loss: 0.0008615194779993836\n",
      "epoch 837 | step 3 | loss: 0.0011656608763354781\n",
      "epoch 837 | step 4 | loss: 0.0014475707897782723\n",
      "epoch 837 | step 5 | loss: 0.0017416806341745404\n",
      "epoch 837 | step 6 | loss: 0.002034721076603796\n",
      "epoch 837 | step 7 | loss: 0.002334499089500207\n",
      "epoch 837 | step 8 | loss: 0.0026363701761475075\n",
      "epoch 837 | step 9 | loss: 0.002915811545425423\n",
      "epoch 837 | step 10 | loss: 0.0031974247649580235\n",
      "epoch 837 | step 11 | loss: 0.003478371647585867\n",
      "epoch 838 | step 0 | loss: 0.0002785535900019922\n",
      "epoch 838 | step 1 | loss: 0.0005911883596227996\n",
      "epoch 838 | step 2 | loss: 0.0008924391850580606\n",
      "epoch 838 | step 3 | loss: 0.0011886927665383314\n",
      "epoch 838 | step 4 | loss: 0.0014716272387680315\n",
      "epoch 838 | step 5 | loss: 0.0017459740104684661\n",
      "epoch 838 | step 6 | loss: 0.0020341932251870033\n",
      "epoch 838 | step 7 | loss: 0.0023127559344860056\n",
      "epoch 838 | step 8 | loss: 0.0026058803026415534\n",
      "epoch 838 | step 9 | loss: 0.0029093173826839785\n",
      "epoch 838 | step 10 | loss: 0.003192051637397496\n",
      "epoch 838 | step 11 | loss: 0.0034803417052695455\n",
      "epoch 839 | step 0 | loss: 0.00030499547762068765\n",
      "epoch 839 | step 1 | loss: 0.0005882442326275077\n",
      "epoch 839 | step 2 | loss: 0.0008759991361891843\n",
      "epoch 839 | step 3 | loss: 0.001179618393073313\n",
      "epoch 839 | step 4 | loss: 0.0014600291800891411\n",
      "epoch 839 | step 5 | loss: 0.00176023401891688\n",
      "epoch 839 | step 6 | loss: 0.0020465699771823314\n",
      "epoch 839 | step 7 | loss: 0.002318572822929633\n",
      "epoch 839 | step 8 | loss: 0.0026114059039576984\n",
      "epoch 839 | step 9 | loss: 0.0028958279220568446\n",
      "epoch 839 | step 10 | loss: 0.0031975397055263575\n",
      "epoch 839 | step 11 | loss: 0.003478117070264539\n",
      "epoch 840 | step 0 | loss: 0.000263189704477979\n",
      "epoch 840 | step 1 | loss: 0.0005514504978630907\n",
      "epoch 840 | step 2 | loss: 0.0008329720007342939\n",
      "epoch 840 | step 3 | loss: 0.0011291454382689734\n",
      "epoch 840 | step 4 | loss: 0.0014116507759859092\n",
      "epoch 840 | step 5 | loss: 0.0017281898121940115\n",
      "epoch 840 | step 6 | loss: 0.0020094754651652875\n",
      "epoch 840 | step 7 | loss: 0.0023258722185933453\n",
      "epoch 840 | step 8 | loss: 0.0025979527206743745\n",
      "epoch 840 | step 9 | loss: 0.002895386569076504\n",
      "epoch 840 | step 10 | loss: 0.003178217468761669\n",
      "epoch 840 | step 11 | loss: 0.003486300478933152\n",
      "epoch 841 | step 0 | loss: 0.0002861549396707124\n",
      "epoch 841 | step 1 | loss: 0.0006002104866696405\n",
      "epoch 841 | step 2 | loss: 0.0008855201591276337\n",
      "epoch 841 | step 3 | loss: 0.0011794317645809041\n",
      "epoch 841 | step 4 | loss: 0.0014547687961662796\n",
      "epoch 841 | step 5 | loss: 0.0017560689548821815\n",
      "epoch 841 | step 6 | loss: 0.0020613043518113564\n",
      "epoch 841 | step 7 | loss: 0.0023328375099932993\n",
      "epoch 841 | step 8 | loss: 0.0026254307453626264\n",
      "epoch 841 | step 9 | loss: 0.0029134954242535536\n",
      "epoch 841 | step 10 | loss: 0.003206205122934746\n",
      "epoch 841 | step 11 | loss: 0.0034743672045270306\n",
      "epoch 842 | step 0 | loss: 0.00027366156747634984\n",
      "epoch 842 | step 1 | loss: 0.0005557096831777105\n",
      "epoch 842 | step 2 | loss: 0.0008240521873132047\n",
      "epoch 842 | step 3 | loss: 0.0011209069168020619\n",
      "epoch 842 | step 4 | loss: 0.0014198853952396763\n",
      "epoch 842 | step 5 | loss: 0.0017324049612780132\n",
      "epoch 842 | step 6 | loss: 0.002005058514818794\n",
      "epoch 842 | step 7 | loss: 0.0023099497177192113\n",
      "epoch 842 | step 8 | loss: 0.00256853825094563\n",
      "epoch 842 | step 9 | loss: 0.0028741863890700994\n",
      "epoch 842 | step 10 | loss: 0.0031732370918263\n",
      "epoch 842 | step 11 | loss: 0.003487320977393303\n",
      "epoch 843 | step 0 | loss: 0.00029098968012087803\n",
      "epoch 843 | step 1 | loss: 0.0005738880191538181\n",
      "epoch 843 | step 2 | loss: 0.0008814734956055891\n",
      "epoch 843 | step 3 | loss: 0.001159054446350177\n",
      "epoch 843 | step 4 | loss: 0.0014648626577080874\n",
      "epoch 843 | step 5 | loss: 0.0017505121739648057\n",
      "epoch 843 | step 6 | loss: 0.002055267293815444\n",
      "epoch 843 | step 7 | loss: 0.00232589929169997\n",
      "epoch 843 | step 8 | loss: 0.002606220678599484\n",
      "epoch 843 | step 9 | loss: 0.0029145486917816687\n",
      "epoch 843 | step 10 | loss: 0.0031855899531365167\n",
      "epoch 843 | step 11 | loss: 0.0034824207392442526\n",
      "epoch 844 | step 0 | loss: 0.00030351914948595446\n",
      "epoch 844 | step 1 | loss: 0.000611349979764579\n",
      "epoch 844 | step 2 | loss: 0.0008881658959768422\n",
      "epoch 844 | step 3 | loss: 0.0011904568180574767\n",
      "epoch 844 | step 4 | loss: 0.001436640852461903\n",
      "epoch 844 | step 5 | loss: 0.0017408193834804694\n",
      "epoch 844 | step 6 | loss: 0.002006128261296391\n",
      "epoch 844 | step 7 | loss: 0.002320483066528761\n",
      "epoch 844 | step 8 | loss: 0.0026242077281094737\n",
      "epoch 844 | step 9 | loss: 0.0028977899837469216\n",
      "epoch 844 | step 10 | loss: 0.0031907402494347676\n",
      "epoch 844 | step 11 | loss: 0.003480402077510805\n",
      "epoch 845 | step 0 | loss: 0.00027903349544001365\n",
      "epoch 845 | step 1 | loss: 0.0005801326188234694\n",
      "epoch 845 | step 2 | loss: 0.0008954508494813278\n",
      "epoch 845 | step 3 | loss: 0.001188185617552977\n",
      "epoch 845 | step 4 | loss: 0.0014690753963825342\n",
      "epoch 845 | step 5 | loss: 0.001760040200997157\n",
      "epoch 845 | step 6 | loss: 0.0020685218356211407\n",
      "epoch 845 | step 7 | loss: 0.002354286920424666\n",
      "epoch 845 | step 8 | loss: 0.002622951338372523\n",
      "epoch 845 | step 9 | loss: 0.0029050358135758723\n",
      "epoch 845 | step 10 | loss: 0.003182316979764087\n",
      "epoch 845 | step 11 | loss: 0.0034836763228525956\n",
      "epoch 846 | step 0 | loss: 0.0002795158066386484\n",
      "epoch 846 | step 1 | loss: 0.0005748366777741358\n",
      "epoch 846 | step 2 | loss: 0.0008731329195240509\n",
      "epoch 846 | step 3 | loss: 0.001148736430769424\n",
      "epoch 846 | step 4 | loss: 0.0014126778730132903\n",
      "epoch 846 | step 5 | loss: 0.0016967771479192053\n",
      "epoch 846 | step 6 | loss: 0.0019965687922503033\n",
      "epoch 846 | step 7 | loss: 0.0022893591073807045\n",
      "epoch 846 | step 8 | loss: 0.0025797266465446517\n",
      "epoch 846 | step 9 | loss: 0.0028755347134831787\n",
      "epoch 846 | step 10 | loss: 0.0031714580384933433\n",
      "epoch 846 | step 11 | loss: 0.0034877351241802863\n",
      "epoch 847 | step 0 | loss: 0.00029680206115227046\n",
      "epoch 847 | step 1 | loss: 0.0005744354195196833\n",
      "epoch 847 | step 2 | loss: 0.000861498178628781\n",
      "epoch 847 | step 3 | loss: 0.001160718487183753\n",
      "epoch 847 | step 4 | loss: 0.0014441929744305388\n",
      "epoch 847 | step 5 | loss: 0.001717107262526199\n",
      "epoch 847 | step 6 | loss: 0.0020080868015522175\n",
      "epoch 847 | step 7 | loss: 0.002317943655940179\n",
      "epoch 847 | step 8 | loss: 0.0025984610745461473\n",
      "epoch 847 | step 9 | loss: 0.002882422961520422\n",
      "epoch 847 | step 10 | loss: 0.0031774336597360257\n",
      "epoch 847 | step 11 | loss: 0.003485294030978481\n",
      "epoch 848 | step 0 | loss: 0.00029306523788214406\n",
      "epoch 848 | step 1 | loss: 0.0005934628661237751\n",
      "epoch 848 | step 2 | loss: 0.0008793030091934923\n",
      "epoch 848 | step 3 | loss: 0.0011912179609061483\n",
      "epoch 848 | step 4 | loss: 0.00151468822810099\n",
      "epoch 848 | step 5 | loss: 0.0017988844973724054\n",
      "epoch 848 | step 6 | loss: 0.0020520437740194505\n",
      "epoch 848 | step 7 | loss: 0.0023357274565604033\n",
      "epoch 848 | step 8 | loss: 0.002620594715241306\n",
      "epoch 848 | step 9 | loss: 0.0029246424428039835\n",
      "epoch 848 | step 10 | loss: 0.0031827588787548635\n",
      "epoch 848 | step 11 | loss: 0.0034829996236899986\n",
      "epoch 849 | step 0 | loss: 0.00029558643310480254\n",
      "epoch 849 | step 1 | loss: 0.00058902914514472\n",
      "epoch 849 | step 2 | loss: 0.000893951709916039\n",
      "epoch 849 | step 3 | loss: 0.0011591814391841047\n",
      "epoch 849 | step 4 | loss: 0.001467267066497443\n",
      "epoch 849 | step 5 | loss: 0.0017563402049423672\n",
      "epoch 849 | step 6 | loss: 0.002038054172111292\n",
      "epoch 849 | step 7 | loss: 0.002329612417403805\n",
      "epoch 849 | step 8 | loss: 0.0026257540544760132\n",
      "epoch 849 | step 9 | loss: 0.0028905233789401864\n",
      "epoch 849 | step 10 | loss: 0.003199098782186787\n",
      "epoch 849 | step 11 | loss: 0.003476538186177976\n",
      "epoch 850 | step 0 | loss: 0.0002762450854829749\n",
      "epoch 850 | step 1 | loss: 0.0005228492236411307\n",
      "epoch 850 | step 2 | loss: 0.0008123470120345193\n",
      "epoch 850 | step 3 | loss: 0.0010765526914477206\n",
      "epoch 850 | step 4 | loss: 0.0013821058661198458\n",
      "epoch 850 | step 5 | loss: 0.001670785129638733\n",
      "epoch 850 | step 6 | loss: 0.0019656800894300034\n",
      "epoch 850 | step 7 | loss: 0.0022672930314156505\n",
      "epoch 850 | step 8 | loss: 0.0025644153675304453\n",
      "epoch 850 | step 9 | loss: 0.0028657051154079537\n",
      "epoch 850 | step 10 | loss: 0.003168878048934484\n",
      "epoch 850 | step 11 | loss: 0.0034881866445729674\n",
      "epoch 851 | step 0 | loss: 0.00026145437087313114\n",
      "epoch 851 | step 1 | loss: 0.0005449533341074571\n",
      "epoch 851 | step 2 | loss: 0.0008039543291413981\n",
      "epoch 851 | step 3 | loss: 0.0011018120347003556\n",
      "epoch 851 | step 4 | loss: 0.0014062876012459417\n",
      "epoch 851 | step 5 | loss: 0.0017243332662983848\n",
      "epoch 851 | step 6 | loss: 0.0020149570821879174\n",
      "epoch 851 | step 7 | loss: 0.002314589632341573\n",
      "epoch 851 | step 8 | loss: 0.002619126207861454\n",
      "epoch 851 | step 9 | loss: 0.002885004288971791\n",
      "epoch 851 | step 10 | loss: 0.0031807511558889267\n",
      "epoch 851 | step 11 | loss: 0.0034833916586135697\n",
      "epoch 852 | step 0 | loss: 0.00027524195663586397\n",
      "epoch 852 | step 1 | loss: 0.0005148785939829694\n",
      "epoch 852 | step 2 | loss: 0.0008265563160698991\n",
      "epoch 852 | step 3 | loss: 0.0011097130012157554\n",
      "epoch 852 | step 4 | loss: 0.0014216293954290753\n",
      "epoch 852 | step 5 | loss: 0.0016978335353311155\n",
      "epoch 852 | step 6 | loss: 0.0020082351235041563\n",
      "epoch 852 | step 7 | loss: 0.002298331590068371\n",
      "epoch 852 | step 8 | loss: 0.0026143933381207783\n",
      "epoch 852 | step 9 | loss: 0.0028856004708977825\n",
      "epoch 852 | step 10 | loss: 0.0031698894528094566\n",
      "epoch 852 | step 11 | loss: 0.0034876920864630894\n",
      "epoch 853 | step 0 | loss: 0.0002742598728282825\n",
      "epoch 853 | step 1 | loss: 0.0005571485279518239\n",
      "epoch 853 | step 2 | loss: 0.000847108852172601\n",
      "epoch 853 | step 3 | loss: 0.0011206994190542097\n",
      "epoch 853 | step 4 | loss: 0.0014169367705847646\n",
      "epoch 853 | step 5 | loss: 0.0017082288400876472\n",
      "epoch 853 | step 6 | loss: 0.0020334969314842626\n",
      "epoch 853 | step 7 | loss: 0.002312623830344376\n",
      "epoch 853 | step 8 | loss: 0.002589944289544109\n",
      "epoch 853 | step 9 | loss: 0.0028960051158888026\n",
      "epoch 853 | step 10 | loss: 0.003187933053214563\n",
      "epoch 853 | step 11 | loss: 0.0034803027754189537\n",
      "epoch 854 | step 0 | loss: 0.00027441385023484747\n",
      "epoch 854 | step 1 | loss: 0.0005728931745354105\n",
      "epoch 854 | step 2 | loss: 0.0008545778573025859\n",
      "epoch 854 | step 3 | loss: 0.001136633859126148\n",
      "epoch 854 | step 4 | loss: 0.0014407712981192946\n",
      "epoch 854 | step 5 | loss: 0.0017640543486486034\n",
      "epoch 854 | step 6 | loss: 0.0020546421076221098\n",
      "epoch 854 | step 7 | loss: 0.0023266200365692864\n",
      "epoch 854 | step 8 | loss: 0.0026222086747843556\n",
      "epoch 854 | step 9 | loss: 0.0029203414696044707\n",
      "epoch 854 | step 10 | loss: 0.0032033187889231934\n",
      "epoch 854 | step 11 | loss: 0.0034742126833564014\n",
      "epoch 855 | step 0 | loss: 0.00030187079530740737\n",
      "epoch 855 | step 1 | loss: 0.0005924066522389299\n",
      "epoch 855 | step 2 | loss: 0.0008810794965193995\n",
      "epoch 855 | step 3 | loss: 0.0012009949984622154\n",
      "epoch 855 | step 4 | loss: 0.0014481961084838696\n",
      "epoch 855 | step 5 | loss: 0.0017195989669970488\n",
      "epoch 855 | step 6 | loss: 0.0019952865614916997\n",
      "epoch 855 | step 7 | loss: 0.002309734442416265\n",
      "epoch 855 | step 8 | loss: 0.0025990650395598305\n",
      "epoch 855 | step 9 | loss: 0.0028671391473564407\n",
      "epoch 855 | step 10 | loss: 0.0031775016479461777\n",
      "epoch 855 | step 11 | loss: 0.003484227347943375\n",
      "epoch 856 | step 0 | loss: 0.00029814921500789825\n",
      "epoch 856 | step 1 | loss: 0.0005760331990583362\n",
      "epoch 856 | step 2 | loss: 0.0008632955091722119\n",
      "epoch 856 | step 3 | loss: 0.0011608941076885584\n",
      "epoch 856 | step 4 | loss: 0.0014549349640253814\n",
      "epoch 856 | step 5 | loss: 0.0017354672284308412\n",
      "epoch 856 | step 6 | loss: 0.0020009753364052677\n",
      "epoch 856 | step 7 | loss: 0.002265352571282766\n",
      "epoch 856 | step 8 | loss: 0.0025778976197077495\n",
      "epoch 856 | step 9 | loss: 0.0028787068761451473\n",
      "epoch 856 | step 10 | loss: 0.00317496572478203\n",
      "epoch 856 | step 11 | loss: 0.0034850574921747785\n",
      "epoch 857 | step 0 | loss: 0.00030193818059484325\n",
      "epoch 857 | step 1 | loss: 0.0005973296670541759\n",
      "epoch 857 | step 2 | loss: 0.0008928881640008684\n",
      "epoch 857 | step 3 | loss: 0.0011633101310979574\n",
      "epoch 857 | step 4 | loss: 0.0014470731401727771\n",
      "epoch 857 | step 5 | loss: 0.0017430432463671434\n",
      "epoch 857 | step 6 | loss: 0.0020276790575590392\n",
      "epoch 857 | step 7 | loss: 0.002312822432119411\n",
      "epoch 857 | step 8 | loss: 0.002601464514243498\n",
      "epoch 857 | step 9 | loss: 0.0029106597112799415\n",
      "epoch 857 | step 10 | loss: 0.003192927791356948\n",
      "epoch 857 | step 11 | loss: 0.003477970827617332\n",
      "epoch 858 | step 0 | loss: 0.00028487499154074725\n",
      "epoch 858 | step 1 | loss: 0.0005708723276330725\n",
      "epoch 858 | step 2 | loss: 0.0008458124920269391\n",
      "epoch 858 | step 3 | loss: 0.0011304537260234209\n",
      "epoch 858 | step 4 | loss: 0.001423162301130427\n",
      "epoch 858 | step 5 | loss: 0.001691320003722818\n",
      "epoch 858 | step 6 | loss: 0.0020046192942441907\n",
      "epoch 858 | step 7 | loss: 0.0022820042187603395\n",
      "epoch 858 | step 8 | loss: 0.002581342647383558\n",
      "epoch 858 | step 9 | loss: 0.002881502682193767\n",
      "epoch 858 | step 10 | loss: 0.0031634622968602587\n",
      "epoch 858 | step 11 | loss: 0.0034893703482335044\n",
      "epoch 859 | step 0 | loss: 0.0003016474726729057\n",
      "epoch 859 | step 1 | loss: 0.0005931107502870315\n",
      "epoch 859 | step 2 | loss: 0.0008830930283843508\n",
      "epoch 859 | step 3 | loss: 0.0011686431963519367\n",
      "epoch 859 | step 4 | loss: 0.0014690723648483873\n",
      "epoch 859 | step 5 | loss: 0.0017338841824150026\n",
      "epoch 859 | step 6 | loss: 0.002011953401068001\n",
      "epoch 859 | step 7 | loss: 0.0022896916824290646\n",
      "epoch 859 | step 8 | loss: 0.002601112527849996\n",
      "epoch 859 | step 9 | loss: 0.00290157495287924\n",
      "epoch 859 | step 10 | loss: 0.003196985273840157\n",
      "epoch 859 | step 11 | loss: 0.0034764333185235787\n",
      "epoch 860 | step 0 | loss: 0.0003046815535337081\n",
      "epoch 860 | step 1 | loss: 0.0006177496548231138\n",
      "epoch 860 | step 2 | loss: 0.0008988464953104561\n",
      "epoch 860 | step 3 | loss: 0.0012013974046203545\n",
      "epoch 860 | step 4 | loss: 0.001474960873095696\n",
      "epoch 860 | step 5 | loss: 0.0017601848100103489\n",
      "epoch 860 | step 6 | loss: 0.002053895301251462\n",
      "epoch 860 | step 7 | loss: 0.002355267994128757\n",
      "epoch 860 | step 8 | loss: 0.00263411577498786\n",
      "epoch 860 | step 9 | loss: 0.0029254031116710635\n",
      "epoch 860 | step 10 | loss: 0.003198120819552093\n",
      "epoch 860 | step 11 | loss: 0.003475513308074865\n",
      "epoch 861 | step 0 | loss: 0.00028619589413892874\n",
      "epoch 861 | step 1 | loss: 0.0005554001637645795\n",
      "epoch 861 | step 2 | loss: 0.0008722582231139753\n",
      "epoch 861 | step 3 | loss: 0.0011800615845798334\n",
      "epoch 861 | step 4 | loss: 0.0014751308538496417\n",
      "epoch 861 | step 5 | loss: 0.0017567309982573782\n",
      "epoch 861 | step 6 | loss: 0.0020384215185853876\n",
      "epoch 861 | step 7 | loss: 0.002323122920991687\n",
      "epoch 861 | step 8 | loss: 0.002611079122267315\n",
      "epoch 861 | step 9 | loss: 0.002910568996380247\n",
      "epoch 861 | step 10 | loss: 0.003208326380041075\n",
      "epoch 861 | step 11 | loss: 0.00347166906711072\n",
      "epoch 862 | step 0 | loss: 0.00028220195548976786\n",
      "epoch 862 | step 1 | loss: 0.0006063496661893788\n",
      "epoch 862 | step 2 | loss: 0.0009149895118279197\n",
      "epoch 862 | step 3 | loss: 0.0012081806481395794\n",
      "epoch 862 | step 4 | loss: 0.0014954237958542409\n",
      "epoch 862 | step 5 | loss: 0.0017840314506938734\n",
      "epoch 862 | step 6 | loss: 0.002083450936623702\n",
      "epoch 862 | step 7 | loss: 0.002363340233623839\n",
      "epoch 862 | step 8 | loss: 0.0026553769854226923\n",
      "epoch 862 | step 9 | loss: 0.002914497578689096\n",
      "epoch 862 | step 10 | loss: 0.0031935265151986393\n",
      "epoch 862 | step 11 | loss: 0.003477113994445522\n",
      "epoch 863 | step 0 | loss: 0.0002813946669145132\n",
      "epoch 863 | step 1 | loss: 0.0005737898606274893\n",
      "epoch 863 | step 2 | loss: 0.0008364019090257349\n",
      "epoch 863 | step 3 | loss: 0.001153190121730294\n",
      "epoch 863 | step 4 | loss: 0.0014466511916629919\n",
      "epoch 863 | step 5 | loss: 0.0017265964587824547\n",
      "epoch 863 | step 6 | loss: 0.002037461867851627\n",
      "epoch 863 | step 7 | loss: 0.00232086800791531\n",
      "epoch 863 | step 8 | loss: 0.0026122787567616278\n",
      "epoch 863 | step 9 | loss: 0.002890442158650917\n",
      "epoch 863 | step 10 | loss: 0.003202863558670138\n",
      "epoch 863 | step 11 | loss: 0.0034733144184302964\n",
      "epoch 864 | step 0 | loss: 0.0002853220328369095\n",
      "epoch 864 | step 1 | loss: 0.0005771282452067744\n",
      "epoch 864 | step 2 | loss: 0.0008669682674361882\n",
      "epoch 864 | step 3 | loss: 0.0011651323886102648\n",
      "epoch 864 | step 4 | loss: 0.0014629211101119532\n",
      "epoch 864 | step 5 | loss: 0.0017498411776455092\n",
      "epoch 864 | step 6 | loss: 0.002035074109541488\n",
      "epoch 864 | step 7 | loss: 0.002297214580023439\n",
      "epoch 864 | step 8 | loss: 0.0026053008490106644\n",
      "epoch 864 | step 9 | loss: 0.002888077628611767\n",
      "epoch 864 | step 10 | loss: 0.0032119348138860784\n",
      "epoch 864 | step 11 | loss: 0.003469555967099986\n",
      "epoch 865 | step 0 | loss: 0.00028202030873624047\n",
      "epoch 865 | step 1 | loss: 0.0006069957347540581\n",
      "epoch 865 | step 2 | loss: 0.0008874868566145081\n",
      "epoch 865 | step 3 | loss: 0.0011859227461668614\n",
      "epoch 865 | step 4 | loss: 0.0014448608731217792\n",
      "epoch 865 | step 5 | loss: 0.0017403739596905763\n",
      "epoch 865 | step 6 | loss: 0.0020092791924226166\n",
      "epoch 865 | step 7 | loss: 0.0023085561866565225\n",
      "epoch 865 | step 8 | loss: 0.0026223720607731612\n",
      "epoch 865 | step 9 | loss: 0.0029097713267407293\n",
      "epoch 865 | step 10 | loss: 0.003180137035706662\n",
      "epoch 865 | step 11 | loss: 0.0034820393890373494\n",
      "epoch 866 | step 0 | loss: 0.0002760026585529132\n",
      "epoch 866 | step 1 | loss: 0.0005535300412629781\n",
      "epoch 866 | step 2 | loss: 0.0008615113269516588\n",
      "epoch 866 | step 3 | loss: 0.0011667770090723153\n",
      "epoch 866 | step 4 | loss: 0.001427892862358611\n",
      "epoch 866 | step 5 | loss: 0.001696466020173966\n",
      "epoch 866 | step 6 | loss: 0.0020120827384972235\n",
      "epoch 866 | step 7 | loss: 0.0023007871777783627\n",
      "epoch 866 | step 8 | loss: 0.0025853061101117638\n",
      "epoch 866 | step 9 | loss: 0.0028691771093111175\n",
      "epoch 866 | step 10 | loss: 0.003204470730004545\n",
      "epoch 866 | step 11 | loss: 0.003472451011218891\n",
      "epoch 867 | step 0 | loss: 0.00033153314702825937\n",
      "epoch 867 | step 1 | loss: 0.0006034028079254455\n",
      "epoch 867 | step 2 | loss: 0.0008678089861465718\n",
      "epoch 867 | step 3 | loss: 0.00113110533969307\n",
      "epoch 867 | step 4 | loss: 0.0014516200561698414\n",
      "epoch 867 | step 5 | loss: 0.0017353938658734246\n",
      "epoch 867 | step 6 | loss: 0.002031089027677729\n",
      "epoch 867 | step 7 | loss: 0.0023345440791280353\n",
      "epoch 867 | step 8 | loss: 0.002620724827409595\n",
      "epoch 867 | step 9 | loss: 0.0029213625521414656\n",
      "epoch 867 | step 10 | loss: 0.003203588910742054\n",
      "epoch 867 | step 11 | loss: 0.003472564237629328\n",
      "epoch 868 | step 0 | loss: 0.0002757227846810732\n",
      "epoch 868 | step 1 | loss: 0.0006118288037473169\n",
      "epoch 868 | step 2 | loss: 0.0009127622650809935\n",
      "epoch 868 | step 3 | loss: 0.0012124122962445367\n",
      "epoch 868 | step 4 | loss: 0.0014809393334798947\n",
      "epoch 868 | step 5 | loss: 0.0017780491364229878\n",
      "epoch 868 | step 6 | loss: 0.0020563926104396842\n",
      "epoch 868 | step 7 | loss: 0.002342442288269045\n",
      "epoch 868 | step 8 | loss: 0.002650610756391313\n",
      "epoch 868 | step 9 | loss: 0.002920249221034495\n",
      "epoch 868 | step 10 | loss: 0.0032094294776680585\n",
      "epoch 868 | step 11 | loss: 0.0034702852060667236\n",
      "epoch 869 | step 0 | loss: 0.0002816585588621756\n",
      "epoch 869 | step 1 | loss: 0.0005731880776296092\n",
      "epoch 869 | step 2 | loss: 0.0008851183248735708\n",
      "epoch 869 | step 3 | loss: 0.0011696762604192063\n",
      "epoch 869 | step 4 | loss: 0.0014543987917603114\n",
      "epoch 869 | step 5 | loss: 0.0017259443718282475\n",
      "epoch 869 | step 6 | loss: 0.0020224145058512716\n",
      "epoch 869 | step 7 | loss: 0.0022932075411241934\n",
      "epoch 869 | step 8 | loss: 0.002597831035232379\n",
      "epoch 869 | step 9 | loss: 0.002891792570463708\n",
      "epoch 869 | step 10 | loss: 0.0031766348549575442\n",
      "epoch 869 | step 11 | loss: 0.003482634212161366\n",
      "epoch 870 | step 0 | loss: 0.00032597434062254106\n",
      "epoch 870 | step 1 | loss: 0.0006003750569855871\n",
      "epoch 870 | step 2 | loss: 0.0008812147645914816\n",
      "epoch 870 | step 3 | loss: 0.0011443980368201572\n",
      "epoch 870 | step 4 | loss: 0.0014392943748004737\n",
      "epoch 870 | step 5 | loss: 0.0017283130347856175\n",
      "epoch 870 | step 6 | loss: 0.00204918564100457\n",
      "epoch 870 | step 7 | loss: 0.002347275924170051\n",
      "epoch 870 | step 8 | loss: 0.002638077963730837\n",
      "epoch 870 | step 9 | loss: 0.0029225684413839033\n",
      "epoch 870 | step 10 | loss: 0.003184665479658265\n",
      "epoch 870 | step 11 | loss: 0.0034794392580440895\n",
      "epoch 871 | step 0 | loss: 0.00027216669546232066\n",
      "epoch 871 | step 1 | loss: 0.000576221979187571\n",
      "epoch 871 | step 2 | loss: 0.0008626264351681776\n",
      "epoch 871 | step 3 | loss: 0.0011310094514639314\n",
      "epoch 871 | step 4 | loss: 0.0014493446510238082\n",
      "epoch 871 | step 5 | loss: 0.0017306926808078541\n",
      "epoch 871 | step 6 | loss: 0.002026186889380457\n",
      "epoch 871 | step 7 | loss: 0.0023395295159436913\n",
      "epoch 871 | step 8 | loss: 0.002605710068001044\n",
      "epoch 871 | step 9 | loss: 0.0028890926068997685\n",
      "epoch 871 | step 10 | loss: 0.0031600935976304075\n",
      "epoch 871 | step 11 | loss: 0.0034890951963686607\n",
      "epoch 872 | step 0 | loss: 0.00027369203351432746\n",
      "epoch 872 | step 1 | loss: 0.0005766543914322919\n",
      "epoch 872 | step 2 | loss: 0.000862139700820811\n",
      "epoch 872 | step 3 | loss: 0.001165970742536221\n",
      "epoch 872 | step 4 | loss: 0.0014592621959042763\n",
      "epoch 872 | step 5 | loss: 0.0017598768733790315\n",
      "epoch 872 | step 6 | loss: 0.0020555778869275282\n",
      "epoch 872 | step 7 | loss: 0.002327279291369241\n",
      "epoch 872 | step 8 | loss: 0.0026275913956933126\n",
      "epoch 872 | step 9 | loss: 0.0028994513018965194\n",
      "epoch 872 | step 10 | loss: 0.0031791594712430927\n",
      "epoch 872 | step 11 | loss: 0.003481439152733255\n",
      "epoch 873 | step 0 | loss: 0.0003138713774496746\n",
      "epoch 873 | step 1 | loss: 0.0006094125138253984\n",
      "epoch 873 | step 2 | loss: 0.0009055911925641265\n",
      "epoch 873 | step 3 | loss: 0.0011856596422349533\n",
      "epoch 873 | step 4 | loss: 0.0014724823925959833\n",
      "epoch 873 | step 5 | loss: 0.0017657942652664089\n",
      "epoch 873 | step 6 | loss: 0.002066858247274935\n",
      "epoch 873 | step 7 | loss: 0.0023615459342486785\n",
      "epoch 873 | step 8 | loss: 0.002650695921337028\n",
      "epoch 873 | step 9 | loss: 0.002922936949981123\n",
      "epoch 873 | step 10 | loss: 0.003186734916530533\n",
      "epoch 873 | step 11 | loss: 0.0034784429895664457\n",
      "epoch 874 | step 0 | loss: 0.0003013972679796617\n",
      "epoch 874 | step 1 | loss: 0.000599291445566165\n",
      "epoch 874 | step 2 | loss: 0.0009094047944049728\n",
      "epoch 874 | step 3 | loss: 0.0012097758091207764\n",
      "epoch 874 | step 4 | loss: 0.0014868157493428986\n",
      "epoch 874 | step 5 | loss: 0.0017613155144174104\n",
      "epoch 874 | step 6 | loss: 0.002034649855623456\n",
      "epoch 874 | step 7 | loss: 0.002330102054786584\n",
      "epoch 874 | step 8 | loss: 0.0026006037766059032\n",
      "epoch 874 | step 9 | loss: 0.0029153363585609736\n",
      "epoch 874 | step 10 | loss: 0.003177573816716046\n",
      "epoch 874 | step 11 | loss: 0.0034821417141849436\n",
      "epoch 875 | step 0 | loss: 0.0002923850544243493\n",
      "epoch 875 | step 1 | loss: 0.0005815579423534498\n",
      "epoch 875 | step 2 | loss: 0.0008657915664210534\n",
      "epoch 875 | step 3 | loss: 0.0011234923789166017\n",
      "epoch 875 | step 4 | loss: 0.001415741276839767\n",
      "epoch 875 | step 5 | loss: 0.0016998530912232294\n",
      "epoch 875 | step 6 | loss: 0.0019712606030409686\n",
      "epoch 875 | step 7 | loss: 0.002274267924768986\n",
      "epoch 875 | step 8 | loss: 0.0025742850785697878\n",
      "epoch 875 | step 9 | loss: 0.002883683842979156\n",
      "epoch 875 | step 10 | loss: 0.0031825698258044948\n",
      "epoch 875 | step 11 | loss: 0.003479999603899734\n",
      "epoch 876 | step 0 | loss: 0.0003225462039215296\n",
      "epoch 876 | step 1 | loss: 0.000599290548127907\n",
      "epoch 876 | step 2 | loss: 0.0008764437013892778\n",
      "epoch 876 | step 3 | loss: 0.0011662562226953332\n",
      "epoch 876 | step 4 | loss: 0.0014695067699025358\n",
      "epoch 876 | step 5 | loss: 0.001771065721415816\n",
      "epoch 876 | step 6 | loss: 0.0020587154196976287\n",
      "epoch 876 | step 7 | loss: 0.002332526125216578\n",
      "epoch 876 | step 8 | loss: 0.002620951210730178\n",
      "epoch 876 | step 9 | loss: 0.002928811593037972\n",
      "epoch 876 | step 10 | loss: 0.0032033123476015333\n",
      "epoch 876 | step 11 | loss: 0.0034716774941632314\n",
      "epoch 877 | step 0 | loss: 0.00031349930840326404\n",
      "epoch 877 | step 1 | loss: 0.0006157404386055122\n",
      "epoch 877 | step 2 | loss: 0.0008930947281494013\n",
      "epoch 877 | step 3 | loss: 0.001183940195613036\n",
      "epoch 877 | step 4 | loss: 0.001457706059411163\n",
      "epoch 877 | step 5 | loss: 0.0017708543346518005\n",
      "epoch 877 | step 6 | loss: 0.0020704317892528156\n",
      "epoch 877 | step 7 | loss: 0.0023444649201176046\n",
      "epoch 877 | step 8 | loss: 0.0026213349023549042\n",
      "epoch 877 | step 9 | loss: 0.002902323557251175\n",
      "epoch 877 | step 10 | loss: 0.003179404599970747\n",
      "epoch 877 | step 11 | loss: 0.0034807434139414667\n",
      "epoch 878 | step 0 | loss: 0.0002826928128682435\n",
      "epoch 878 | step 1 | loss: 0.0005720889261297088\n",
      "epoch 878 | step 2 | loss: 0.0008505933417490985\n",
      "epoch 878 | step 3 | loss: 0.001128592519563739\n",
      "epoch 878 | step 4 | loss: 0.0014461073658596246\n",
      "epoch 878 | step 5 | loss: 0.0017483660112069972\n",
      "epoch 878 | step 6 | loss: 0.0020098014098537172\n",
      "epoch 878 | step 7 | loss: 0.0022929871994912957\n",
      "epoch 878 | step 8 | loss: 0.0025978388507757674\n",
      "epoch 878 | step 9 | loss: 0.0029005157494114324\n",
      "epoch 878 | step 10 | loss: 0.0031989922150561637\n",
      "epoch 878 | step 11 | loss: 0.003473246873471697\n",
      "epoch 879 | step 0 | loss: 0.000284099035538502\n",
      "epoch 879 | step 1 | loss: 0.0005661263925543586\n",
      "epoch 879 | step 2 | loss: 0.0008656054755980015\n",
      "epoch 879 | step 3 | loss: 0.0011626551757973691\n",
      "epoch 879 | step 4 | loss: 0.0014341582305949336\n",
      "epoch 879 | step 5 | loss: 0.0017270749077334872\n",
      "epoch 879 | step 6 | loss: 0.0020413683032334236\n",
      "epoch 879 | step 7 | loss: 0.0023067566599739133\n",
      "epoch 879 | step 8 | loss: 0.002620903728971104\n",
      "epoch 879 | step 9 | loss: 0.0028989276236430356\n",
      "epoch 879 | step 10 | loss: 0.0032139266581517265\n",
      "epoch 879 | step 11 | loss: 0.0034671098856196\n",
      "epoch 880 | step 0 | loss: 0.00030606662970180846\n",
      "epoch 880 | step 1 | loss: 0.0005485712461728102\n",
      "epoch 880 | step 2 | loss: 0.0008177715916274092\n",
      "epoch 880 | step 3 | loss: 0.0011653313317962742\n",
      "epoch 880 | step 4 | loss: 0.001449246690144972\n",
      "epoch 880 | step 5 | loss: 0.0017377253743452912\n",
      "epoch 880 | step 6 | loss: 0.00203028854924549\n",
      "epoch 880 | step 7 | loss: 0.002326042695427624\n",
      "epoch 880 | step 8 | loss: 0.0026233109519595626\n",
      "epoch 880 | step 9 | loss: 0.002905862478105435\n",
      "epoch 880 | step 10 | loss: 0.003189353521873957\n",
      "epoch 880 | step 11 | loss: 0.003476740890134584\n",
      "epoch 881 | step 0 | loss: 0.00027110569290056624\n",
      "epoch 881 | step 1 | loss: 0.0005937848598112323\n",
      "epoch 881 | step 2 | loss: 0.0008996290298853062\n",
      "epoch 881 | step 3 | loss: 0.0011844588443753305\n",
      "epoch 881 | step 4 | loss: 0.0014722190592929823\n",
      "epoch 881 | step 5 | loss: 0.0017519762051713735\n",
      "epoch 881 | step 6 | loss: 0.002058229743210663\n",
      "epoch 881 | step 7 | loss: 0.0023549782845843793\n",
      "epoch 881 | step 8 | loss: 0.002647331915993387\n",
      "epoch 881 | step 9 | loss: 0.0029257939531606067\n",
      "epoch 881 | step 10 | loss: 0.003189130656473108\n",
      "epoch 881 | step 11 | loss: 0.0034765037774755736\n",
      "epoch 882 | step 0 | loss: 0.0003034141806720855\n",
      "epoch 882 | step 1 | loss: 0.0005581608630322611\n",
      "epoch 882 | step 2 | loss: 0.0008345873018081826\n",
      "epoch 882 | step 3 | loss: 0.0011229233915701017\n",
      "epoch 882 | step 4 | loss: 0.0013831714288934304\n",
      "epoch 882 | step 5 | loss: 0.0016621284454766006\n",
      "epoch 882 | step 6 | loss: 0.0019817209638893405\n",
      "epoch 882 | step 7 | loss: 0.002279980491528834\n",
      "epoch 882 | step 8 | loss: 0.0025648640256095987\n",
      "epoch 882 | step 9 | loss: 0.0028753680631450453\n",
      "epoch 882 | step 10 | loss: 0.0031685762169371802\n",
      "epoch 882 | step 11 | loss: 0.0034843139372813846\n",
      "epoch 883 | step 0 | loss: 0.00029165396199313154\n",
      "epoch 883 | step 1 | loss: 0.000563986348391194\n",
      "epoch 883 | step 2 | loss: 0.0008433690238083653\n",
      "epoch 883 | step 3 | loss: 0.001130052729084867\n",
      "epoch 883 | step 4 | loss: 0.0014374875484901742\n",
      "epoch 883 | step 5 | loss: 0.0017319406968096592\n",
      "epoch 883 | step 6 | loss: 0.0020205767874124244\n",
      "epoch 883 | step 7 | loss: 0.002328178119518988\n",
      "epoch 883 | step 8 | loss: 0.002629043998273079\n",
      "epoch 883 | step 9 | loss: 0.0028874315143205724\n",
      "epoch 883 | step 10 | loss: 0.0031963584593193775\n",
      "epoch 883 | step 11 | loss: 0.003473533001181912\n",
      "epoch 884 | step 0 | loss: 0.0002800276822339822\n",
      "epoch 884 | step 1 | loss: 0.0005714713381988286\n",
      "epoch 884 | step 2 | loss: 0.000858021528100835\n",
      "epoch 884 | step 3 | loss: 0.0011823064615553041\n",
      "epoch 884 | step 4 | loss: 0.0014652212435245044\n",
      "epoch 884 | step 5 | loss: 0.0017241080185828813\n",
      "epoch 884 | step 6 | loss: 0.0020011662183233477\n",
      "epoch 884 | step 7 | loss: 0.0022799027834403953\n",
      "epoch 884 | step 8 | loss: 0.002592267391941132\n",
      "epoch 884 | step 9 | loss: 0.0028825498288861046\n",
      "epoch 884 | step 10 | loss: 0.0031846303611974868\n",
      "epoch 884 | step 11 | loss: 0.0034779635416346912\n",
      "epoch 885 | step 0 | loss: 0.00029153114226379025\n",
      "epoch 885 | step 1 | loss: 0.0005994625430099786\n",
      "epoch 885 | step 2 | loss: 0.0008839426865102969\n",
      "epoch 885 | step 3 | loss: 0.001184943267944784\n",
      "epoch 885 | step 4 | loss: 0.0014909516912335046\n",
      "epoch 885 | step 5 | loss: 0.0017925921213851378\n",
      "epoch 885 | step 6 | loss: 0.002040921808211262\n",
      "epoch 885 | step 7 | loss: 0.0023182597475442795\n",
      "epoch 885 | step 8 | loss: 0.0025930836711461837\n",
      "epoch 885 | step 9 | loss: 0.0029018713201764555\n",
      "epoch 885 | step 10 | loss: 0.003186552475827633\n",
      "epoch 885 | step 11 | loss: 0.0034773501138745965\n",
      "epoch 886 | step 0 | loss: 0.0003047139395073033\n",
      "epoch 886 | step 1 | loss: 0.0005906781983906019\n",
      "epoch 886 | step 2 | loss: 0.0008598948815153069\n",
      "epoch 886 | step 3 | loss: 0.0011636325613461908\n",
      "epoch 886 | step 4 | loss: 0.0014467396201739064\n",
      "epoch 886 | step 5 | loss: 0.001734195588061531\n",
      "epoch 886 | step 6 | loss: 0.002039783862394759\n",
      "epoch 886 | step 7 | loss: 0.0023113230004583704\n",
      "epoch 886 | step 8 | loss: 0.002597988795622017\n",
      "epoch 886 | step 9 | loss: 0.002915001490250364\n",
      "epoch 886 | step 10 | loss: 0.0031988944208372243\n",
      "epoch 886 | step 11 | loss: 0.0034719920490509566\n",
      "epoch 887 | step 0 | loss: 0.0002942852839911571\n",
      "epoch 887 | step 1 | loss: 0.000590831010303417\n",
      "epoch 887 | step 2 | loss: 0.0008839759898049542\n",
      "epoch 887 | step 3 | loss: 0.0011981365448592838\n",
      "epoch 887 | step 4 | loss: 0.00149131597492133\n",
      "epoch 887 | step 5 | loss: 0.0017768930994892015\n",
      "epoch 887 | step 6 | loss: 0.0020579496808931655\n",
      "epoch 887 | step 7 | loss: 0.00233749320577301\n",
      "epoch 887 | step 8 | loss: 0.002605127780414047\n",
      "epoch 887 | step 9 | loss: 0.0029049307675456577\n",
      "epoch 887 | step 10 | loss: 0.003178405196592983\n",
      "epoch 887 | step 11 | loss: 0.0034800648710388168\n",
      "epoch 888 | step 0 | loss: 0.0003154582993804454\n",
      "epoch 888 | step 1 | loss: 0.0006013102585669202\n",
      "epoch 888 | step 2 | loss: 0.0008679357649926337\n",
      "epoch 888 | step 3 | loss: 0.001170653166739698\n",
      "epoch 888 | step 4 | loss: 0.0014640729037690274\n",
      "epoch 888 | step 5 | loss: 0.001745804501072784\n",
      "epoch 888 | step 6 | loss: 0.002031791207901523\n",
      "epoch 888 | step 7 | loss: 0.002334109502056169\n",
      "epoch 888 | step 8 | loss: 0.002614437400369515\n",
      "epoch 888 | step 9 | loss: 0.0029028817068800926\n",
      "epoch 888 | step 10 | loss: 0.003190669505372724\n",
      "epoch 888 | step 11 | loss: 0.0034751996944286656\n",
      "epoch 889 | step 0 | loss: 0.00027285313805794885\n",
      "epoch 889 | step 1 | loss: 0.0005895002841378113\n",
      "epoch 889 | step 2 | loss: 0.000893135463216994\n",
      "epoch 889 | step 3 | loss: 0.0011786534933281361\n",
      "epoch 889 | step 4 | loss: 0.0014476055147738232\n",
      "epoch 889 | step 5 | loss: 0.0017472075718957824\n",
      "epoch 889 | step 6 | loss: 0.002062098162612516\n",
      "epoch 889 | step 7 | loss: 0.002351508182086741\n",
      "epoch 889 | step 8 | loss: 0.0026595962850956578\n",
      "epoch 889 | step 9 | loss: 0.002913403802198405\n",
      "epoch 889 | step 10 | loss: 0.003201730138131699\n",
      "epoch 889 | step 11 | loss: 0.003470791673640601\n",
      "epoch 890 | step 0 | loss: 0.00030116788826269243\n",
      "epoch 890 | step 1 | loss: 0.0006051870373153538\n",
      "epoch 890 | step 2 | loss: 0.0009107991444890941\n",
      "epoch 890 | step 3 | loss: 0.001188285027224423\n",
      "epoch 890 | step 4 | loss: 0.0014837140314695354\n",
      "epoch 890 | step 5 | loss: 0.0017585526959661834\n",
      "epoch 890 | step 6 | loss: 0.002049590730115488\n",
      "epoch 890 | step 7 | loss: 0.0023446438566954745\n",
      "epoch 890 | step 8 | loss: 0.002618416227008729\n",
      "epoch 890 | step 9 | loss: 0.0028908411922777743\n",
      "epoch 890 | step 10 | loss: 0.003181884446776414\n",
      "epoch 890 | step 11 | loss: 0.0034782372629822483\n",
      "epoch 891 | step 0 | loss: 0.000294134669837876\n",
      "epoch 891 | step 1 | loss: 0.0006078602744690817\n",
      "epoch 891 | step 2 | loss: 0.000894383203860533\n",
      "epoch 891 | step 3 | loss: 0.0011859393549372372\n",
      "epoch 891 | step 4 | loss: 0.0014799759433896844\n",
      "epoch 891 | step 5 | loss: 0.0017662709778861574\n",
      "epoch 891 | step 6 | loss: 0.0020773375405499558\n",
      "epoch 891 | step 7 | loss: 0.0023403706870728344\n",
      "epoch 891 | step 8 | loss: 0.0026191387349271634\n",
      "epoch 891 | step 9 | loss: 0.0028899007906167573\n",
      "epoch 891 | step 10 | loss: 0.003185645486156692\n",
      "epoch 891 | step 11 | loss: 0.0034767734062635808\n",
      "epoch 892 | step 0 | loss: 0.0002780601439036601\n",
      "epoch 892 | step 1 | loss: 0.000556352064610033\n",
      "epoch 892 | step 2 | loss: 0.0008481654227102377\n",
      "epoch 892 | step 3 | loss: 0.0011464212978053538\n",
      "epoch 892 | step 4 | loss: 0.0014017856756642622\n",
      "epoch 892 | step 5 | loss: 0.0016627111234145473\n",
      "epoch 892 | step 6 | loss: 0.0019504491920955852\n",
      "epoch 892 | step 7 | loss: 0.0022572106687624915\n",
      "epoch 892 | step 8 | loss: 0.0025335709572084186\n",
      "epoch 892 | step 9 | loss: 0.0028393073999769215\n",
      "epoch 892 | step 10 | loss: 0.0031742502526666454\n",
      "epoch 892 | step 11 | loss: 0.0034811796283494693\n",
      "epoch 893 | step 0 | loss: 0.00027066574839837985\n",
      "epoch 893 | step 1 | loss: 0.000561757709929233\n",
      "epoch 893 | step 2 | loss: 0.0008304901151021442\n",
      "epoch 893 | step 3 | loss: 0.001094793099051553\n",
      "epoch 893 | step 4 | loss: 0.0014089869622305553\n",
      "epoch 893 | step 5 | loss: 0.001698012429337203\n",
      "epoch 893 | step 6 | loss: 0.0020037021953804315\n",
      "epoch 893 | step 7 | loss: 0.002294940283688234\n",
      "epoch 893 | step 8 | loss: 0.002590899362855558\n",
      "epoch 893 | step 9 | loss: 0.002876695113264296\n",
      "epoch 893 | step 10 | loss: 0.00318469202952531\n",
      "epoch 893 | step 11 | loss: 0.0034768461073360914\n",
      "epoch 894 | step 0 | loss: 0.000296169257630161\n",
      "epoch 894 | step 1 | loss: 0.0005894843243439064\n",
      "epoch 894 | step 2 | loss: 0.0008588351718445851\n",
      "epoch 894 | step 3 | loss: 0.0011428262558385242\n",
      "epoch 894 | step 4 | loss: 0.0014202975378287044\n",
      "epoch 894 | step 5 | loss: 0.001714213938811301\n",
      "epoch 894 | step 6 | loss: 0.0019968084337701414\n",
      "epoch 894 | step 7 | loss: 0.0022850470865166794\n",
      "epoch 894 | step 8 | loss: 0.002554724437243773\n",
      "epoch 894 | step 9 | loss: 0.002854443134409408\n",
      "epoch 894 | step 10 | loss: 0.003142729425608404\n",
      "epoch 894 | step 11 | loss: 0.003493269147252717\n",
      "epoch 895 | step 0 | loss: 0.0002590332443404992\n",
      "epoch 895 | step 1 | loss: 0.0005344881615802034\n",
      "epoch 895 | step 2 | loss: 0.0008347427434281391\n",
      "epoch 895 | step 3 | loss: 0.0011278092103923797\n",
      "epoch 895 | step 4 | loss: 0.0014282623974235594\n",
      "epoch 895 | step 5 | loss: 0.0017034283838877628\n",
      "epoch 895 | step 6 | loss: 0.002012425543473246\n",
      "epoch 895 | step 7 | loss: 0.0022975450616152205\n",
      "epoch 895 | step 8 | loss: 0.002602454553635514\n",
      "epoch 895 | step 9 | loss: 0.00288304954726686\n",
      "epoch 895 | step 10 | loss: 0.0032112943260537057\n",
      "epoch 895 | step 11 | loss: 0.0034662479043484186\n",
      "epoch 896 | step 0 | loss: 0.0002889952906551073\n",
      "epoch 896 | step 1 | loss: 0.0005877999063111657\n",
      "epoch 896 | step 2 | loss: 0.0008715341275632726\n",
      "epoch 896 | step 3 | loss: 0.0011350115812017339\n",
      "epoch 896 | step 4 | loss: 0.0014302762825130519\n",
      "epoch 896 | step 5 | loss: 0.0016925644534822014\n",
      "epoch 896 | step 6 | loss: 0.002001881709986409\n",
      "epoch 896 | step 7 | loss: 0.002297168155736278\n",
      "epoch 896 | step 8 | loss: 0.0025889601809046454\n",
      "epoch 896 | step 9 | loss: 0.0028871998675410126\n",
      "epoch 896 | step 10 | loss: 0.003186109749441031\n",
      "epoch 896 | step 11 | loss: 0.0034760216312584974\n",
      "epoch 897 | step 0 | loss: 0.0002763821060952538\n",
      "epoch 897 | step 1 | loss: 0.0005805766918917825\n",
      "epoch 897 | step 2 | loss: 0.0008895970941605318\n",
      "epoch 897 | step 3 | loss: 0.0011879233272544404\n",
      "epoch 897 | step 4 | loss: 0.0014771484777105627\n",
      "epoch 897 | step 5 | loss: 0.0017434182897227257\n",
      "epoch 897 | step 6 | loss: 0.0020129453067325017\n",
      "epoch 897 | step 7 | loss: 0.0022931415564904196\n",
      "epoch 897 | step 8 | loss: 0.002568227548296769\n",
      "epoch 897 | step 9 | loss: 0.0028579502661670447\n",
      "epoch 897 | step 10 | loss: 0.0031706340219240666\n",
      "epoch 897 | step 11 | loss: 0.0034817700863486985\n",
      "epoch 898 | step 0 | loss: 0.00028254114645528397\n",
      "epoch 898 | step 1 | loss: 0.0005594468582179002\n",
      "epoch 898 | step 2 | loss: 0.0008652294900262493\n",
      "epoch 898 | step 3 | loss: 0.0011670820041990728\n",
      "epoch 898 | step 4 | loss: 0.001468927394780248\n",
      "epoch 898 | step 5 | loss: 0.0017527204122396614\n",
      "epoch 898 | step 6 | loss: 0.002057188715764891\n",
      "epoch 898 | step 7 | loss: 0.0023314521176738302\n",
      "epoch 898 | step 8 | loss: 0.002609756497257599\n",
      "epoch 898 | step 9 | loss: 0.002896160545330937\n",
      "epoch 898 | step 10 | loss: 0.00318708534244172\n",
      "epoch 898 | step 11 | loss: 0.003475403874984973\n",
      "epoch 899 | step 0 | loss: 0.0002714113602934435\n",
      "epoch 899 | step 1 | loss: 0.0005664117075637008\n",
      "epoch 899 | step 2 | loss: 0.0008604993587607175\n",
      "epoch 899 | step 3 | loss: 0.0011410822217222207\n",
      "epoch 899 | step 4 | loss: 0.0014456265003216142\n",
      "epoch 899 | step 5 | loss: 0.001741565176849143\n",
      "epoch 899 | step 6 | loss: 0.002026289492623766\n",
      "epoch 899 | step 7 | loss: 0.002327488411159463\n",
      "epoch 899 | step 8 | loss: 0.002615490431217177\n",
      "epoch 899 | step 9 | loss: 0.0029131698540614532\n",
      "epoch 899 | step 10 | loss: 0.00320883452440881\n",
      "epoch 899 | step 11 | loss: 0.0034666552365029977\n",
      "epoch 900 | step 0 | loss: 0.000287991291187449\n",
      "epoch 900 | step 1 | loss: 0.0005984609360003505\n",
      "epoch 900 | step 2 | loss: 0.0008634789815707277\n",
      "epoch 900 | step 3 | loss: 0.0011515407345400088\n",
      "epoch 900 | step 4 | loss: 0.0014342841555623285\n",
      "epoch 900 | step 5 | loss: 0.0017474483298856344\n",
      "epoch 900 | step 6 | loss: 0.002025332429375733\n",
      "epoch 900 | step 7 | loss: 0.002327707774166459\n",
      "epoch 900 | step 8 | loss: 0.0026284587717287523\n",
      "epoch 900 | step 9 | loss: 0.002912245956933774\n",
      "epoch 900 | step 10 | loss: 0.003193290811535174\n",
      "epoch 900 | step 11 | loss: 0.003472574754049608\n",
      "epoch 901 | step 0 | loss: 0.0002779310546403688\n",
      "epoch 901 | step 1 | loss: 0.0005570764510576208\n",
      "epoch 901 | step 2 | loss: 0.0008607275856971671\n",
      "epoch 901 | step 3 | loss: 0.001140428976645503\n",
      "epoch 901 | step 4 | loss: 0.0014075530071829341\n",
      "epoch 901 | step 5 | loss: 0.0017027360475465447\n",
      "epoch 901 | step 6 | loss: 0.002005143787188675\n",
      "epoch 901 | step 7 | loss: 0.0023254186367524412\n",
      "epoch 901 | step 8 | loss: 0.0026118035614491287\n",
      "epoch 901 | step 9 | loss: 0.0029128518671463605\n",
      "epoch 901 | step 10 | loss: 0.0031822197735995065\n",
      "epoch 901 | step 11 | loss: 0.0034769469679378196\n",
      "epoch 902 | step 0 | loss: 0.000292160777258762\n",
      "epoch 902 | step 1 | loss: 0.0005897349125502907\n",
      "epoch 902 | step 2 | loss: 0.0008672489179349149\n",
      "epoch 902 | step 3 | loss: 0.0011591016654875536\n",
      "epoch 902 | step 4 | loss: 0.0014463755708635717\n",
      "epoch 902 | step 5 | loss: 0.001715593077119979\n",
      "epoch 902 | step 6 | loss: 0.0020215468922356373\n",
      "epoch 902 | step 7 | loss: 0.002329120919243939\n",
      "epoch 902 | step 8 | loss: 0.0026256792347197073\n",
      "epoch 902 | step 9 | loss: 0.0028929389699495515\n",
      "epoch 902 | step 10 | loss: 0.0031788669619105467\n",
      "epoch 902 | step 11 | loss: 0.0034779941661247614\n",
      "epoch 903 | step 0 | loss: 0.0002914315468481441\n",
      "epoch 903 | step 1 | loss: 0.0005808685799381279\n",
      "epoch 903 | step 2 | loss: 0.0008713488159349792\n",
      "epoch 903 | step 3 | loss: 0.0011705978559854336\n",
      "epoch 903 | step 4 | loss: 0.0014452763391403305\n",
      "epoch 903 | step 5 | loss: 0.0017415057976750941\n",
      "epoch 903 | step 6 | loss: 0.002041024409375329\n",
      "epoch 903 | step 7 | loss: 0.0023243178377324353\n",
      "epoch 903 | step 8 | loss: 0.0026355586444563968\n",
      "epoch 903 | step 9 | loss: 0.0029332582076772445\n",
      "epoch 903 | step 10 | loss: 0.003202606114034236\n",
      "epoch 903 | step 11 | loss: 0.003468630012704096\n",
      "epoch 904 | step 0 | loss: 0.0003245553710355039\n",
      "epoch 904 | step 1 | loss: 0.0006280594207669899\n",
      "epoch 904 | step 2 | loss: 0.0009176444119275076\n",
      "epoch 904 | step 3 | loss: 0.0011843417140061994\n",
      "epoch 904 | step 4 | loss: 0.0014815880401317949\n",
      "epoch 904 | step 5 | loss: 0.0017729737496559085\n",
      "epoch 904 | step 6 | loss: 0.002060240193958099\n",
      "epoch 904 | step 7 | loss: 0.0023700185544978077\n",
      "epoch 904 | step 8 | loss: 0.0026376660650018796\n",
      "epoch 904 | step 9 | loss: 0.00292646108256192\n",
      "epoch 904 | step 10 | loss: 0.003211348546309579\n",
      "epoch 904 | step 11 | loss: 0.00346512291273189\n",
      "epoch 905 | step 0 | loss: 0.0003085778425679809\n",
      "epoch 905 | step 1 | loss: 0.0005967067805892443\n",
      "epoch 905 | step 2 | loss: 0.0008808221224142325\n",
      "epoch 905 | step 3 | loss: 0.0011749897112618881\n",
      "epoch 905 | step 4 | loss: 0.0014575583417662339\n",
      "epoch 905 | step 5 | loss: 0.0017400181528225665\n",
      "epoch 905 | step 6 | loss: 0.002015144125797618\n",
      "epoch 905 | step 7 | loss: 0.002312062054687155\n",
      "epoch 905 | step 8 | loss: 0.0025991102130071128\n",
      "epoch 905 | step 9 | loss: 0.0028829947122310474\n",
      "epoch 905 | step 10 | loss: 0.0031645746611165828\n",
      "epoch 905 | step 11 | loss: 0.0034831368132399202\n",
      "epoch 906 | step 0 | loss: 0.0003024682379808465\n",
      "epoch 906 | step 1 | loss: 0.0005596824200347889\n",
      "epoch 906 | step 2 | loss: 0.0008644811461811409\n",
      "epoch 906 | step 3 | loss: 0.0011721695230001653\n",
      "epoch 906 | step 4 | loss: 0.0014710712558452844\n",
      "epoch 906 | step 5 | loss: 0.0017708671152789623\n",
      "epoch 906 | step 6 | loss: 0.002042819848526537\n",
      "epoch 906 | step 7 | loss: 0.002297727196682816\n",
      "epoch 906 | step 8 | loss: 0.0025996268179174138\n",
      "epoch 906 | step 9 | loss: 0.0028813958269646176\n",
      "epoch 906 | step 10 | loss: 0.003176944117034073\n",
      "epoch 906 | step 11 | loss: 0.0034783783726457422\n",
      "epoch 907 | step 0 | loss: 0.000294944394910359\n",
      "epoch 907 | step 1 | loss: 0.0005833304537653523\n",
      "epoch 907 | step 2 | loss: 0.0008411954738968743\n",
      "epoch 907 | step 3 | loss: 0.0011326824612665604\n",
      "epoch 907 | step 4 | loss: 0.0014428518600228962\n",
      "epoch 907 | step 5 | loss: 0.0017222687222141653\n",
      "epoch 907 | step 6 | loss: 0.0020104328782871135\n",
      "epoch 907 | step 7 | loss: 0.002303757182321043\n",
      "epoch 907 | step 8 | loss: 0.0025843155792810066\n",
      "epoch 907 | step 9 | loss: 0.0028702777424509046\n",
      "epoch 907 | step 10 | loss: 0.0031724229544963798\n",
      "epoch 907 | step 11 | loss: 0.0034799748240377897\n",
      "epoch 908 | step 0 | loss: 0.00026692528788933867\n",
      "epoch 908 | step 1 | loss: 0.0005696604446871245\n",
      "epoch 908 | step 2 | loss: 0.0008696258405405603\n",
      "epoch 908 | step 3 | loss: 0.0011621465374193037\n",
      "epoch 908 | step 4 | loss: 0.0014507212619406557\n",
      "epoch 908 | step 5 | loss: 0.0017390485901449624\n",
      "epoch 908 | step 6 | loss: 0.0020327017498173597\n",
      "epoch 908 | step 7 | loss: 0.0023281443724715235\n",
      "epoch 908 | step 8 | loss: 0.0026102010926635335\n",
      "epoch 908 | step 9 | loss: 0.002900056331911237\n",
      "epoch 908 | step 10 | loss: 0.0032048148435377807\n",
      "epoch 908 | step 11 | loss: 0.0034672364939589257\n",
      "epoch 909 | step 0 | loss: 0.00029416684417898675\n",
      "epoch 909 | step 1 | loss: 0.0005753435980205997\n",
      "epoch 909 | step 2 | loss: 0.000879490614015232\n",
      "epoch 909 | step 3 | loss: 0.0011479891783950148\n",
      "epoch 909 | step 4 | loss: 0.0014313979412612136\n",
      "epoch 909 | step 5 | loss: 0.0017206561513297496\n",
      "epoch 909 | step 6 | loss: 0.0019970681554057\n",
      "epoch 909 | step 7 | loss: 0.002301506232572526\n",
      "epoch 909 | step 8 | loss: 0.0025967227342601575\n",
      "epoch 909 | step 9 | loss: 0.002904418278694147\n",
      "epoch 909 | step 10 | loss: 0.003183551498272384\n",
      "epoch 909 | step 11 | loss: 0.003475545102183731\n",
      "epoch 910 | step 0 | loss: 0.00025810869762195744\n",
      "epoch 910 | step 1 | loss: 0.0005629976801560185\n",
      "epoch 910 | step 2 | loss: 0.0008485403219775745\n",
      "epoch 910 | step 3 | loss: 0.0011549737366163086\n",
      "epoch 910 | step 4 | loss: 0.0014435780308126704\n",
      "epoch 910 | step 5 | loss: 0.0017614458596181882\n",
      "epoch 910 | step 6 | loss: 0.0020311354819088274\n",
      "epoch 910 | step 7 | loss: 0.002320720315708287\n",
      "epoch 910 | step 8 | loss: 0.002611421248682759\n",
      "epoch 910 | step 9 | loss: 0.002904554536084627\n",
      "epoch 910 | step 10 | loss: 0.0032053405780229686\n",
      "epoch 910 | step 11 | loss: 0.0034666521372113007\n",
      "epoch 911 | step 0 | loss: 0.00027412386743151977\n",
      "epoch 911 | step 1 | loss: 0.0005693773366626061\n",
      "epoch 911 | step 2 | loss: 0.0008294751586675619\n",
      "epoch 911 | step 3 | loss: 0.0011063013434361034\n",
      "epoch 911 | step 4 | loss: 0.0014221657219553236\n",
      "epoch 911 | step 5 | loss: 0.0017222297236801514\n",
      "epoch 911 | step 6 | loss: 0.0020013243997618353\n",
      "epoch 911 | step 7 | loss: 0.002300571156396469\n",
      "epoch 911 | step 8 | loss: 0.0025732253791816927\n",
      "epoch 911 | step 9 | loss: 0.002855781030340643\n",
      "epoch 911 | step 10 | loss: 0.0031592061566747417\n",
      "epoch 911 | step 11 | loss: 0.003484678242624526\n",
      "epoch 912 | step 0 | loss: 0.00026538747949513995\n",
      "epoch 912 | step 1 | loss: 0.000565427849913278\n",
      "epoch 912 | step 2 | loss: 0.0008487133563874295\n",
      "epoch 912 | step 3 | loss: 0.0011519065255320767\n",
      "epoch 912 | step 4 | loss: 0.00147908360447296\n",
      "epoch 912 | step 5 | loss: 0.001769213203538987\n",
      "epoch 912 | step 6 | loss: 0.0020440898985006027\n",
      "epoch 912 | step 7 | loss: 0.0023300691364791593\n",
      "epoch 912 | step 8 | loss: 0.0026160108436361637\n",
      "epoch 912 | step 9 | loss: 0.0029113861628246086\n",
      "epoch 912 | step 10 | loss: 0.0031923409459673354\n",
      "epoch 912 | step 11 | loss: 0.0034717076863261358\n",
      "epoch 913 | step 0 | loss: 0.0002870022366802582\n",
      "epoch 913 | step 1 | loss: 0.0005712559853548532\n",
      "epoch 913 | step 2 | loss: 0.0008816081952356768\n",
      "epoch 913 | step 3 | loss: 0.0011358640179394389\n",
      "epoch 913 | step 4 | loss: 0.0014057580640010486\n",
      "epoch 913 | step 5 | loss: 0.001682863506498032\n",
      "epoch 913 | step 6 | loss: 0.0019882123922237717\n",
      "epoch 913 | step 7 | loss: 0.0022752327418319894\n",
      "epoch 913 | step 8 | loss: 0.0025631899384167146\n",
      "epoch 913 | step 9 | loss: 0.002868902689811569\n",
      "epoch 913 | step 10 | loss: 0.003174550565420845\n",
      "epoch 913 | step 11 | loss: 0.0034785035256045442\n",
      "epoch 914 | step 0 | loss: 0.00028624508068886756\n",
      "epoch 914 | step 1 | loss: 0.0005703695544942218\n",
      "epoch 914 | step 2 | loss: 0.0008503933820778731\n",
      "epoch 914 | step 3 | loss: 0.0011430689141306571\n",
      "epoch 914 | step 4 | loss: 0.001408546497261704\n",
      "epoch 914 | step 5 | loss: 0.0016952332796543311\n",
      "epoch 914 | step 6 | loss: 0.002005566480506927\n",
      "epoch 914 | step 7 | loss: 0.0023162887950299764\n",
      "epoch 914 | step 8 | loss: 0.002596282946566956\n",
      "epoch 914 | step 9 | loss: 0.002885049303804169\n",
      "epoch 914 | step 10 | loss: 0.003168167893662767\n",
      "epoch 914 | step 11 | loss: 0.0034809249983815127\n",
      "epoch 915 | step 0 | loss: 0.00028165226348284656\n",
      "epoch 915 | step 1 | loss: 0.0005600202749784322\n",
      "epoch 915 | step 2 | loss: 0.0008410693431643634\n",
      "epoch 915 | step 3 | loss: 0.0011280287826348607\n",
      "epoch 915 | step 4 | loss: 0.0014043360401275631\n",
      "epoch 915 | step 5 | loss: 0.0017146064524324156\n",
      "epoch 915 | step 6 | loss: 0.0020005890652760165\n",
      "epoch 915 | step 7 | loss: 0.002279060683587011\n",
      "epoch 915 | step 8 | loss: 0.0026044564219279325\n",
      "epoch 915 | step 9 | loss: 0.002902037341947203\n",
      "epoch 915 | step 10 | loss: 0.003192026705729086\n",
      "epoch 915 | step 11 | loss: 0.003471245815899614\n",
      "epoch 916 | step 0 | loss: 0.0002939957659424527\n",
      "epoch 916 | step 1 | loss: 0.0005907844991141314\n",
      "epoch 916 | step 2 | loss: 0.0008836209857292394\n",
      "epoch 916 | step 3 | loss: 0.0011819823166655935\n",
      "epoch 916 | step 4 | loss: 0.001446190640421294\n",
      "epoch 916 | step 5 | loss: 0.0017327855683570135\n",
      "epoch 916 | step 6 | loss: 0.002050390093518293\n",
      "epoch 916 | step 7 | loss: 0.0023424740967090086\n",
      "epoch 916 | step 8 | loss: 0.002633337342852939\n",
      "epoch 916 | step 9 | loss: 0.002917834080489261\n",
      "epoch 916 | step 10 | loss: 0.003193762953276111\n",
      "epoch 916 | step 11 | loss: 0.003470655383316872\n",
      "epoch 917 | step 0 | loss: 0.0003041296174981424\n",
      "epoch 917 | step 1 | loss: 0.000594882130487984\n",
      "epoch 917 | step 2 | loss: 0.0008683376768268183\n",
      "epoch 917 | step 3 | loss: 0.0011494504964807899\n",
      "epoch 917 | step 4 | loss: 0.0014314060905169057\n",
      "epoch 917 | step 5 | loss: 0.0017191272968393398\n",
      "epoch 917 | step 6 | loss: 0.0020117287152370354\n",
      "epoch 917 | step 7 | loss: 0.00232125466955519\n",
      "epoch 917 | step 8 | loss: 0.0026097159943046567\n",
      "epoch 917 | step 9 | loss: 0.002909253290770155\n",
      "epoch 917 | step 10 | loss: 0.0031687446734121895\n",
      "epoch 917 | step 11 | loss: 0.0034804818061511646\n",
      "epoch 918 | step 0 | loss: 0.0002726219336706543\n",
      "epoch 918 | step 1 | loss: 0.0005734433302833282\n",
      "epoch 918 | step 2 | loss: 0.0008580805202292871\n",
      "epoch 918 | step 3 | loss: 0.0011654643729614577\n",
      "epoch 918 | step 4 | loss: 0.001470748281873689\n",
      "epoch 918 | step 5 | loss: 0.0017446394316648086\n",
      "epoch 918 | step 6 | loss: 0.002031040316946496\n",
      "epoch 918 | step 7 | loss: 0.002304020203693315\n",
      "epoch 918 | step 8 | loss: 0.002597363699513395\n",
      "epoch 918 | step 9 | loss: 0.0029011072073912574\n",
      "epoch 918 | step 10 | loss: 0.0031966011873420233\n",
      "epoch 918 | step 11 | loss: 0.0034690449012640115\n",
      "epoch 919 | step 0 | loss: 0.00029879588345423227\n",
      "epoch 919 | step 1 | loss: 0.0005717263097549686\n",
      "epoch 919 | step 2 | loss: 0.0008676761046882217\n",
      "epoch 919 | step 3 | loss: 0.0011364167531405143\n",
      "epoch 919 | step 4 | loss: 0.0014108626807847898\n",
      "epoch 919 | step 5 | loss: 0.0016977724037031358\n",
      "epoch 919 | step 6 | loss: 0.002014887130067293\n",
      "epoch 919 | step 7 | loss: 0.002291416871899489\n",
      "epoch 919 | step 8 | loss: 0.0025633665752948166\n",
      "epoch 919 | step 9 | loss: 0.0028435211027794064\n",
      "epoch 919 | step 10 | loss: 0.0031632708193960316\n",
      "epoch 919 | step 11 | loss: 0.0034821206424462724\n",
      "epoch 920 | step 0 | loss: 0.00028433678926316383\n",
      "epoch 920 | step 1 | loss: 0.0005822915336733739\n",
      "epoch 920 | step 2 | loss: 0.000864850968642346\n",
      "epoch 920 | step 3 | loss: 0.0011757668455716368\n",
      "epoch 920 | step 4 | loss: 0.001462623323721783\n",
      "epoch 920 | step 5 | loss: 0.0017273572706296143\n",
      "epoch 920 | step 6 | loss: 0.0020088125228960607\n",
      "epoch 920 | step 7 | loss: 0.0023106092626135975\n",
      "epoch 920 | step 8 | loss: 0.0025954418026346284\n",
      "epoch 920 | step 9 | loss: 0.0028818552150834758\n",
      "epoch 920 | step 10 | loss: 0.0031749734445391505\n",
      "epoch 920 | step 11 | loss: 0.003477412933510974\n",
      "epoch 921 | step 0 | loss: 0.000264583503854536\n",
      "epoch 921 | step 1 | loss: 0.0005455983322995531\n",
      "epoch 921 | step 2 | loss: 0.0008444012989305378\n",
      "epoch 921 | step 3 | loss: 0.0011529280714343757\n",
      "epoch 921 | step 4 | loss: 0.0014727381232707373\n",
      "epoch 921 | step 5 | loss: 0.0017350198009694912\n",
      "epoch 921 | step 6 | loss: 0.0020236481368084516\n",
      "epoch 921 | step 7 | loss: 0.0023226328568145766\n",
      "epoch 921 | step 8 | loss: 0.0026392455887649124\n",
      "epoch 921 | step 9 | loss: 0.002915700110070342\n",
      "epoch 921 | step 10 | loss: 0.0031989665558021705\n",
      "epoch 921 | step 11 | loss: 0.0034679260688860397\n",
      "epoch 922 | step 0 | loss: 0.000277965284667859\n",
      "epoch 922 | step 1 | loss: 0.0005506248984355996\n",
      "epoch 922 | step 2 | loss: 0.000848723874273519\n",
      "epoch 922 | step 3 | loss: 0.0011619879019620226\n",
      "epoch 922 | step 4 | loss: 0.001459110114685802\n",
      "epoch 922 | step 5 | loss: 0.0017431235381996598\n",
      "epoch 922 | step 6 | loss: 0.002034620147159408\n",
      "epoch 922 | step 7 | loss: 0.0022864815030291\n",
      "epoch 922 | step 8 | loss: 0.0025613410001320484\n",
      "epoch 922 | step 9 | loss: 0.0028528809831122976\n",
      "epoch 922 | step 10 | loss: 0.0031468159188622015\n",
      "epoch 922 | step 11 | loss: 0.0034882056499607435\n",
      "epoch 923 | step 0 | loss: 0.000285005821452109\n",
      "epoch 923 | step 1 | loss: 0.0005993338625043848\n",
      "epoch 923 | step 2 | loss: 0.0008764840924537353\n",
      "epoch 923 | step 3 | loss: 0.0011920854549228167\n",
      "epoch 923 | step 4 | loss: 0.0014599455474937357\n",
      "epoch 923 | step 5 | loss: 0.001752720068111585\n",
      "epoch 923 | step 6 | loss: 0.002020743851509174\n",
      "epoch 923 | step 7 | loss: 0.0022939706670384676\n",
      "epoch 923 | step 8 | loss: 0.0025878117297192175\n",
      "epoch 923 | step 9 | loss: 0.0028917010863405493\n",
      "epoch 923 | step 10 | loss: 0.0032161585889416465\n",
      "epoch 923 | step 11 | loss: 0.0034609368900899093\n",
      "epoch 924 | step 0 | loss: 0.00029088615375983887\n",
      "epoch 924 | step 1 | loss: 0.0005846799370675842\n",
      "epoch 924 | step 2 | loss: 0.000864203953493079\n",
      "epoch 924 | step 3 | loss: 0.001137470688553068\n",
      "epoch 924 | step 4 | loss: 0.0014379720530903666\n",
      "epoch 924 | step 5 | loss: 0.0017213961831299394\n",
      "epoch 924 | step 6 | loss: 0.001999486528979788\n",
      "epoch 924 | step 7 | loss: 0.002291041027205182\n",
      "epoch 924 | step 8 | loss: 0.002583787182742339\n",
      "epoch 924 | step 9 | loss: 0.0029041895564043615\n",
      "epoch 924 | step 10 | loss: 0.003191687674037367\n",
      "epoch 924 | step 11 | loss: 0.0034704691067754465\n",
      "epoch 925 | step 0 | loss: 0.0003128205916214117\n",
      "epoch 925 | step 1 | loss: 0.000583307137094863\n",
      "epoch 925 | step 2 | loss: 0.0008686838390515874\n",
      "epoch 925 | step 3 | loss: 0.0011730602791435874\n",
      "epoch 925 | step 4 | loss: 0.001473782362504059\n",
      "epoch 925 | step 5 | loss: 0.0017542123779447755\n",
      "epoch 925 | step 6 | loss: 0.0020313710331596596\n",
      "epoch 925 | step 7 | loss: 0.0023150960408469612\n",
      "epoch 925 | step 8 | loss: 0.0025897138440974447\n",
      "epoch 925 | step 9 | loss: 0.002882375213752711\n",
      "epoch 925 | step 10 | loss: 0.0031754744796952373\n",
      "epoch 925 | step 11 | loss: 0.0034770105951315113\n",
      "epoch 926 | step 0 | loss: 0.00027599849304781667\n",
      "epoch 926 | step 1 | loss: 0.0005357991257246224\n",
      "epoch 926 | step 2 | loss: 0.0008249622009632292\n",
      "epoch 926 | step 3 | loss: 0.0011023805663960403\n",
      "epoch 926 | step 4 | loss: 0.0013963376395287267\n",
      "epoch 926 | step 5 | loss: 0.0016841264360710844\n",
      "epoch 926 | step 6 | loss: 0.0019816643558535386\n",
      "epoch 926 | step 7 | loss: 0.0022912068775356033\n",
      "epoch 926 | step 8 | loss: 0.0025810157521208845\n",
      "epoch 926 | step 9 | loss: 0.002898471279834519\n",
      "epoch 926 | step 10 | loss: 0.0031962911023608416\n",
      "epoch 926 | step 11 | loss: 0.0034685862697528607\n",
      "epoch 927 | step 0 | loss: 0.0002730426787623935\n",
      "epoch 927 | step 1 | loss: 0.0005375905836161655\n",
      "epoch 927 | step 2 | loss: 0.0008405010968023684\n",
      "epoch 927 | step 3 | loss: 0.0011227764332478929\n",
      "epoch 927 | step 4 | loss: 0.001404275961095641\n",
      "epoch 927 | step 5 | loss: 0.0017054899651797156\n",
      "epoch 927 | step 6 | loss: 0.002010559879077949\n",
      "epoch 927 | step 7 | loss: 0.002320031725805397\n",
      "epoch 927 | step 8 | loss: 0.0026001087737779287\n",
      "epoch 927 | step 9 | loss: 0.0028983443569096314\n",
      "epoch 927 | step 10 | loss: 0.0031895173676714968\n",
      "epoch 927 | step 11 | loss: 0.00347067942881907\n",
      "epoch 928 | step 0 | loss: 0.0003143758603713958\n",
      "epoch 928 | step 1 | loss: 0.0006155131919182698\n",
      "epoch 928 | step 2 | loss: 0.0008995286394809816\n",
      "epoch 928 | step 3 | loss: 0.0011543203999894325\n",
      "epoch 928 | step 4 | loss: 0.0014667617331773045\n",
      "epoch 928 | step 5 | loss: 0.0017634421295718793\n",
      "epoch 928 | step 6 | loss: 0.0020584583218989606\n",
      "epoch 928 | step 7 | loss: 0.0023646095352658747\n",
      "epoch 928 | step 8 | loss: 0.0026296576983384744\n",
      "epoch 928 | step 9 | loss: 0.0029241514053353584\n",
      "epoch 928 | step 10 | loss: 0.0031971389946943533\n",
      "epoch 928 | step 11 | loss: 0.0034678667271586363\n",
      "epoch 929 | step 0 | loss: 0.00027569591441025645\n",
      "epoch 929 | step 1 | loss: 0.0005544411418852525\n",
      "epoch 929 | step 2 | loss: 0.0008483175156187399\n",
      "epoch 929 | step 3 | loss: 0.0011691053284958689\n",
      "epoch 929 | step 4 | loss: 0.0014558162313993068\n",
      "epoch 929 | step 5 | loss: 0.001709175815050122\n",
      "epoch 929 | step 6 | loss: 0.0019965428094913177\n",
      "epoch 929 | step 7 | loss: 0.002268148243471925\n",
      "epoch 929 | step 8 | loss: 0.0025482770318444427\n",
      "epoch 929 | step 9 | loss: 0.002847901077801193\n",
      "epoch 929 | step 10 | loss: 0.0031675364941954345\n",
      "epoch 929 | step 11 | loss: 0.0034792183084773097\n",
      "epoch 930 | step 0 | loss: 0.0002672972708140305\n",
      "epoch 930 | step 1 | loss: 0.0005482344694734525\n",
      "epoch 930 | step 2 | loss: 0.000830189402336692\n",
      "epoch 930 | step 3 | loss: 0.0011470464402023308\n",
      "epoch 930 | step 4 | loss: 0.0014366310496574206\n",
      "epoch 930 | step 5 | loss: 0.0017402405608968254\n",
      "epoch 930 | step 6 | loss: 0.00202705933824341\n",
      "epoch 930 | step 7 | loss: 0.0023086394587625418\n",
      "epoch 930 | step 8 | loss: 0.0025964621706992055\n",
      "epoch 930 | step 9 | loss: 0.0028890343549449215\n",
      "epoch 930 | step 10 | loss: 0.0031903761076196835\n",
      "epoch 930 | step 11 | loss: 0.0034702276462246127\n",
      "epoch 931 | step 0 | loss: 0.00028352421937387985\n",
      "epoch 931 | step 1 | loss: 0.0005627978562699936\n",
      "epoch 931 | step 2 | loss: 0.0008512851914693366\n",
      "epoch 931 | step 3 | loss: 0.0011575362916720448\n",
      "epoch 931 | step 4 | loss: 0.001475070501152115\n",
      "epoch 931 | step 5 | loss: 0.0017479317601818618\n",
      "epoch 931 | step 6 | loss: 0.0020370163040900518\n",
      "epoch 931 | step 7 | loss: 0.00233333118508477\n",
      "epoch 931 | step 8 | loss: 0.002640984803109555\n",
      "epoch 931 | step 9 | loss: 0.002916925668629001\n",
      "epoch 931 | step 10 | loss: 0.003216687316723618\n",
      "epoch 931 | step 11 | loss: 0.003459886051842098\n",
      "epoch 932 | step 0 | loss: 0.0002854417628294379\n",
      "epoch 932 | step 1 | loss: 0.0005503098522670758\n",
      "epoch 932 | step 2 | loss: 0.0008506358420392824\n",
      "epoch 932 | step 3 | loss: 0.001153316328596339\n",
      "epoch 932 | step 4 | loss: 0.001434449204666889\n",
      "epoch 932 | step 5 | loss: 0.0017227075336119653\n",
      "epoch 932 | step 6 | loss: 0.002008764487454209\n",
      "epoch 932 | step 7 | loss: 0.002305820269820214\n",
      "epoch 932 | step 8 | loss: 0.0026041248820817117\n",
      "epoch 932 | step 9 | loss: 0.002894444950219981\n",
      "epoch 932 | step 10 | loss: 0.0031823033409075427\n",
      "epoch 932 | step 11 | loss: 0.0034735400954686536\n",
      "epoch 933 | step 0 | loss: 0.0002954470783450162\n",
      "epoch 933 | step 1 | loss: 0.0005871858932856861\n",
      "epoch 933 | step 2 | loss: 0.000880011977785447\n",
      "epoch 933 | step 3 | loss: 0.0011480215166983978\n",
      "epoch 933 | step 4 | loss: 0.001473453918405041\n",
      "epoch 933 | step 5 | loss: 0.001779938077427095\n",
      "epoch 933 | step 6 | loss: 0.0020591911930805023\n",
      "epoch 933 | step 7 | loss: 0.0023366000818747677\n",
      "epoch 933 | step 8 | loss: 0.002615620643732438\n",
      "epoch 933 | step 9 | loss: 0.002875764283118439\n",
      "epoch 933 | step 10 | loss: 0.003174668892310803\n",
      "epoch 933 | step 11 | loss: 0.0034759157139473856\n",
      "epoch 934 | step 0 | loss: 0.0002746093361331343\n",
      "epoch 934 | step 1 | loss: 0.0005443166193500711\n",
      "epoch 934 | step 2 | loss: 0.0008292798395359626\n",
      "epoch 934 | step 3 | loss: 0.0011420998761105517\n",
      "epoch 934 | step 4 | loss: 0.0014182581645865102\n",
      "epoch 934 | step 5 | loss: 0.0016939272804021438\n",
      "epoch 934 | step 6 | loss: 0.001997171822948813\n",
      "epoch 934 | step 7 | loss: 0.0022891443231951884\n",
      "epoch 934 | step 8 | loss: 0.00257970097921713\n",
      "epoch 934 | step 9 | loss: 0.002864234635620331\n",
      "epoch 934 | step 10 | loss: 0.003175649642711715\n",
      "epoch 934 | step 11 | loss: 0.003475661705461524\n",
      "epoch 935 | step 0 | loss: 0.00027113480489943185\n",
      "epoch 935 | step 1 | loss: 0.0005648866141507497\n",
      "epoch 935 | step 2 | loss: 0.0008598330240181272\n",
      "epoch 935 | step 3 | loss: 0.0011478699526509813\n",
      "epoch 935 | step 4 | loss: 0.0014275716663039245\n",
      "epoch 935 | step 5 | loss: 0.0017154869763388234\n",
      "epoch 935 | step 6 | loss: 0.0019997266809143655\n",
      "epoch 935 | step 7 | loss: 0.002324818918018545\n",
      "epoch 935 | step 8 | loss: 0.0026130590398917566\n",
      "epoch 935 | step 9 | loss: 0.0029096681491481424\n",
      "epoch 935 | step 10 | loss: 0.0032107093617969097\n",
      "epoch 935 | step 11 | loss: 0.0034615325382729967\n",
      "epoch 936 | step 0 | loss: 0.0003215505156585888\n",
      "epoch 936 | step 1 | loss: 0.0006173270807103185\n",
      "epoch 936 | step 2 | loss: 0.0009029351273024224\n",
      "epoch 936 | step 3 | loss: 0.0011841780596193376\n",
      "epoch 936 | step 4 | loss: 0.0014586200842836715\n",
      "epoch 936 | step 5 | loss: 0.0017326694205654462\n",
      "epoch 936 | step 6 | loss: 0.002009578349984623\n",
      "epoch 936 | step 7 | loss: 0.002328599137022208\n",
      "epoch 936 | step 8 | loss: 0.0026090274090784316\n",
      "epoch 936 | step 9 | loss: 0.0029047916860867216\n",
      "epoch 936 | step 10 | loss: 0.003184592425856136\n",
      "epoch 936 | step 11 | loss: 0.0034723285921987707\n",
      "epoch 937 | step 0 | loss: 0.00031237821293782534\n",
      "epoch 937 | step 1 | loss: 0.0005953607050947907\n",
      "epoch 937 | step 2 | loss: 0.0008989518300668156\n",
      "epoch 937 | step 3 | loss: 0.001200835797324002\n",
      "epoch 937 | step 4 | loss: 0.0014989516663578416\n",
      "epoch 937 | step 5 | loss: 0.0017962369453744785\n",
      "epoch 937 | step 6 | loss: 0.002069487136137418\n",
      "epoch 937 | step 7 | loss: 0.002333300845371459\n",
      "epoch 937 | step 8 | loss: 0.002622791578829821\n",
      "epoch 937 | step 9 | loss: 0.0029205236769866702\n",
      "epoch 937 | step 10 | loss: 0.0032037507020570843\n",
      "epoch 937 | step 11 | loss: 0.0034644322567607675\n",
      "epoch 938 | step 0 | loss: 0.00026544990521742935\n",
      "epoch 938 | step 1 | loss: 0.0005472820335850946\n",
      "epoch 938 | step 2 | loss: 0.0008380609816671499\n",
      "epoch 938 | step 3 | loss: 0.0011333404101637281\n",
      "epoch 938 | step 4 | loss: 0.0014316361594739625\n",
      "epoch 938 | step 5 | loss: 0.0016831372935180462\n",
      "epoch 938 | step 6 | loss: 0.001999866566068606\n",
      "epoch 938 | step 7 | loss: 0.00228491018532877\n",
      "epoch 938 | step 8 | loss: 0.002591062281044254\n",
      "epoch 938 | step 9 | loss: 0.0028553952149528218\n",
      "epoch 938 | step 10 | loss: 0.0031665976679472957\n",
      "epoch 938 | step 11 | loss: 0.00347835141404249\n",
      "epoch 939 | step 0 | loss: 0.00030810439554658427\n",
      "epoch 939 | step 1 | loss: 0.000601025196596441\n",
      "epoch 939 | step 2 | loss: 0.0009099612408852168\n",
      "epoch 939 | step 3 | loss: 0.0012176213699701755\n",
      "epoch 939 | step 4 | loss: 0.0015105726016368802\n",
      "epoch 939 | step 5 | loss: 0.001795616602317337\n",
      "epoch 939 | step 6 | loss: 0.0020614184737190394\n",
      "epoch 939 | step 7 | loss: 0.002334031759315153\n",
      "epoch 939 | step 8 | loss: 0.0026132511468072574\n",
      "epoch 939 | step 9 | loss: 0.0029041141681146786\n",
      "epoch 939 | step 10 | loss: 0.0031909898389396183\n",
      "epoch 939 | step 11 | loss: 0.003468855336068056\n",
      "epoch 940 | step 0 | loss: 0.0002866682218323036\n",
      "epoch 940 | step 1 | loss: 0.0005820141185957694\n",
      "epoch 940 | step 2 | loss: 0.0008589051775326035\n",
      "epoch 940 | step 3 | loss: 0.001169155637791773\n",
      "epoch 940 | step 4 | loss: 0.001442042965261435\n",
      "epoch 940 | step 5 | loss: 0.0017230103305938712\n",
      "epoch 940 | step 6 | loss: 0.0020086148152949792\n",
      "epoch 940 | step 7 | loss: 0.0022868816847112345\n",
      "epoch 940 | step 8 | loss: 0.0025825151617549997\n",
      "epoch 940 | step 9 | loss: 0.0028998507285972488\n",
      "epoch 940 | step 10 | loss: 0.003182378674109017\n",
      "epoch 940 | step 11 | loss: 0.0034723336158422797\n",
      "epoch 941 | step 0 | loss: 0.0003143731391892629\n",
      "epoch 941 | step 1 | loss: 0.0006050079040504011\n",
      "epoch 941 | step 2 | loss: 0.000901618992435676\n",
      "epoch 941 | step 3 | loss: 0.0011918134806472584\n",
      "epoch 941 | step 4 | loss: 0.0014772230740030072\n",
      "epoch 941 | step 5 | loss: 0.0017737309454718068\n",
      "epoch 941 | step 6 | loss: 0.0020703311734393396\n",
      "epoch 941 | step 7 | loss: 0.0023454927025485764\n",
      "epoch 941 | step 8 | loss: 0.0026333470370525688\n",
      "epoch 941 | step 9 | loss: 0.0029224635135029755\n",
      "epoch 941 | step 10 | loss: 0.003202825674946907\n",
      "epoch 941 | step 11 | loss: 0.0034641479470784815\n",
      "epoch 942 | step 0 | loss: 0.0003013644653231338\n",
      "epoch 942 | step 1 | loss: 0.0005787986403512429\n",
      "epoch 942 | step 2 | loss: 0.0008760881693216337\n",
      "epoch 942 | step 3 | loss: 0.001144483335150185\n",
      "epoch 942 | step 4 | loss: 0.0014224745959335906\n",
      "epoch 942 | step 5 | loss: 0.0017226635840123464\n",
      "epoch 942 | step 6 | loss: 0.0020223829003891873\n",
      "epoch 942 | step 7 | loss: 0.0023116634124685986\n",
      "epoch 942 | step 8 | loss: 0.002602124579265102\n",
      "epoch 942 | step 9 | loss: 0.0028961082006325333\n",
      "epoch 942 | step 10 | loss: 0.003176749885390255\n",
      "epoch 942 | step 11 | loss: 0.003474559914482566\n",
      "epoch 943 | step 0 | loss: 0.00029518554665873813\n",
      "epoch 943 | step 1 | loss: 0.0005966968878032909\n",
      "epoch 943 | step 2 | loss: 0.0008881245286105372\n",
      "epoch 943 | step 3 | loss: 0.0011785244529779523\n",
      "epoch 943 | step 4 | loss: 0.0014298939802783969\n",
      "epoch 943 | step 5 | loss: 0.0017502246247703245\n",
      "epoch 943 | step 6 | loss: 0.002046038374170861\n",
      "epoch 943 | step 7 | loss: 0.0023309927562966386\n",
      "epoch 943 | step 8 | loss: 0.002622748523357599\n",
      "epoch 943 | step 9 | loss: 0.0029293315808375637\n",
      "epoch 943 | step 10 | loss: 0.003194613173936535\n",
      "epoch 943 | step 11 | loss: 0.0034670296491148616\n",
      "epoch 944 | step 0 | loss: 0.00028792331281758313\n",
      "epoch 944 | step 1 | loss: 0.0005937098825842538\n",
      "epoch 944 | step 2 | loss: 0.0008496721633588729\n",
      "epoch 944 | step 3 | loss: 0.001144055855240317\n",
      "epoch 944 | step 4 | loss: 0.001461668410674526\n",
      "epoch 944 | step 5 | loss: 0.0017613204670117472\n",
      "epoch 944 | step 6 | loss: 0.002050654958874111\n",
      "epoch 944 | step 7 | loss: 0.0023379537887978983\n",
      "epoch 944 | step 8 | loss: 0.0026298364867850524\n",
      "epoch 944 | step 9 | loss: 0.0029016144041941107\n",
      "epoch 944 | step 10 | loss: 0.0031832292671455445\n",
      "epoch 944 | step 11 | loss: 0.0034714642614296445\n",
      "epoch 945 | step 0 | loss: 0.0002743121699112268\n",
      "epoch 945 | step 1 | loss: 0.0005545707382495752\n",
      "epoch 945 | step 2 | loss: 0.0008482099896183163\n",
      "epoch 945 | step 3 | loss: 0.0011176262207272959\n",
      "epoch 945 | step 4 | loss: 0.0014143318047800315\n",
      "epoch 945 | step 5 | loss: 0.0017256160921247671\n",
      "epoch 945 | step 6 | loss: 0.001996105747113236\n",
      "epoch 945 | step 7 | loss: 0.002277276241487964\n",
      "epoch 945 | step 8 | loss: 0.0025563434418112768\n",
      "epoch 945 | step 9 | loss: 0.0028669853259212195\n",
      "epoch 945 | step 10 | loss: 0.0031438598610207154\n",
      "epoch 945 | step 11 | loss: 0.0034866579031984654\n",
      "epoch 946 | step 0 | loss: 0.0002963424829007151\n",
      "epoch 946 | step 1 | loss: 0.0005714462859411823\n",
      "epoch 946 | step 2 | loss: 0.0008502566460558469\n",
      "epoch 946 | step 3 | loss: 0.0011453362370529609\n",
      "epoch 946 | step 4 | loss: 0.0014484021614582353\n",
      "epoch 946 | step 5 | loss: 0.0017220423142661333\n",
      "epoch 946 | step 6 | loss: 0.0020397308152975803\n",
      "epoch 946 | step 7 | loss: 0.00233335793458314\n",
      "epoch 946 | step 8 | loss: 0.002604283587787907\n",
      "epoch 946 | step 9 | loss: 0.0028937477171674724\n",
      "epoch 946 | step 10 | loss: 0.003185733990647264\n",
      "epoch 946 | step 11 | loss: 0.0034698306930117466\n",
      "epoch 947 | step 0 | loss: 0.0002949234347930741\n",
      "epoch 947 | step 1 | loss: 0.0005987526792804822\n",
      "epoch 947 | step 2 | loss: 0.0008803456028534707\n",
      "epoch 947 | step 3 | loss: 0.0011655573737702908\n",
      "epoch 947 | step 4 | loss: 0.0014211435864413939\n",
      "epoch 947 | step 5 | loss: 0.0017086630867031774\n",
      "epoch 947 | step 6 | loss: 0.0019727366268644693\n",
      "epoch 947 | step 7 | loss: 0.002285599528409681\n",
      "epoch 947 | step 8 | loss: 0.002586642367473393\n",
      "epoch 947 | step 9 | loss: 0.0028707111328916757\n",
      "epoch 947 | step 10 | loss: 0.003177565139564434\n",
      "epoch 947 | step 11 | loss: 0.003473403259294417\n",
      "epoch 948 | step 0 | loss: 0.00030421136118443363\n",
      "epoch 948 | step 1 | loss: 0.0005879344868537815\n",
      "epoch 948 | step 2 | loss: 0.000875169841625835\n",
      "epoch 948 | step 3 | loss: 0.0011634906559459064\n",
      "epoch 948 | step 4 | loss: 0.0014696350747076595\n",
      "epoch 948 | step 5 | loss: 0.0017414845960057657\n",
      "epoch 948 | step 6 | loss: 0.002019940047818202\n",
      "epoch 948 | step 7 | loss: 0.002314220168161864\n",
      "epoch 948 | step 8 | loss: 0.0026220646272106995\n",
      "epoch 948 | step 9 | loss: 0.002903620216899784\n",
      "epoch 948 | step 10 | loss: 0.003181802243779381\n",
      "epoch 948 | step 11 | loss: 0.0034716483867046665\n",
      "epoch 949 | step 0 | loss: 0.0002979218794250002\n",
      "epoch 949 | step 1 | loss: 0.0005833818220473974\n",
      "epoch 949 | step 2 | loss: 0.0008767917212688431\n",
      "epoch 949 | step 3 | loss: 0.0011691980341109656\n",
      "epoch 949 | step 4 | loss: 0.0014735552086752638\n",
      "epoch 949 | step 5 | loss: 0.0017564681182306297\n",
      "epoch 949 | step 6 | loss: 0.0020435219552106336\n",
      "epoch 949 | step 7 | loss: 0.0023237147221612355\n",
      "epoch 949 | step 8 | loss: 0.0026120103402089865\n",
      "epoch 949 | step 9 | loss: 0.0029021727616264066\n",
      "epoch 949 | step 10 | loss: 0.0031931634886916854\n",
      "epoch 949 | step 11 | loss: 0.0034667124288429428\n",
      "epoch 950 | step 0 | loss: 0.0002900610864864666\n",
      "epoch 950 | step 1 | loss: 0.0005812436725577812\n",
      "epoch 950 | step 2 | loss: 0.0008761577875820604\n",
      "epoch 950 | step 3 | loss: 0.0011574384111543857\n",
      "epoch 950 | step 4 | loss: 0.0014467943588832512\n",
      "epoch 950 | step 5 | loss: 0.0017313336442249306\n",
      "epoch 950 | step 6 | loss: 0.0020469599494671196\n",
      "epoch 950 | step 7 | loss: 0.0023165102152241167\n",
      "epoch 950 | step 8 | loss: 0.0026069587894100273\n",
      "epoch 950 | step 9 | loss: 0.00288311122364918\n",
      "epoch 950 | step 10 | loss: 0.003166595434955987\n",
      "epoch 950 | step 11 | loss: 0.0034772187177010785\n",
      "epoch 951 | step 0 | loss: 0.00029174197959267844\n",
      "epoch 951 | step 1 | loss: 0.0005954418863037047\n",
      "epoch 951 | step 2 | loss: 0.0008838793661244302\n",
      "epoch 951 | step 3 | loss: 0.0011858745358344284\n",
      "epoch 951 | step 4 | loss: 0.0014766272892083605\n",
      "epoch 951 | step 5 | loss: 0.0017408671540262428\n",
      "epoch 951 | step 6 | loss: 0.001994880335531531\n",
      "epoch 951 | step 7 | loss: 0.0022841945355631795\n",
      "epoch 951 | step 8 | loss: 0.0025776912276160428\n",
      "epoch 951 | step 9 | loss: 0.0028836702296328124\n",
      "epoch 951 | step 10 | loss: 0.003177550415451738\n",
      "epoch 951 | step 11 | loss: 0.0034727007146579726\n",
      "epoch 952 | step 0 | loss: 0.00027236235644405933\n",
      "epoch 952 | step 1 | loss: 0.0005656610408523836\n",
      "epoch 952 | step 2 | loss: 0.0008262747094232861\n",
      "epoch 952 | step 3 | loss: 0.0011338815693553103\n",
      "epoch 952 | step 4 | loss: 0.0014207878365231532\n",
      "epoch 952 | step 5 | loss: 0.001719422080464407\n",
      "epoch 952 | step 6 | loss: 0.001998480104686334\n",
      "epoch 952 | step 7 | loss: 0.0022974781512332255\n",
      "epoch 952 | step 8 | loss: 0.002606058948391005\n",
      "epoch 952 | step 9 | loss: 0.002895284672478142\n",
      "epoch 952 | step 10 | loss: 0.0031771734564301464\n",
      "epoch 952 | step 11 | loss: 0.0034726032896337062\n",
      "epoch 953 | step 0 | loss: 0.0003084640959508709\n",
      "epoch 953 | step 1 | loss: 0.0006006145426392868\n",
      "epoch 953 | step 2 | loss: 0.000864563242523176\n",
      "epoch 953 | step 3 | loss: 0.0011411222197945155\n",
      "epoch 953 | step 4 | loss: 0.0014146511637836531\n",
      "epoch 953 | step 5 | loss: 0.0016987249904065922\n",
      "epoch 953 | step 6 | loss: 0.0020064944662112807\n",
      "epoch 953 | step 7 | loss: 0.0023005968892790564\n",
      "epoch 953 | step 8 | loss: 0.002590248320052149\n",
      "epoch 953 | step 9 | loss: 0.0028849163006839764\n",
      "epoch 953 | step 10 | loss: 0.0031794268168211168\n",
      "epoch 953 | step 11 | loss: 0.003471770852412081\n",
      "epoch 954 | step 0 | loss: 0.00029910175850661415\n",
      "epoch 954 | step 1 | loss: 0.0006030910094020697\n",
      "epoch 954 | step 2 | loss: 0.0009047513775957431\n",
      "epoch 954 | step 3 | loss: 0.00120424553324665\n",
      "epoch 954 | step 4 | loss: 0.0014880728271457681\n",
      "epoch 954 | step 5 | loss: 0.0017761067955948955\n",
      "epoch 954 | step 6 | loss: 0.0020558102321915584\n",
      "epoch 954 | step 7 | loss: 0.002343127734818481\n",
      "epoch 954 | step 8 | loss: 0.002621449351366636\n",
      "epoch 954 | step 9 | loss: 0.0028954070114490765\n",
      "epoch 954 | step 10 | loss: 0.003189020147557148\n",
      "epoch 954 | step 11 | loss: 0.003467976926462231\n",
      "epoch 955 | step 0 | loss: 0.0002608812094396731\n",
      "epoch 955 | step 1 | loss: 0.0005425209202409871\n",
      "epoch 955 | step 2 | loss: 0.0008278993876308354\n",
      "epoch 955 | step 3 | loss: 0.0011436754602997617\n",
      "epoch 955 | step 4 | loss: 0.0014278409708081221\n",
      "epoch 955 | step 5 | loss: 0.001719980578700995\n",
      "epoch 955 | step 6 | loss: 0.002009935393791864\n",
      "epoch 955 | step 7 | loss: 0.0023338520344249073\n",
      "epoch 955 | step 8 | loss: 0.002631197704487071\n",
      "epoch 955 | step 9 | loss: 0.0029284799312725286\n",
      "epoch 955 | step 10 | loss: 0.003189756722467902\n",
      "epoch 955 | step 11 | loss: 0.0034674866622263654\n",
      "epoch 956 | step 0 | loss: 0.0003008808296773924\n",
      "epoch 956 | step 1 | loss: 0.0005785048770566134\n",
      "epoch 956 | step 2 | loss: 0.0008992803447455758\n",
      "epoch 956 | step 3 | loss: 0.0012213317850944074\n",
      "epoch 956 | step 4 | loss: 0.0015010502690453841\n",
      "epoch 956 | step 5 | loss: 0.0017988880498457597\n",
      "epoch 956 | step 6 | loss: 0.002064990030621311\n",
      "epoch 956 | step 7 | loss: 0.002367961518706445\n",
      "epoch 956 | step 8 | loss: 0.002629259604983939\n",
      "epoch 956 | step 9 | loss: 0.0029198499404868954\n",
      "epoch 956 | step 10 | loss: 0.0031810622322155243\n",
      "epoch 956 | step 11 | loss: 0.003470775806877531\n",
      "epoch 957 | step 0 | loss: 0.0002800659621083716\n",
      "epoch 957 | step 1 | loss: 0.0005509081750394128\n",
      "epoch 957 | step 2 | loss: 0.0008664215479727503\n",
      "epoch 957 | step 3 | loss: 0.0011604658414677196\n",
      "epoch 957 | step 4 | loss: 0.0014245046800719714\n",
      "epoch 957 | step 5 | loss: 0.0016943281790882548\n",
      "epoch 957 | step 6 | loss: 0.001968302162756751\n",
      "epoch 957 | step 7 | loss: 0.002243580753716951\n",
      "epoch 957 | step 8 | loss: 0.0025968388439198973\n",
      "epoch 957 | step 9 | loss: 0.002879703942989794\n",
      "epoch 957 | step 10 | loss: 0.0031724147728176373\n",
      "epoch 957 | step 11 | loss: 0.003473867542304066\n",
      "epoch 958 | step 0 | loss: 0.0002925573641899429\n",
      "epoch 958 | step 1 | loss: 0.00059398779554889\n",
      "epoch 958 | step 2 | loss: 0.0008878381712665421\n",
      "epoch 958 | step 3 | loss: 0.0011916968923502147\n",
      "epoch 958 | step 4 | loss: 0.0014942681577108608\n",
      "epoch 958 | step 5 | loss: 0.0017868907128190774\n",
      "epoch 958 | step 6 | loss: 0.002042538727144534\n",
      "epoch 958 | step 7 | loss: 0.002322271063332436\n",
      "epoch 958 | step 8 | loss: 0.0026041255031124394\n",
      "epoch 958 | step 9 | loss: 0.0028877813228698366\n",
      "epoch 958 | step 10 | loss: 0.0031802192224878157\n",
      "epoch 958 | step 11 | loss: 0.003470739074912351\n",
      "epoch 959 | step 0 | loss: 0.00031231830782682643\n",
      "epoch 959 | step 1 | loss: 0.0005807653217751587\n",
      "epoch 959 | step 2 | loss: 0.0008670232353358232\n",
      "epoch 959 | step 3 | loss: 0.0011487279539505288\n",
      "epoch 959 | step 4 | loss: 0.0014463947196873789\n",
      "epoch 959 | step 5 | loss: 0.0017743479170883353\n",
      "epoch 959 | step 6 | loss: 0.0020645829300498754\n",
      "epoch 959 | step 7 | loss: 0.0023460793155864056\n",
      "epoch 959 | step 8 | loss: 0.002618316073038306\n",
      "epoch 959 | step 9 | loss: 0.0028983290784207146\n",
      "epoch 959 | step 10 | loss: 0.0031963150680231977\n",
      "epoch 959 | step 11 | loss: 0.0034643521465193366\n",
      "epoch 960 | step 0 | loss: 0.00028352653718403725\n",
      "epoch 960 | step 1 | loss: 0.0005690540197376397\n",
      "epoch 960 | step 2 | loss: 0.0008696549169314903\n",
      "epoch 960 | step 3 | loss: 0.0011570897058002503\n",
      "epoch 960 | step 4 | loss: 0.0014580208848536467\n",
      "epoch 960 | step 5 | loss: 0.0017452078226429355\n",
      "epoch 960 | step 6 | loss: 0.002017164645766671\n",
      "epoch 960 | step 7 | loss: 0.002312640967987289\n",
      "epoch 960 | step 8 | loss: 0.0026048379430892673\n",
      "epoch 960 | step 9 | loss: 0.0028945810652748843\n",
      "epoch 960 | step 10 | loss: 0.0031944315007671163\n",
      "epoch 960 | step 11 | loss: 0.0034649552693505397\n",
      "epoch 961 | step 0 | loss: 0.0002846815747320165\n",
      "epoch 961 | step 1 | loss: 0.0005853310232658369\n",
      "epoch 961 | step 2 | loss: 0.0008773747350780091\n",
      "epoch 961 | step 3 | loss: 0.0011729640075654607\n",
      "epoch 961 | step 4 | loss: 0.0014500581370124538\n",
      "epoch 961 | step 5 | loss: 0.0017532130515150012\n",
      "epoch 961 | step 6 | loss: 0.002038229259791225\n",
      "epoch 961 | step 7 | loss: 0.0023071375755007316\n",
      "epoch 961 | step 8 | loss: 0.0025799532524019555\n",
      "epoch 961 | step 9 | loss: 0.002874945077330249\n",
      "epoch 961 | step 10 | loss: 0.0031654562583906904\n",
      "epoch 961 | step 11 | loss: 0.0034764963844444324\n",
      "epoch 962 | step 0 | loss: 0.00029171693370832433\n",
      "epoch 962 | step 1 | loss: 0.0005702600962345281\n",
      "epoch 962 | step 2 | loss: 0.0008432662873923099\n",
      "epoch 962 | step 3 | loss: 0.0011202753172402488\n",
      "epoch 962 | step 4 | loss: 0.0014192784379876498\n",
      "epoch 962 | step 5 | loss: 0.0016905545079032448\n",
      "epoch 962 | step 6 | loss: 0.0019884907865362644\n",
      "epoch 962 | step 7 | loss: 0.002272321534185119\n",
      "epoch 962 | step 8 | loss: 0.002580390800311558\n",
      "epoch 962 | step 9 | loss: 0.0028747267632169525\n",
      "epoch 962 | step 10 | loss: 0.0031869410318161205\n",
      "epoch 962 | step 11 | loss: 0.0034679492668565765\n",
      "epoch 963 | step 0 | loss: 0.00025673228654235996\n",
      "epoch 963 | step 1 | loss: 0.0005598337001052981\n",
      "epoch 963 | step 2 | loss: 0.0008357169904076249\n",
      "epoch 963 | step 3 | loss: 0.0011299324724449865\n",
      "epoch 963 | step 4 | loss: 0.0014296924038751633\n",
      "epoch 963 | step 5 | loss: 0.0017103235758147086\n",
      "epoch 963 | step 6 | loss: 0.0019965715250627805\n",
      "epoch 963 | step 7 | loss: 0.002267866468670306\n",
      "epoch 963 | step 8 | loss: 0.002559078444525286\n",
      "epoch 963 | step 9 | loss: 0.0028759127262639205\n",
      "epoch 963 | step 10 | loss: 0.0031846645100525525\n",
      "epoch 963 | step 11 | loss: 0.003468351031807448\n",
      "epoch 964 | step 0 | loss: 0.00031563880959063013\n",
      "epoch 964 | step 1 | loss: 0.0005988179876001223\n",
      "epoch 964 | step 2 | loss: 0.0008864973428089741\n",
      "epoch 964 | step 3 | loss: 0.0011737824105601477\n",
      "epoch 964 | step 4 | loss: 0.0014552822940692004\n",
      "epoch 964 | step 5 | loss: 0.0017320982880931891\n",
      "epoch 964 | step 6 | loss: 0.00200185722381474\n",
      "epoch 964 | step 7 | loss: 0.002318325722419963\n",
      "epoch 964 | step 8 | loss: 0.0026120934635353752\n",
      "epoch 964 | step 9 | loss: 0.002888267169855025\n",
      "epoch 964 | step 10 | loss: 0.0031840803724063685\n",
      "epoch 964 | step 11 | loss: 0.0034686625340330313\n",
      "epoch 965 | step 0 | loss: 0.00027492089312544513\n",
      "epoch 965 | step 1 | loss: 0.0005920989568695829\n",
      "epoch 965 | step 2 | loss: 0.0008851481834398452\n",
      "epoch 965 | step 3 | loss: 0.0011788962058551765\n",
      "epoch 965 | step 4 | loss: 0.0014819481957346115\n",
      "epoch 965 | step 5 | loss: 0.0017917244843403232\n",
      "epoch 965 | step 6 | loss: 0.00208657524794218\n",
      "epoch 965 | step 7 | loss: 0.0023484908178763907\n",
      "epoch 965 | step 8 | loss: 0.002607331716707884\n",
      "epoch 965 | step 9 | loss: 0.0028950408591929097\n",
      "epoch 965 | step 10 | loss: 0.0031905535043390415\n",
      "epoch 965 | step 11 | loss: 0.003466051174968999\n",
      "epoch 966 | step 0 | loss: 0.00029444064276527686\n",
      "epoch 966 | step 1 | loss: 0.0005613158483005032\n",
      "epoch 966 | step 2 | loss: 0.0008576002072512654\n",
      "epoch 966 | step 3 | loss: 0.0011387716190549824\n",
      "epoch 966 | step 4 | loss: 0.0014179915580044317\n",
      "epoch 966 | step 5 | loss: 0.0017067288568818847\n",
      "epoch 966 | step 6 | loss: 0.0020068786227495127\n",
      "epoch 966 | step 7 | loss: 0.0023019968073355766\n",
      "epoch 966 | step 8 | loss: 0.0025806678180569065\n",
      "epoch 966 | step 9 | loss: 0.002858603418086866\n",
      "epoch 966 | step 10 | loss: 0.0031575288928951304\n",
      "epoch 966 | step 11 | loss: 0.003478621238955526\n",
      "epoch 967 | step 0 | loss: 0.00027911262732427703\n",
      "epoch 967 | step 1 | loss: 0.000563632023379377\n",
      "epoch 967 | step 2 | loss: 0.0008387341918240351\n",
      "epoch 967 | step 3 | loss: 0.001145709126676916\n",
      "epoch 967 | step 4 | loss: 0.0014355211646188471\n",
      "epoch 967 | step 5 | loss: 0.001750264919332331\n",
      "epoch 967 | step 6 | loss: 0.002012547095572754\n",
      "epoch 967 | step 7 | loss: 0.002285439664875662\n",
      "epoch 967 | step 8 | loss: 0.0025673048095541822\n",
      "epoch 967 | step 9 | loss: 0.002882481563516763\n",
      "epoch 967 | step 10 | loss: 0.0031703024991875037\n",
      "epoch 967 | step 11 | loss: 0.0034734733560892956\n",
      "epoch 968 | step 0 | loss: 0.00029975996172423934\n",
      "epoch 968 | step 1 | loss: 0.0005807683677955463\n",
      "epoch 968 | step 2 | loss: 0.0008951205440436\n",
      "epoch 968 | step 3 | loss: 0.0011830490478944259\n",
      "epoch 968 | step 4 | loss: 0.001480326694367404\n",
      "epoch 968 | step 5 | loss: 0.0017610536400553337\n",
      "epoch 968 | step 6 | loss: 0.0020397449353795026\n",
      "epoch 968 | step 7 | loss: 0.0023485164409291588\n",
      "epoch 968 | step 8 | loss: 0.00263659852687497\n",
      "epoch 968 | step 9 | loss: 0.002885589594811816\n",
      "epoch 968 | step 10 | loss: 0.0031712173873126117\n",
      "epoch 968 | step 11 | loss: 0.003473231002673305\n",
      "epoch 969 | step 0 | loss: 0.00029829972807379925\n",
      "epoch 969 | step 1 | loss: 0.0006056963600626134\n",
      "epoch 969 | step 2 | loss: 0.0008789160889698247\n",
      "epoch 969 | step 3 | loss: 0.0011651329983835463\n",
      "epoch 969 | step 4 | loss: 0.0014493801889097339\n",
      "epoch 969 | step 5 | loss: 0.0017414606114626717\n",
      "epoch 969 | step 6 | loss: 0.0020396095688781646\n",
      "epoch 969 | step 7 | loss: 0.0023354833498694665\n",
      "epoch 969 | step 8 | loss: 0.00259742875854343\n",
      "epoch 969 | step 9 | loss: 0.0028946288514191073\n",
      "epoch 969 | step 10 | loss: 0.0031754329565911016\n",
      "epoch 969 | step 11 | loss: 0.0034714136877099775\n",
      "epoch 970 | step 0 | loss: 0.00032776719656868886\n",
      "epoch 970 | step 1 | loss: 0.0005950789177344285\n",
      "epoch 970 | step 2 | loss: 0.0008662282582574895\n",
      "epoch 970 | step 3 | loss: 0.0011873321808064895\n",
      "epoch 970 | step 4 | loss: 0.0014795658954805196\n",
      "epoch 970 | step 5 | loss: 0.0017609276377399089\n",
      "epoch 970 | step 6 | loss: 0.0020414525204063266\n",
      "epoch 970 | step 7 | loss: 0.002325361221801417\n",
      "epoch 970 | step 8 | loss: 0.0026008467579399664\n",
      "epoch 970 | step 9 | loss: 0.002894070172534284\n",
      "epoch 970 | step 10 | loss: 0.0031678262423566104\n",
      "epoch 970 | step 11 | loss: 0.0034746258927285598\n",
      "epoch 971 | step 0 | loss: 0.00029547306364646433\n",
      "epoch 971 | step 1 | loss: 0.0006037157369070057\n",
      "epoch 971 | step 2 | loss: 0.0008826770427545089\n",
      "epoch 971 | step 3 | loss: 0.0011714560612383987\n",
      "epoch 971 | step 4 | loss: 0.0014910828386162595\n",
      "epoch 971 | step 5 | loss: 0.0017569604468754365\n",
      "epoch 971 | step 6 | loss: 0.002054599735342393\n",
      "epoch 971 | step 7 | loss: 0.0023633589473885433\n",
      "epoch 971 | step 8 | loss: 0.00264482427607818\n",
      "epoch 971 | step 9 | loss: 0.002932015813286554\n",
      "epoch 971 | step 10 | loss: 0.0032096578075605626\n",
      "epoch 971 | step 11 | loss: 0.0034578310913564434\n",
      "epoch 972 | step 0 | loss: 0.0002868622277167377\n",
      "epoch 972 | step 1 | loss: 0.0005836790203248137\n",
      "epoch 972 | step 2 | loss: 0.0009137272183733485\n",
      "epoch 972 | step 3 | loss: 0.001190898707320124\n",
      "epoch 972 | step 4 | loss: 0.0014896481334101565\n",
      "epoch 972 | step 5 | loss: 0.0017741693806804\n",
      "epoch 972 | step 6 | loss: 0.002065844920432833\n",
      "epoch 972 | step 7 | loss: 0.002369305965827636\n",
      "epoch 972 | step 8 | loss: 0.002674288189944471\n",
      "epoch 972 | step 9 | loss: 0.00293123065329955\n",
      "epoch 972 | step 10 | loss: 0.003211436024407288\n",
      "epoch 972 | step 11 | loss: 0.003456861832240643\n",
      "epoch 973 | step 0 | loss: 0.00030618777787391933\n",
      "epoch 973 | step 1 | loss: 0.0005845159505104417\n",
      "epoch 973 | step 2 | loss: 0.0008944499993148094\n",
      "epoch 973 | step 3 | loss: 0.0011921442107904864\n",
      "epoch 973 | step 4 | loss: 0.001457569204078289\n",
      "epoch 973 | step 5 | loss: 0.0017471472992380739\n",
      "epoch 973 | step 6 | loss: 0.002042890625886913\n",
      "epoch 973 | step 7 | loss: 0.0023335824122668646\n",
      "epoch 973 | step 8 | loss: 0.0026374540954712967\n",
      "epoch 973 | step 9 | loss: 0.00291188671383437\n",
      "epoch 973 | step 10 | loss: 0.00320239387283123\n",
      "epoch 973 | step 11 | loss: 0.0034605657129271328\n",
      "epoch 974 | step 0 | loss: 0.00029748197483658054\n",
      "epoch 974 | step 1 | loss: 0.0005922278813269432\n",
      "epoch 974 | step 2 | loss: 0.0008623098256394003\n",
      "epoch 974 | step 3 | loss: 0.0011538935568118649\n",
      "epoch 974 | step 4 | loss: 0.0014491576687978903\n",
      "epoch 974 | step 5 | loss: 0.0017466601227591912\n",
      "epoch 974 | step 6 | loss: 0.002042481024839622\n",
      "epoch 974 | step 7 | loss: 0.002312783939163167\n",
      "epoch 974 | step 8 | loss: 0.0025922019535361087\n",
      "epoch 974 | step 9 | loss: 0.0028855401322727754\n",
      "epoch 974 | step 10 | loss: 0.0031873213263354768\n",
      "epoch 974 | step 11 | loss: 0.0034662437193804215\n",
      "epoch 975 | step 0 | loss: 0.0002830967054485322\n",
      "epoch 975 | step 1 | loss: 0.0005600440245784913\n",
      "epoch 975 | step 2 | loss: 0.0008246742672604393\n",
      "epoch 975 | step 3 | loss: 0.0011333525776830297\n",
      "epoch 975 | step 4 | loss: 0.0014347442883662393\n",
      "epoch 975 | step 5 | loss: 0.0017207375164061019\n",
      "epoch 975 | step 6 | loss: 0.0020021332390072607\n",
      "epoch 975 | step 7 | loss: 0.002299026682860818\n",
      "epoch 975 | step 8 | loss: 0.0026019432884306914\n",
      "epoch 975 | step 9 | loss: 0.0028934634108197547\n",
      "epoch 975 | step 10 | loss: 0.003202266333793205\n",
      "epoch 975 | step 11 | loss: 0.003460200769964124\n",
      "epoch 976 | step 0 | loss: 0.00031064780056990735\n",
      "epoch 976 | step 1 | loss: 0.0005789399340019871\n",
      "epoch 976 | step 2 | loss: 0.0008879818141639947\n",
      "epoch 976 | step 3 | loss: 0.0012019112209082905\n",
      "epoch 976 | step 4 | loss: 0.0014927888488412261\n",
      "epoch 976 | step 5 | loss: 0.0017704066004410515\n",
      "epoch 976 | step 6 | loss: 0.0020484340771972794\n",
      "epoch 976 | step 7 | loss: 0.00234726275017429\n",
      "epoch 976 | step 8 | loss: 0.0026047376618951786\n",
      "epoch 976 | step 9 | loss: 0.002899755897297735\n",
      "epoch 976 | step 10 | loss: 0.0031847716283358598\n",
      "epoch 976 | step 11 | loss: 0.0034669140574945523\n",
      "epoch 977 | step 0 | loss: 0.00031785282985931373\n",
      "epoch 977 | step 1 | loss: 0.0005756826602826334\n",
      "epoch 977 | step 2 | loss: 0.0009030405316858171\n",
      "epoch 977 | step 3 | loss: 0.0011983834692291411\n",
      "epoch 977 | step 4 | loss: 0.0014754971200196423\n",
      "epoch 977 | step 5 | loss: 0.0017624277647448725\n",
      "epoch 977 | step 6 | loss: 0.002075030047912609\n",
      "epoch 977 | step 7 | loss: 0.0023617679281966748\n",
      "epoch 977 | step 8 | loss: 0.002612679757835158\n",
      "epoch 977 | step 9 | loss: 0.0029143232348801183\n",
      "epoch 977 | step 10 | loss: 0.003167190598944209\n",
      "epoch 977 | step 11 | loss: 0.003473709666697403\n",
      "epoch 978 | step 0 | loss: 0.0002714869095868567\n",
      "epoch 978 | step 1 | loss: 0.000577140760967044\n",
      "epoch 978 | step 2 | loss: 0.0008768589253647923\n",
      "epoch 978 | step 3 | loss: 0.0011552404894155134\n",
      "epoch 978 | step 4 | loss: 0.001448497810819916\n",
      "epoch 978 | step 5 | loss: 0.0017585831587118759\n",
      "epoch 978 | step 6 | loss: 0.002050852009313864\n",
      "epoch 978 | step 7 | loss: 0.002329705855883479\n",
      "epoch 978 | step 8 | loss: 0.0025975390583246813\n",
      "epoch 978 | step 9 | loss: 0.002881683069051521\n",
      "epoch 978 | step 10 | loss: 0.003162194790087925\n",
      "epoch 978 | step 11 | loss: 0.003475466516320755\n",
      "epoch 979 | step 0 | loss: 0.00027477311076419287\n",
      "epoch 979 | step 1 | loss: 0.0005721496987125924\n",
      "epoch 979 | step 2 | loss: 0.0008765943710331255\n",
      "epoch 979 | step 3 | loss: 0.0011390285545658549\n",
      "epoch 979 | step 4 | loss: 0.0014440916304023213\n",
      "epoch 979 | step 5 | loss: 0.0017198668620448019\n",
      "epoch 979 | step 6 | loss: 0.0020317210401629903\n",
      "epoch 979 | step 7 | loss: 0.002327196720172006\n",
      "epoch 979 | step 8 | loss: 0.0026304649610417816\n",
      "epoch 979 | step 9 | loss: 0.002895232362559172\n",
      "epoch 979 | step 10 | loss: 0.0031806596215351165\n",
      "epoch 979 | step 11 | loss: 0.0034684130028402155\n",
      "epoch 980 | step 0 | loss: 0.00025032923790954686\n",
      "epoch 980 | step 1 | loss: 0.0005133885831671366\n",
      "epoch 980 | step 2 | loss: 0.0008056008282195905\n",
      "epoch 980 | step 3 | loss: 0.001130999055735074\n",
      "epoch 980 | step 4 | loss: 0.0014215299486472959\n",
      "epoch 980 | step 5 | loss: 0.0017074022503866964\n",
      "epoch 980 | step 6 | loss: 0.0020094943142264284\n",
      "epoch 980 | step 7 | loss: 0.0023160645352241667\n",
      "epoch 980 | step 8 | loss: 0.0026018032244820477\n",
      "epoch 980 | step 9 | loss: 0.0028798602039262723\n",
      "epoch 980 | step 10 | loss: 0.00319491485984416\n",
      "epoch 980 | step 11 | loss: 0.0034624231387739004\n",
      "epoch 981 | step 0 | loss: 0.0003004493928911693\n",
      "epoch 981 | step 1 | loss: 0.0006126040281941278\n",
      "epoch 981 | step 2 | loss: 0.0008863070611039934\n",
      "epoch 981 | step 3 | loss: 0.0011769713599911731\n",
      "epoch 981 | step 4 | loss: 0.00146463308933142\n",
      "epoch 981 | step 5 | loss: 0.0017376158947884648\n",
      "epoch 981 | step 6 | loss: 0.0020062420338726634\n",
      "epoch 981 | step 7 | loss: 0.0022953886901945927\n",
      "epoch 981 | step 8 | loss: 0.002572275946215483\n",
      "epoch 981 | step 9 | loss: 0.00286406017131765\n",
      "epoch 981 | step 10 | loss: 0.003176556583110832\n",
      "epoch 981 | step 11 | loss: 0.0034694042669373196\n",
      "epoch 982 | step 0 | loss: 0.00029474179944595166\n",
      "epoch 982 | step 1 | loss: 0.0006040024950867216\n",
      "epoch 982 | step 2 | loss: 0.0008797470392808843\n",
      "epoch 982 | step 3 | loss: 0.0011643599537619262\n",
      "epoch 982 | step 4 | loss: 0.0014414793558123924\n",
      "epoch 982 | step 5 | loss: 0.0017592169401745843\n",
      "epoch 982 | step 6 | loss: 0.0020299471985533355\n",
      "epoch 982 | step 7 | loss: 0.0023032772017209146\n",
      "epoch 982 | step 8 | loss: 0.002589587090672924\n",
      "epoch 982 | step 9 | loss: 0.002864752864206268\n",
      "epoch 982 | step 10 | loss: 0.003171424995398159\n",
      "epoch 982 | step 11 | loss: 0.003471270820642179\n",
      "epoch 983 | step 0 | loss: 0.00028960412078444034\n",
      "epoch 983 | step 1 | loss: 0.0005822051413575604\n",
      "epoch 983 | step 2 | loss: 0.0008557873487790897\n",
      "epoch 983 | step 3 | loss: 0.001138563573832309\n",
      "epoch 983 | step 4 | loss: 0.0014159392875289425\n",
      "epoch 983 | step 5 | loss: 0.001701363409173657\n",
      "epoch 983 | step 6 | loss: 0.0019758002936782738\n",
      "epoch 983 | step 7 | loss: 0.0022738440214100123\n",
      "epoch 983 | step 8 | loss: 0.0025630270833206666\n",
      "epoch 983 | step 9 | loss: 0.002875565628342133\n",
      "epoch 983 | step 10 | loss: 0.0031679992722876047\n",
      "epoch 983 | step 11 | loss: 0.003472766399097011\n",
      "epoch 984 | step 0 | loss: 0.00027434276719959553\n",
      "epoch 984 | step 1 | loss: 0.0005552765693284819\n",
      "epoch 984 | step 2 | loss: 0.0008602922264195009\n",
      "epoch 984 | step 3 | loss: 0.0011460967964382178\n",
      "epoch 984 | step 4 | loss: 0.0014307629808138376\n",
      "epoch 984 | step 5 | loss: 0.0017434333676274506\n",
      "epoch 984 | step 6 | loss: 0.0020404062191170655\n",
      "epoch 984 | step 7 | loss: 0.002326430484509396\n",
      "epoch 984 | step 8 | loss: 0.0026184959074125012\n",
      "epoch 984 | step 9 | loss: 0.0029013501905441404\n",
      "epoch 984 | step 10 | loss: 0.0031883909799300613\n",
      "epoch 984 | step 11 | loss: 0.0034646640397489646\n",
      "epoch 985 | step 0 | loss: 0.00028694452624640124\n",
      "epoch 985 | step 1 | loss: 0.0005758360444562433\n",
      "epoch 985 | step 2 | loss: 0.0008635320126974914\n",
      "epoch 985 | step 3 | loss: 0.0011618399772347588\n",
      "epoch 985 | step 4 | loss: 0.0014622252282529685\n",
      "epoch 985 | step 5 | loss: 0.001757755389271199\n",
      "epoch 985 | step 6 | loss: 0.002031403191651124\n",
      "epoch 985 | step 7 | loss: 0.002334802020897396\n",
      "epoch 985 | step 8 | loss: 0.002624109248887257\n",
      "epoch 985 | step 9 | loss: 0.0029135366692354114\n",
      "epoch 985 | step 10 | loss: 0.0031974340091236115\n",
      "epoch 985 | step 11 | loss: 0.003460928747321878\n",
      "epoch 986 | step 0 | loss: 0.00027889969874370134\n",
      "epoch 986 | step 1 | loss: 0.0005815940570746546\n",
      "epoch 986 | step 2 | loss: 0.0008489414466762941\n",
      "epoch 986 | step 3 | loss: 0.0011355096392381958\n",
      "epoch 986 | step 4 | loss: 0.0014309212428053133\n",
      "epoch 986 | step 5 | loss: 0.0017300575590186538\n",
      "epoch 986 | step 6 | loss: 0.002022838935059176\n",
      "epoch 986 | step 7 | loss: 0.002306611093800853\n",
      "epoch 986 | step 8 | loss: 0.002601079571469318\n",
      "epoch 986 | step 9 | loss: 0.0028862507075284186\n",
      "epoch 986 | step 10 | loss: 0.0031818944594016857\n",
      "epoch 986 | step 11 | loss: 0.0034666715890470643\n",
      "epoch 987 | step 0 | loss: 0.00027342689798344727\n",
      "epoch 987 | step 1 | loss: 0.000585237859498118\n",
      "epoch 987 | step 2 | loss: 0.0008600176560615888\n",
      "epoch 987 | step 3 | loss: 0.0011624646631306524\n",
      "epoch 987 | step 4 | loss: 0.0014693507825119189\n",
      "epoch 987 | step 5 | loss: 0.0017567255689427644\n",
      "epoch 987 | step 6 | loss: 0.002075503383972927\n",
      "epoch 987 | step 7 | loss: 0.0023767333601066036\n",
      "epoch 987 | step 8 | loss: 0.0026187157766175972\n",
      "epoch 987 | step 9 | loss: 0.002895019188515208\n",
      "epoch 987 | step 10 | loss: 0.0031870359145991104\n",
      "epoch 987 | step 11 | loss: 0.003464537410142691\n",
      "epoch 988 | step 0 | loss: 0.0002804738308153314\n",
      "epoch 988 | step 1 | loss: 0.0005562094411700564\n",
      "epoch 988 | step 2 | loss: 0.0008349321517294762\n",
      "epoch 988 | step 3 | loss: 0.001141410656632339\n",
      "epoch 988 | step 4 | loss: 0.0014319864836948137\n",
      "epoch 988 | step 5 | loss: 0.0017130326829960345\n",
      "epoch 988 | step 6 | loss: 0.001995616840992062\n",
      "epoch 988 | step 7 | loss: 0.0022805197899698286\n",
      "epoch 988 | step 8 | loss: 0.002581255823241054\n",
      "epoch 988 | step 9 | loss: 0.002900045343821118\n",
      "epoch 988 | step 10 | loss: 0.003204140999800482\n",
      "epoch 988 | step 11 | loss: 0.0034576644037482875\n",
      "epoch 989 | step 0 | loss: 0.0002762003095194035\n",
      "epoch 989 | step 1 | loss: 0.0005713617298518216\n",
      "epoch 989 | step 2 | loss: 0.0008809754804566132\n",
      "epoch 989 | step 3 | loss: 0.001164203102596334\n",
      "epoch 989 | step 4 | loss: 0.0014635644485478925\n",
      "epoch 989 | step 5 | loss: 0.0017619878909988702\n",
      "epoch 989 | step 6 | loss: 0.0020506771370092857\n",
      "epoch 989 | step 7 | loss: 0.0023389619150593725\n",
      "epoch 989 | step 8 | loss: 0.00259940681987198\n",
      "epoch 989 | step 9 | loss: 0.0028894464061459645\n",
      "epoch 989 | step 10 | loss: 0.0031759198942172746\n",
      "epoch 989 | step 11 | loss: 0.003468724784112522\n",
      "epoch 990 | step 0 | loss: 0.00027152277817604634\n",
      "epoch 990 | step 1 | loss: 0.0005551687552848431\n",
      "epoch 990 | step 2 | loss: 0.0008426723431437343\n",
      "epoch 990 | step 3 | loss: 0.0011206044677568596\n",
      "epoch 990 | step 4 | loss: 0.0014278463184736441\n",
      "epoch 990 | step 5 | loss: 0.0016960555237712054\n",
      "epoch 990 | step 6 | loss: 0.0020227802575900483\n",
      "epoch 990 | step 7 | loss: 0.0022954218873780805\n",
      "epoch 990 | step 8 | loss: 0.0026139494894425784\n",
      "epoch 990 | step 9 | loss: 0.0029183269927591606\n",
      "epoch 990 | step 10 | loss: 0.0032031986088606724\n",
      "epoch 990 | step 11 | loss: 0.0034578488869815107\n",
      "epoch 991 | step 0 | loss: 0.0002945409633597825\n",
      "epoch 991 | step 1 | loss: 0.0005901101567305667\n",
      "epoch 991 | step 2 | loss: 0.0008559676250827128\n",
      "epoch 991 | step 3 | loss: 0.0011349281103222159\n",
      "epoch 991 | step 4 | loss: 0.0014129144574277805\n",
      "epoch 991 | step 5 | loss: 0.0016952405028390616\n",
      "epoch 991 | step 6 | loss: 0.0019849223149749928\n",
      "epoch 991 | step 7 | loss: 0.002295018500679517\n",
      "epoch 991 | step 8 | loss: 0.002565725820392614\n",
      "epoch 991 | step 9 | loss: 0.002861050053873827\n",
      "epoch 991 | step 10 | loss: 0.0031767050549469837\n",
      "epoch 991 | step 11 | loss: 0.003468368188511428\n",
      "epoch 992 | step 0 | loss: 0.0002872038049687062\n",
      "epoch 992 | step 1 | loss: 0.0005692534184273351\n",
      "epoch 992 | step 2 | loss: 0.0008437458138026877\n",
      "epoch 992 | step 3 | loss: 0.0011243295798724383\n",
      "epoch 992 | step 4 | loss: 0.001431499502765717\n",
      "epoch 992 | step 5 | loss: 0.0017531191501156253\n",
      "epoch 992 | step 6 | loss: 0.002020382617575103\n",
      "epoch 992 | step 7 | loss: 0.0022807944384480843\n",
      "epoch 992 | step 8 | loss: 0.0025767895846654006\n",
      "epoch 992 | step 9 | loss: 0.0028785063372757424\n",
      "epoch 992 | step 10 | loss: 0.0031649956241382214\n",
      "epoch 992 | step 11 | loss: 0.0034727874802850923\n",
      "epoch 993 | step 0 | loss: 0.0002824476949623159\n",
      "epoch 993 | step 1 | loss: 0.0005635177041289441\n",
      "epoch 993 | step 2 | loss: 0.0008539629633844354\n",
      "epoch 993 | step 3 | loss: 0.0011355994916434746\n",
      "epoch 993 | step 4 | loss: 0.0014408772839047661\n",
      "epoch 993 | step 5 | loss: 0.0017410010464054318\n",
      "epoch 993 | step 6 | loss: 0.0020331830937199277\n",
      "epoch 993 | step 7 | loss: 0.002337504004684953\n",
      "epoch 993 | step 8 | loss: 0.0026322595059716618\n",
      "epoch 993 | step 9 | loss: 0.002916005897511158\n",
      "epoch 993 | step 10 | loss: 0.003200318520238441\n",
      "epoch 993 | step 11 | loss: 0.0034587803022966116\n",
      "epoch 994 | step 0 | loss: 0.0003090403746338061\n",
      "epoch 994 | step 1 | loss: 0.0006264952076691871\n",
      "epoch 994 | step 2 | loss: 0.0009531331913162644\n",
      "epoch 994 | step 3 | loss: 0.0012160278226388528\n",
      "epoch 994 | step 4 | loss: 0.00150348323769673\n",
      "epoch 994 | step 5 | loss: 0.0017786190054324635\n",
      "epoch 994 | step 6 | loss: 0.002085072893411426\n",
      "epoch 994 | step 7 | loss: 0.0023544841584511593\n",
      "epoch 994 | step 8 | loss: 0.002613668714388042\n",
      "epoch 994 | step 9 | loss: 0.002924580467445555\n",
      "epoch 994 | step 10 | loss: 0.0031833584910031442\n",
      "epoch 994 | step 11 | loss: 0.003465334996655902\n",
      "epoch 995 | step 0 | loss: 0.00029155955539356744\n",
      "epoch 995 | step 1 | loss: 0.0005936995265579031\n",
      "epoch 995 | step 2 | loss: 0.0008556603393629443\n",
      "epoch 995 | step 3 | loss: 0.0011308516431775433\n",
      "epoch 995 | step 4 | loss: 0.0014067508892830424\n",
      "epoch 995 | step 5 | loss: 0.0016878859704030393\n",
      "epoch 995 | step 6 | loss: 0.0020006139905043084\n",
      "epoch 995 | step 7 | loss: 0.002318041649000634\n",
      "epoch 995 | step 8 | loss: 0.002597158139477841\n",
      "epoch 995 | step 9 | loss: 0.002886058721031556\n",
      "epoch 995 | step 10 | loss: 0.003172687620430647\n",
      "epoch 995 | step 11 | loss: 0.003469032546242\n",
      "epoch 996 | step 0 | loss: 0.00028074203537504323\n",
      "epoch 996 | step 1 | loss: 0.0005633559843129631\n",
      "epoch 996 | step 2 | loss: 0.0008386776663800579\n",
      "epoch 996 | step 3 | loss: 0.0011404126441467543\n",
      "epoch 996 | step 4 | loss: 0.001425588715600431\n",
      "epoch 996 | step 5 | loss: 0.0017058859550306796\n",
      "epoch 996 | step 6 | loss: 0.0019844505751238526\n",
      "epoch 996 | step 7 | loss: 0.0022847485660786194\n",
      "epoch 996 | step 8 | loss: 0.002568686034112664\n",
      "epoch 996 | step 9 | loss: 0.002858184718942639\n",
      "epoch 996 | step 10 | loss: 0.003159557771961952\n",
      "epoch 996 | step 11 | loss: 0.003474196338245049\n",
      "epoch 997 | step 0 | loss: 0.0002813267451155546\n",
      "epoch 997 | step 1 | loss: 0.0005884993562100748\n",
      "epoch 997 | step 2 | loss: 0.0009019126860768247\n",
      "epoch 997 | step 3 | loss: 0.001182713010489661\n",
      "epoch 997 | step 4 | loss: 0.001469724320868148\n",
      "epoch 997 | step 5 | loss: 0.0017547910761683068\n",
      "epoch 997 | step 6 | loss: 0.0020508927642559484\n",
      "epoch 997 | step 7 | loss: 0.002333873327373777\n",
      "epoch 997 | step 8 | loss: 0.002617327088106147\n",
      "epoch 997 | step 9 | loss: 0.0029056380657189146\n",
      "epoch 997 | step 10 | loss: 0.003180728230489234\n",
      "epoch 997 | step 11 | loss: 0.003465745033442623\n",
      "epoch 998 | step 0 | loss: 0.0002528923489561465\n",
      "epoch 998 | step 1 | loss: 0.0005335198036075079\n",
      "epoch 998 | step 2 | loss: 0.0008419801220239694\n",
      "epoch 998 | step 3 | loss: 0.0011245997491671537\n",
      "epoch 998 | step 4 | loss: 0.0014129457745295615\n",
      "epoch 998 | step 5 | loss: 0.0017320631237805555\n",
      "epoch 998 | step 6 | loss: 0.0020348599368105176\n",
      "epoch 998 | step 7 | loss: 0.0023061264080146233\n",
      "epoch 998 | step 8 | loss: 0.0026045088667317794\n",
      "epoch 998 | step 9 | loss: 0.0028819610369411794\n",
      "epoch 998 | step 10 | loss: 0.003170682417071943\n",
      "epoch 998 | step 11 | loss: 0.003469714742975856\n",
      "epoch 999 | step 0 | loss: 0.00029351790225888975\n",
      "epoch 999 | step 1 | loss: 0.0006070488082046685\n",
      "epoch 999 | step 2 | loss: 0.0009036166704669915\n",
      "epoch 999 | step 3 | loss: 0.0011792302566805367\n",
      "epoch 999 | step 4 | loss: 0.0014498561269307671\n",
      "epoch 999 | step 5 | loss: 0.001747031380442478\n",
      "epoch 999 | step 6 | loss: 0.0020341745170764577\n",
      "epoch 999 | step 7 | loss: 0.002362049532117908\n",
      "epoch 999 | step 8 | loss: 0.0026014928621659766\n",
      "epoch 999 | step 9 | loss: 0.0028749781227618773\n",
      "epoch 999 | step 10 | loss: 0.003168714059403507\n",
      "epoch 999 | step 11 | loss: 0.0034705347944802047\n"
     ]
    }
   ],
   "source": [
    "freq = 5\n",
    "for epoch in range(1000):\n",
    "    total_loss = 0.0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        _pulse_parameters = batch[\"x0\"].numpy()\n",
    "        _unitaries = batch[\"x1\"].numpy()\n",
    "        _expectations = batch[\"y\"].numpy()\n",
    "\n",
    "        model_params, opt_state, loss = train_step(model_params, opt_state, _pulse_parameters, _unitaries, _expectations)\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "        print(f\"epoch {epoch} | step {i} | loss: {total_loss / freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': {'U': Array([[3.73226933, 4.05474459, 2.63021372],\n",
       "         [3.68750149, 4.00503233, 2.75178667],\n",
       "         [3.61604906, 3.90828079, 2.76443914],\n",
       "         [3.57998813, 3.89382677, 2.76589543],\n",
       "         [3.60015026, 4.02776351, 2.60074799]], dtype=float64),\n",
       "  'D': Array([[ 0.97739975, -0.96378737],\n",
       "         [ 0.96928637, -0.96483688],\n",
       "         [ 0.96360744, -0.94448443],\n",
       "         [ 0.96407468, -0.94210232],\n",
       "         [ 0.97535586, -0.95301118]], dtype=float64)},\n",
       " 'Y': {'U': Array([[3.13014621, 3.76144292, 1.65054678],\n",
       "         [3.18739323, 3.8081313 , 1.86559491],\n",
       "         [2.87538439, 3.78014599, 1.84613854],\n",
       "         [2.86750494, 3.78851834, 1.89684213],\n",
       "         [3.02375846, 3.80736038, 1.71132905]], dtype=float64),\n",
       "  'D': Array([[ 0.98018032, -0.9674017 ],\n",
       "         [ 0.9728488 , -0.94090151],\n",
       "         [ 0.9679564 , -0.95663107],\n",
       "         [ 0.96319901, -0.94365806],\n",
       "         [ 0.97502298, -0.95996603]], dtype=float64)},\n",
       " 'Z': {'U': Array([[1.91569763, 2.99089409, 2.02792776],\n",
       "         [2.14723686, 2.92410405, 2.51485735],\n",
       "         [2.39252693, 3.01642429, 2.38142095],\n",
       "         [2.50844508, 3.01797444, 2.51735146],\n",
       "         [2.06296189, 2.98072292, 2.16017666]], dtype=float64),\n",
       "  'D': Array([[ 0.98028745, -0.97559424],\n",
       "         [ 0.97164762, -0.96148653],\n",
       "         [ 0.95948853, -0.9530622 ],\n",
       "         [ 0.95368575, -0.95052382],\n",
       "         [ 0.97404433, -0.96706767]], dtype=float64)}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_out = model.apply(model_params, _pulse_parameters[:5])\n",
    "test_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.optimize import minimize\n",
    "from core import gate_loss, X, Y, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.01586717, dtype=float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the gate loss to test the function\n",
    "gate_loss(\n",
    "    jnp.array(pulse_parameters[0]).flatten(),\n",
    "    model,\n",
    "    model_params,\n",
    "    simulator,\n",
    "    pulse_sequence,\n",
    "    X,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/porametpathumsoot/miniconda3/envs/specq-dev/lib/python3.12/site-packages/jax/_src/lax/lax.py:2660: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  x_bar = _convert_element_type(x_bar, x.aval.dtype, x.aval.weak_type)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(8.61050063e-05, dtype=float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "pulse_params = pulse_sequence.sample_params(key)\n",
    "x0 = pulse_sequence.list_of_params_to_array(pulse_params)\n",
    "res = minimize(\n",
    "    gate_loss,\n",
    "    x0=jnp.array(x0),\n",
    "    method=\"BFGS\",\n",
    "    args=(model, model_params, simulator, pulse_sequence, X),\n",
    ")\n",
    "res.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 18 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the optmized result\n",
    "opt_pulse_params = pulse_sequence.array_to_list_of_params(res.x)\n",
    "pulse_sequence.draw(opt_pulse_params)\n",
    "\n",
    "# Calculate the expectation values\n",
    "waveforms = pulse_sequence.get_waveform(opt_pulse_params)\n",
    "unitaries = simulator(waveforms)\n",
    "\n",
    "expvals = calculate_expvals(unitaries)\n",
    "plot_expvals(expvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'X': Array(0.99105151, dtype=float64),\n",
       "  'Y': Array(0.99019143, dtype=float64),\n",
       "  'Z': Array(0.99502516, dtype=float64)},\n",
       " Array(0.99684331, dtype=float64))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wo_params = model.apply(model_params, jnp.expand_dims(res.x, axis=0))\n",
    "Wo_params = jax.tree_map(lambda x: jnp.squeeze(x, 0), Wo_params)\n",
    "\n",
    "fidelities = {}\n",
    "for pauli_str, pauli_op in zip([\"X\", \"Y\", \"Z\"], [X, Y, Z]):\n",
    "    Wo = Wo_2_level(U=Wo_params[pauli_str][\"U\"], D=Wo_params[pauli_str][\"D\"])\n",
    "    # evaluate the fidleity to the Pauli operator\n",
    "    fidelities[pauli_str] = gate_fidelity(Wo, pauli_op)\n",
    "\n",
    "fidelities, gate_fidelity(unitaries[-1], X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "specq-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
